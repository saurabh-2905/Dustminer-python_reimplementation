{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ce3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dustminer Implementation\n",
    "# Loading the normal data as Good pile and Faulty data as Bad pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from libraries.utils import get_paths, read_traces, read_json, mapint2var, is_consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e01a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CODE = 'theft_protection'               ### application (code) theft_protection, mamba2, lora_ducy\n",
    "BEHAVIOUR_FAULTY = 'faulty_data/diag_subseq/subseq/'        ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal/'             ### normal, faulty_data\n",
    "THREAD = 'single'                       ### single, multi\n",
    "VER = 4                                 ### format of data collection\n",
    "\n",
    "base_dir = '../trace_data'              ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(\"Normal base path:\", normalbase_path)\n",
    "print(\"Faulty base path:\", faultybase_path)\n",
    "\n",
    "#######\n",
    "MAX_PATTERN_LEN = 15\n",
    "DELTA = 0.05\n",
    "TOP_K = 10       # 10, 1000\n",
    "\n",
    "MIN_COVERAGE = 0.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dcff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base_path = os.path.join(normalbase_path, 'train_data') #'diag_refsamples500')\n",
    "print(\"Train base path:\", train_base_path)\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "train_data_path = [os.path.join(train_base_path, x) for x in os.listdir(train_base_path)]\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in os.listdir(normalbase_path) if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "train_data_path = [x for x in train_data_path if '.DS_Store' not in x]\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "paths_log = [x for x in paths_log if '.DS_Store' not in x]\n",
    "paths_traces = [x for x in paths_traces if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "\n",
    "print(\"Number of training data files:\", len(train_data_path))\n",
    "print(\"Number of training varlist files:\", len(train_varlist_path))\n",
    "print(\"Number of faulty log files:\", len(paths_log))\n",
    "print(\"Number of faulty trace files:\", len(paths_traces))\n",
    "print(\"Number of faulty varlist files:\", len(varlist_path))\n",
    "print(\"Number of faulty label files:\", len(paths_label))\n",
    "\n",
    "paths_log.sort()\n",
    "paths_traces.sort()\n",
    "varlist_path.sort()\n",
    "paths_label.sort()\n",
    "\n",
    "test_data_path = paths_traces\n",
    "test_label_path = paths_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33afd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path[::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f87f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file in file_paths:\n",
    "        traces = read_traces(file)\n",
    "        if isinstance(traces, list) and len(traces) <=2:\n",
    "            # id_sequence = [trace for trace in traces]\n",
    "            id_sequence = traces[0]\n",
    "            print(\"id_sequence:\", id_sequence)\n",
    "\n",
    "        elif isinstance(traces, list) and len(traces) > 2:\n",
    "            id_sequence = [int(trace[0]) for trace in traces if isinstance(trace, list) and len(trace) >= 2]\n",
    "        \n",
    "        data.append(id_sequence)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_log_directory = train_data_path\n",
    "bad_log_directory = test_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_log_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_log_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load data ####\n",
    "good_sequences = load_data(good_log_directory)\n",
    "bad_sequences = load_data(bad_log_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cfa59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12855792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def get_ground_truth_file(path, ground_truth_files):\n",
    "    filename = []\n",
    "    for file in ground_truth_files:\n",
    "        if Path(path).stem in Path(file).stem:\n",
    "            print(\"from function - matched ground truth file is :\", file)\n",
    "            filename = file\n",
    "    return filename\n",
    "\n",
    "def get_trace_info(path):\n",
    "    filename = Path(path).stem\n",
    "    match = re.search(r\"(trace_trial_?\\d+)_(\\d+)-(\\d+)\", filename)\n",
    "    if match:\n",
    "        name = match.group(1)\n",
    "        start = int(match.group(2))\n",
    "        end = int(match.group(3))\n",
    "        test_data_name = name+'_'+str(start)+'-'+str(end)+'.json'\n",
    "        return name, start, end, test_data_name\n",
    "    else:\n",
    "        raise ValueError(\"Filename format not recognized\")\n",
    "\n",
    "def find_sequence_ground_truth(test_data_path, ground_truth):\n",
    "    trace = read_traces(test_data_path)\n",
    "    name, start, end,test_data_name = get_trace_info(test_data_path)\n",
    "    sequence = [int(ev[0]) for ev in trace if isinstance(ev, list) and len(ev) >= 2]\n",
    "    gt_start_end_pair = [[x[0], x[1]] for x in ground_truth]\n",
    "    return sequence, gt_start_end_pair\n",
    "\n",
    "\n",
    "def create_labels(sequence, gt_start_end_pair, test_data_start_index, test_data_end_index):\n",
    "    start_index = test_data_start_index\n",
    "    end_index = test_data_end_index\n",
    "    event_list = []\n",
    "    event_id_list = []\n",
    "    for start, end in gt_start_end_pair:\n",
    "        event_list = []\n",
    "        for event_id in range(start_index, end_index):\n",
    "            if event_id >= start and event_id <= end:\n",
    "                print(\"Event ID {} is in ground truth range ({}, {})\".format(event_id, start, end))\n",
    "                print(\"event_id - start_index:\", event_id , start_index)\n",
    "                event_list.append(sequence[event_id - start_index])\n",
    "        if event_list:\n",
    "            event_id_list.append(event_list)\n",
    "\n",
    "    return event_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.anomaly_detection import discover_test_files, load_ground_truth_dir, build_labels\n",
    "import json \n",
    " \n",
    "gt_path   = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/labels\" # example\n",
    "ground_truth_path = [os.path.join(gt_path, x) for x in os.listdir(gt_path)]\n",
    "\n",
    "print(\"ground truth path:\", ground_truth_path)\n",
    "\n",
    "new_label = {}\n",
    "\n",
    "for test_data in test_data_path:\n",
    "    print(\"Test data file:\", test_data)\n",
    "\n",
    "\n",
    "    print(\"---------------------------------------------------\")\n",
    "    test_data_name_1, test_data_start_index, test_data_end_index, test_data_name= get_trace_info(test_data)\n",
    "    print(\"Test data name is : \", test_data_name_1)\n",
    "    print(\"Test data start index is : \", test_data_start_index)\n",
    "    print(\"Test data end index is : \", test_data_end_index)\n",
    "\n",
    "\n",
    "    ground_truth_filename = get_ground_truth_file(test_data_name_1, ground_truth_path)\n",
    "    if not ground_truth_filename:\n",
    "        print(\"No matching ground truth file found for test data:\", test_data)\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        print(\"Ground truth file name is : \", ground_truth_filename)\n",
    "\n",
    "\n",
    "        ground_truth_raw = read_traces(ground_truth_filename)                                               # read ground truth labels from the label file\n",
    "        ground_truth = ground_truth_raw['labels']                                                # extract labels from dictionary from ground truth data\n",
    "\n",
    "        label_trace_name = list(ground_truth.keys())[0]\n",
    "        ground_truth = ground_truth[label_trace_name]\n",
    "\n",
    "        print(\"ground truth:\", ground_truth)\n",
    "\n",
    "        print(\"The test data file \", test_data, \" is not corresponding to ground truth file : \", ground_truth_filename)\n",
    "        print(\"The test data file \", test_data, \" is corresponding to ground truth file : \", ground_truth_filename)\n",
    "        sequence, gt_start_end_pair = find_sequence_ground_truth(test_data, ground_truth)\n",
    "        print(\"Event ID sequence is : \", sequence)\n",
    "        print(\"Ground truth start-end pairs are : \", gt_start_end_pair)\n",
    "        labels = create_labels(sequence, gt_start_end_pair, test_data_start_index, test_data_end_index)\n",
    "        print(\"Labels are : \", labels)\n",
    "        new_label[test_data_name] = labels\n",
    "        print(\"New label dictionary is : \", new_label)\n",
    "\n",
    "output_dir = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"gt_test_data_labels.json\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_label, f, indent=4)\n",
    "\n",
    "print(f\"\\n Saved to file: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac416e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_max_length(seq, event_id):\n",
    "    max_length = 0\n",
    "    start_index = seq.index(event_id)\n",
    "    end_index = 0\n",
    "    for i in range(start_index + 1 , len(seq)):\n",
    "        if seq[i] == event_id:\n",
    "            end_index = i - start_index\n",
    "            if end_index > max_length:\n",
    "                max_length = end_index\n",
    "            start_index = i\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def max_gap_two_lists(sequences):\n",
    "    max_length = 0\n",
    "    events = []\n",
    "    event_id_max_length = {}\n",
    "\n",
    "    for seq in sequences:\n",
    "        if seq and isinstance(seq[0], list):\n",
    "            inner_sequences = seq\n",
    "        else:\n",
    "            inner_sequences = [seq]  \n",
    "        for inner_seq in inner_sequences:\n",
    "            for i, x in enumerate(inner_seq):\n",
    "                gap = get_max_length(inner_seq, x)\n",
    "                if x not in event_id_max_length:\n",
    "                    event_id_max_length[x] = gap\n",
    "                    events.append(x)\n",
    "                else:\n",
    "                    if gap > event_id_max_length[x]:\n",
    "                        event_id_max_length[x] = gap\n",
    "\n",
    "                if event_id_max_length[x] > max_length:\n",
    "                    max_length = event_id_max_length[x]\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c14571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_sequences_1 = good_sequences\n",
    "\n",
    "# MAX_PATTERN_LEN = max_gap_two_lists(all_sequences_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_same_index(seq):\n",
    "    \"\"\"\"\n",
    "    This function is used to compute next occurence of every element in the sequence.\n",
    "    parameters :\n",
    "        seq - list(int)\n",
    "    Returns:\n",
    "        nxt - list(int) where nxt[i] is the index of the next occurrence of seq[i] in seq and n if none.\n",
    "    \"\"\"\n",
    "    last_pos = {}\n",
    "    n = len(seq)\n",
    "    nxt = [n] * n\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        v = seq[i]\n",
    "        if v in last_pos:\n",
    "            nxt[i] = last_pos[v]\n",
    "        last_pos[v] = i\n",
    "    return nxt\n",
    "\n",
    "\n",
    "def dynamic_window_sequence(sequences):\n",
    "    \"\"\"\n",
    "    This function generates dynamic windows for each sequence in sequences.\n",
    "    For example, for sequence [1,2,3,1,4,2], the dynamic windows are: [1,2,3],[1,4,2]\n",
    "    parameters :\n",
    "        sequences - list of list(int)\n",
    "    Returns:\n",
    "        final_windows - list of list(int) containing all dynamic windows from all sequences.\n",
    "    \"\"\"\n",
    "    final_windows = []\n",
    "    for seq in sequences:\n",
    "        if not seq:\n",
    "            continue\n",
    "        # print(\"Generating dynamic windows for sequence:\", seq)\n",
    "        nxt = next_same_index(seq)\n",
    "        # print(\"Next same index array:\", nxt)\n",
    "        n = len(seq)\n",
    "        for i in range(n):\n",
    "            j = nxt[i]\n",
    "            if j > i:\n",
    "                final_windows.append(seq[i:j])\n",
    "    return final_windows\n",
    "\n",
    "def has_substring(pattern, sequence):\n",
    "    \"\"\"\n",
    "    This function checks if the given pattern is a subsequence of sequence.\n",
    "    parameters : \n",
    "        pattern - list(int)\n",
    "        sequence - list(int)\n",
    "    Returns:\n",
    "        True if the pattern appears as a continous match in sequence.\n",
    "        False if no match\n",
    "    \"\"\"\n",
    "    pattern = tuple(pattern)\n",
    "    sequence = tuple(sequence)\n",
    "    m = len(pattern)\n",
    "    n = len(sequence)\n",
    "    if m == 0:\n",
    "        return True\n",
    "    if m > n:\n",
    "        return False\n",
    "    for i in range(n - m + 1):\n",
    "        if sequence[i:i+m] == pattern:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# def compress_patterns(patterns):\n",
    "#     \"\"\"\n",
    "#     Compress frequent patterns by removing redundant shorter ones.\n",
    "#     For each pattern P, if there exists a longer pattern Q such that:\n",
    "#         - P appears as a CONTIGUOUS substring of Q, and has support(P) == support(Q), then P is dropped (only Q is kept).\n",
    "\n",
    "#     Parameters:\n",
    "#         patterns (dict[tuple[int, ...], int]): Dictionary mapping each pattern tuple to its support count.\n",
    "\n",
    "#     Returns:\n",
    "#         dict[tuple[int, ...], int]: Compressed dictionary containing only longest unique-support patterns.\n",
    "\n",
    "#     For example:\n",
    "#         Input  : {(6,7):3, (6,7,8):3, (6,8):2}\n",
    "#         Output : {(6,7,8):3, (6,8):2} Because (6,7) is a substring of (6,7,8) and both have same support=3.\n",
    "#     \"\"\"\n",
    "#     items = list(patterns.items())\n",
    "#     items.sort(key=lambda x: (-len(x[0]), x[0]))\n",
    "#     print(items)\n",
    "#     compressed_patterns = {}\n",
    "#     for pattern, support in items:\n",
    "#         drop = False\n",
    "#         for other in compressed_patterns.keys():\n",
    "#             if patterns[other] == support and has_substring(pattern, other):\n",
    "#                 drop = True\n",
    "#                 break\n",
    "#         if not drop:\n",
    "#             compressed_patterns[pattern] = support\n",
    "#     return compressed_patterns\n",
    "\n",
    "\n",
    "def compress_patterns(current, next):\n",
    "    \"\"\"\n",
    "    Compress frequent patterns by removing redundant shorter ones.\n",
    "    For each pattern P, if there exists a longer pattern Q such that:\n",
    "        - P appears as a CONTIGUOUS substring of Q, and has support(P) == support(Q), then P is dropped (only Q is kept).\n",
    "\n",
    "    Parameters:\n",
    "        patterns (dict[tuple[int, ...], int]): Dictionary mapping each pattern tuple to its support count.\n",
    "\n",
    "    Returns:\n",
    "        dict[tuple[int, ...], int]: Compressed dictionary containing only longest unique-support patterns.\n",
    "\n",
    "    For example:\n",
    "        Input  : {(6,7):3, (6,7,8):3, (6,8):2}\n",
    "        Output : {(6,7,8):3, (6,8):2} Because (6,7) is a substring of (6,7,8) and both have same support=3.\n",
    "    \"\"\"\n",
    "    current_pats = list(current.keys())\n",
    "    next_pats = list(next.keys())\n",
    "    compressed_patterns = {}\n",
    "    for pattern in current_pats:\n",
    "        drop = False\n",
    "        for other in next_pats:\n",
    "            # print(\"Comparing pattern:\", pattern, \"with other pattern:\", other)\n",
    "            # print(\"Has substring:\", has_substring(pattern, other))\n",
    "            if pattern == other[:-1]:\n",
    "                # print(\"Pattern\", pattern, \"is a substring of\", other, current[pattern], next[other])\n",
    "                if abs(current[pattern] - next[other])<=0.3:\n",
    "                    # print(\"Droped Pattern\", pattern)\n",
    "                    drop = True\n",
    "                    break\n",
    "        if not drop:\n",
    "            compressed_patterns[pattern] = current[pattern]\n",
    "    return compressed_patterns\n",
    "\n",
    "def check_ordered_support(pattern, windows):\n",
    "    if not windows:\n",
    "        return 0.0\n",
    "\n",
    "    # print(\"pattern from check is : \", pattern)\n",
    "    # print(\"pattern from check is : \", pattern[0])\n",
    "\n",
    "    # match_count = 0\n",
    "    # pattern1 = pattern[1:]\n",
    "    # for window in windows:\n",
    "    #     # print(\"Evaluating window:\", window)\n",
    "    #     # print(\"Evaluating pattern1:\", pattern1)\n",
    "    #     i = 1\n",
    "    #     j = 0\n",
    "    #     while i < len(window) and j < len(pattern1):\n",
    "    #         if window[i] == pattern1[j]:\n",
    "    #             j = j + 1\n",
    "    #         i = i + 1\n",
    "\n",
    "    #     if j == len(pattern1):\n",
    "    #         match_count += 1\n",
    "\n",
    "    ###################\n",
    "    pattern_str = ','.join(map(str, pattern))\n",
    "    # print(\"pattern from check is : \", pattern_str, windows_str)\n",
    "    match_count = 0\n",
    "    for window in windows:\n",
    "        window_str = ','.join(map(str, window))\n",
    "        if pattern_str in window_str:\n",
    "            match_count += 1\n",
    "            # print(\"Matched pattern:\", pattern_str, \"in window:\", window_str)\n",
    "\n",
    "    return round(match_count / len(windows), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5774ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def generate_cartesian_event(sequence, delta=0.1, max_len=10):\n",
    "    seq = sequence\n",
    "    print(f\"Sequence: {seq}\")\n",
    "    print(\"Length:\", len(seq))\n",
    "\n",
    "    dynamic_windows = dynamic_window_sequence([seq]) \n",
    "    # print(\"Dynamic windows generated:\", dynamic_windows)\n",
    "    sorted_dynamic_windows = defaultdict(list)\n",
    "    for _win in dynamic_windows:\n",
    "        key = _win[0]\n",
    "        sorted_dynamic_windows[key].append(_win)\n",
    "\n",
    "    # print(\"sorted_dynamic_windows:\", sorted_dynamic_windows)\n",
    "    seq_length = len(seq)\n",
    "    S1 = list(set(seq))\n",
    "    infile_support = {e: seq.count(e)/seq_length for e in S1}\n",
    "\n",
    "    # print(\"S1 is \", infile_support)\n",
    "    \n",
    "    # min_val = min(infile_support.values())\n",
    "    min_val = 0\n",
    "    max_val = max(infile_support.values())\n",
    "    # print(\"Min and Max values are :\", min_val, max_val)\n",
    "    \n",
    "    normalized_infile_value = {}\n",
    "    if min_val == max_val:\n",
    "        normalized_infile_value = {e: 1.0 for e in infile_support}\n",
    "    else:\n",
    "        for e, v in infile_support.items():\n",
    "            normalized_infile_value[e] = round((v - min_val) / (max_val - min_val), 4)\n",
    "    \n",
    "    # print(\"Normalized S1 is \", normalized_infile_value)\n",
    "    s1_new = {k: v for k, v in normalized_infile_value.items() if v > delta}\n",
    "    event_id_s1 = list(s1_new.keys())\n",
    "    \n",
    "    # print(f\"S1 Generated: {len(s1_new)} items\")\n",
    "    # print(\"S1\", s1_new)\n",
    "    # print('event_id_s1:', event_id_s1)\n",
    "\n",
    "    S2_candidates = set()\n",
    "    for a in range(len(event_id_s1)):\n",
    "        for b in range(len(event_id_s1)):\n",
    "            if a != b: \n",
    "                S2_candidates.add((event_id_s1[a], event_id_s1[b]))\n",
    "\n",
    "    all_patterns = {}\n",
    "    s2_with_support = {}\n",
    "    \n",
    "    # print(f\"Total S2 candidates: {S2_candidates}\")\n",
    "    for pair in S2_candidates:\n",
    "        # print(\"Evaluating pair:\", pair)\n",
    "        # print(\"Evaluating pair:\", pair[0])\n",
    "        support = check_ordered_support(pair, sorted_dynamic_windows[pair[0]])\n",
    "        if support >= delta:\n",
    "            s2_with_support[pair] = support\n",
    "\n",
    "    # print(\"S2 count is :\", len(s2_with_support))\n",
    "    # print(\"S2: \", s2_with_support)\n",
    "    all_patterns[2] = s2_with_support\n",
    "\n",
    "    current_patterns = s2_with_support\n",
    "    S1_items = list(s1_new.keys())\n",
    "    k = 3\n",
    "\n",
    "    while k<=max_len:\n",
    "        candidates = set()\n",
    "        \n",
    "        for pattern in current_patterns.keys(): \n",
    "            for item in S1_items:\n",
    "                if item not in pattern:\n",
    "                    new_cand = tuple(list(pattern) + [item])\n",
    "                    # print('pattern:', pattern)\n",
    "                    # print(\"Generated new candidate pattern:\", new_cand)\n",
    "                    candidates.add(new_cand)\n",
    "                            \n",
    "        next_patterns = {}\n",
    "        for m in candidates:\n",
    "            # support = check_ordered_support(m, dynamic_windows)\n",
    "            support = check_ordered_support(m, sorted_dynamic_windows[m[0]])\n",
    "            if support >= delta:\n",
    "                next_patterns[m] = support\n",
    "        \n",
    "        if not next_patterns:\n",
    "            break\n",
    "        # print('current patterns:', current_patterns)\n",
    "        # print('next_patterns', next_patterns)\n",
    "        compressed_pattern = compress_patterns(current_patterns, next_patterns)\n",
    "        # print(f\"Compressed S{k} patterns:\", compressed_pattern)\n",
    "\n",
    "        # print(f\"S{k}: \", len(compressed_pattern))\n",
    "        # print(f\"S{k}: \", compressed_pattern)\n",
    "\n",
    "        ### storing compressed pattern of previous length\n",
    "        # print('k', k)\n",
    "        # print('len of', k-1, len(all_patterns[k-1]))\n",
    "        # print('compressed pattern length:', len(compressed_pattern))\n",
    "        all_patterns[k-1] = compressed_pattern\n",
    "        all_patterns[k] = next_patterns\n",
    "        current_patterns = next_patterns\n",
    "        k += 1\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return all_patterns, sorted_dynamic_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b23e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "good_sequence_patterns = []\n",
    "for i in range(0, len(good_sequences)):\n",
    "    print(f\"length of {i} good file is : {len(good_sequences[i])}\")\n",
    "    _patterns, _ = generate_cartesian_event(good_sequences[i], delta=DELTA, max_len=MAX_PATTERN_LEN)\n",
    "    good_sequence_patterns.append(_patterns)\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7406f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (14, 13)\n",
    "y = [14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13]\n",
    "\n",
    "x_str = ','.join(map(str, x))\n",
    "y_str = ','.join(map(str, y))\n",
    "print(x_str, y_str)\n",
    "print(x_str in y_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5439375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sequence_patterns(pattern_list_of_dicts):\n",
    "    all_supports = {}\n",
    "    # print(\"Number of sequences to merge:\", len(pattern_list_of_dicts))\n",
    "    for i in range(len(pattern_list_of_dicts)):\n",
    "        _keys = pattern_list_of_dicts[i].keys()\n",
    "        # print(\"Keys in pattern_list_of_dicts:\", _keys)\n",
    "        for key in _keys:\n",
    "            pattern_dict = pattern_list_of_dicts[i][key]\n",
    "            for pattern, support in pattern_dict.items():\n",
    "                # print('p', pattern, 's', support)\n",
    "                if pattern not in all_supports:\n",
    "                    all_supports[pattern] = []\n",
    "                all_supports[pattern].append(support)\n",
    "    # print(\"All supports collected:\", all_supports)\n",
    "    final_patterns = {}\n",
    "    for pattern, support_list in all_supports.items():\n",
    "        average_val = sum(support_list) / len(support_list)\n",
    "        final_patterns[pattern] = round(average_val, 4)\n",
    "        \n",
    "    ### sort them in dict according to length\n",
    "    pattern_keys = list(final_patterns.keys())\n",
    "    pattern_keys.sort(key=lambda x: len(x))\n",
    "    # print(\"Sorted pattern keys by length:\", pattern_keys)\n",
    "    sorted_patterns = defaultdict(dict)\n",
    "    for key in pattern_keys:\n",
    "        key_len = len(key)\n",
    "        sorted_patterns[key_len][key] = final_patterns[key]\n",
    "\n",
    "    return sorted_patterns\n",
    "\n",
    "\n",
    "def check_subsequence(discriminative_pat, gt_seq, MIN_COVERAGE):\n",
    "    pt = len(discriminative_pat)\n",
    "    gt = len(gt_seq)\n",
    "\n",
    "    if pt == 0:\n",
    "        return False\n",
    "    # elif pt < gt:\n",
    "    #     for i in range(gt - pt + 1):\n",
    "    #         if gt_seq[i:i+pt] == list(discriminative_pat):\n",
    "    #             coverage = pt / gt\n",
    "    #             print(\"Coverage is :\", coverage)\n",
    "    #             return coverage >= MIN_COVERAGE         \n",
    "    else:\n",
    "        #### slide the shorter seq over longer seq and check how much portion of gt is matching\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_good_patterns = merge_sequence_patterns(good_sequence_patterns)\n",
    "print(\"total Good Patterns : \", len(merged_good_patterns))\n",
    "print(\"Good Patterns : \", merged_good_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634866e4",
   "metadata": {},
   "source": [
    "### Save Good Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b13daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save good patterns as using pandas dataframe\n",
    "import os\n",
    "from genericpath import isdir\n",
    "import json\n",
    "output_dir = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/normal/\"\n",
    "isdir(output_dir)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"dustminer_train\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_path, f\"good_patterns_{MAX_PATTERN_LEN}_{DELTA}.json\")\n",
    "\n",
    "### convert keys from tuple to string for json serialization\n",
    "# _merged_good_patterns = {str(k): v for k, v in merged_good_patterns.items()}\n",
    "\n",
    "serialize_2 = {}\n",
    "for length, patterns in merged_good_patterns.items():\n",
    "    serialize_1 = {}\n",
    "    for pattern, support in patterns.items():\n",
    "        serialize_1[str(pattern)] = support\n",
    "    serialize_2[length] = serialize_1\n",
    "\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(_merged_good_patterns, f, indent=4)\n",
    "# print(f\"\\n Saved to file: {output_path}\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(serialize_2, f, indent=4)\n",
    "print(f\"\\n Saved to file: {output_path}\")\n",
    "\n",
    "def load_good_patterns(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        patterns_str = json.load(f)\n",
    "    # Convert string keys back to tuple\n",
    "    # patterns = {eval(k): v for k, v in patterns_str.items()}\n",
    "    patterns = defaultdict(dict)\n",
    "    for length, pattern_dict in patterns_str.items():\n",
    "        for pattern_str, support in pattern_dict.items():\n",
    "            pattern_tuple = eval(pattern_str)\n",
    "            length_tuple = int(length)\n",
    "            patterns[length_tuple][pattern_tuple] = support\n",
    "            \n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e1d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _patterns = load_good_patterns(output_path)\n",
    "# print(\"Loaded patterns:\", _patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216ee6d",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4938a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_common_contiguous_length(a, b):\n",
    "    \"\"\"Return length of longest common contiguous subsequence between lists a and b.\"\"\"\n",
    "    if not a or not b:\n",
    "        return 0\n",
    "    m, n = len(a), len(b)\n",
    "    # dp[j] will represent length of longest suffix match for a[:i] and b[:j]\n",
    "    dp = [0] * (n + 1)\n",
    "    maxlen = 0\n",
    "    for i in range(1, m + 1):\n",
    "        # iterate backwards to avoid overwriting values needed for this row\n",
    "        for j in range(n, 0, -1):\n",
    "            if a[i - 1] == b[j - 1]:\n",
    "                dp[j] = dp[j - 1] + 1\n",
    "                if dp[j] > maxlen:\n",
    "                    maxlen = dp[j]\n",
    "            else:\n",
    "                dp[j] = 0\n",
    "    return maxlen\n",
    "\n",
    "\n",
    "def continuous_coverage(gt_seq, detection_seq):\n",
    "    \"\"\"\n",
    "    Compute maximum continuous overlap (as fraction of ground-truth length).\n",
    "    It finds the longest contiguous common subsequence between gt_seq and detection_seq\n",
    "    then returns matched_length / len(gt_seq).\n",
    "    \"\"\"\n",
    "    if not isinstance(gt_seq, (list, tuple)) or not isinstance(detection_seq, (list, tuple)):\n",
    "        return 0.0\n",
    "    if len(gt_seq) == 0:\n",
    "        return 0.0\n",
    "    max_common = longest_common_contiguous_length(list(gt_seq), list(detection_seq))\n",
    "    return max_common / len(gt_seq)\n",
    "\n",
    "\n",
    "def check_subsequence(discriminative_pat, gt_seq, MIN_COVERAGE=0.6):\n",
    "    \"\"\"\n",
    "    Return True if the best continuous overlap between discriminative_pat and gt_seq\n",
    "    covers at least MIN_COVERAGE fraction of the ground-truth (gt_seq).\n",
    "    \"\"\"\n",
    "    coverage = continuous_coverage(gt_seq, discriminative_pat)\n",
    "    return coverage >= MIN_COVERAGE\n",
    "\n",
    "def check_correct_detections(detections, gt_seq, MIN_COVERAGE=0.6):\n",
    "    \"\"\"\n",
    "    Calculate TP and FP in given detections against ground-truth sequence.\n",
    "    Returns list of y_true and y_pred for each detection.\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    correct_pred = []\n",
    "    rest_pred = []\n",
    "    false_neg = []\n",
    "    correct_det_ind = []\n",
    "    if not detections:\n",
    "        return y_true, y_pred\n",
    "    \n",
    "    for gt in gt_seq:\n",
    "        matched = False\n",
    "        gt_coverage = []\n",
    "        for det in detections:\n",
    "            coverage = continuous_coverage(gt, det)\n",
    "            gt_coverage.append(coverage)\n",
    "        \n",
    "        print(\"gt_coverage:\", gt_coverage)\n",
    "        if gt_coverage:\n",
    "            max_ind = np.argmax(np.array(gt_coverage))\n",
    "            # print(\"max_ind:\", max_ind,)\n",
    "            if gt_coverage[max_ind] >= MIN_COVERAGE:\n",
    "                matched = True\n",
    "                correct_pred.append(detections[max_ind])\n",
    "                # print(\"Matched detection:\", detections[max_ind], \"with coverage:\", gt_coverage[max_ind])\n",
    "                # print(\"Ground truth sequence:\", gt)\n",
    "                # print('detections before pop:', detections)\n",
    "                \n",
    "                correct_det_ind.append(max_ind)\n",
    "                # print('detections after pop:', detections)\n",
    "\n",
    "        if matched:\n",
    "            ### true positive\n",
    "            y_true.append(1)\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            ### false negative\n",
    "            y_true.append(1)\n",
    "            y_pred.append(0)\n",
    "            false_neg.append(gt)\n",
    "\n",
    "    print('detections:', detections)\n",
    "    correct_det_ind = set(correct_det_ind)\n",
    "    correct_det_ind = sorted(correct_det_ind, reverse=True)\n",
    "    print(\"correct_det_ind:\", correct_det_ind)\n",
    "    correct_pred = set(correct_pred)\n",
    "    for ind in correct_det_ind:\n",
    "        detections.pop(ind)  # remove matched detection to avoid double counting\n",
    "\n",
    "    for det in detections:\n",
    "        ### false positive\n",
    "        rest_pred.append(det)\n",
    "        y_true.append(0)\n",
    "        y_pred.append(1)\n",
    "            \n",
    "    return correct_pred, rest_pred, false_neg, y_true, y_pred\n",
    "\n",
    "\n",
    "# # Example usage with provided sequences\n",
    "# # gt = [12, 6, 7, 8, 9, 6]\n",
    "# s1 = [12, 6, 7, 8, 9, 13, 14]\n",
    "# s2 = [11, 12, 6, 7, 8, 9, 13, 14]\n",
    "# s3 = [10, 11, 12, 6, 7, 8, 9, 13, 14]\n",
    "\n",
    "# gt = [6,7,10, 11, 12, 6, 7, 8, 9, 6]\n",
    "# for s in (s1, s2, s3):\n",
    "#     cov = continuous_coverage(gt, s)\n",
    "#     ok = check_subsequence(s, gt, MIN_COVERAGE=0.5)\n",
    "#     print(f\"detection={s}  coverage_of_gt={cov:.4f}  passes_0.5={ok}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c882c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bad_sequences = load_data(bad_log_directory)\n",
    "\n",
    "bad_sequence_patterns = []\n",
    "for i in range(0, len(bad_sequences)):\n",
    "    print(f\"length of {i} bad file is : {len(bad_sequences[i])}\")\n",
    "    _patterns, _ = generate_cartesian_event(bad_sequences[i], delta=DELTA, max_len=MAX_PATTERN_LEN)\n",
    "    bad_sequence_patterns.append(_patterns)\n",
    "    print('')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ace48",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sequence_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a0b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get Discriminative Patterns\n",
    "# good paterns in merged_good_patterns\n",
    "\n",
    "LABEL_PATH = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/gt_test_data_labels.json\"\n",
    "\n",
    "\n",
    "# ### get all good patterns without support\n",
    "# good_key_patterns = list(merged_good_patterns.keys())\n",
    "# ### sort good patterns based on length\n",
    "# good_key_patterns.sort(key=lambda x: len(x), reverse=True)\n",
    "# print(\"Total Good Patterns before comparison: \", len(good_key_patterns))\n",
    "all_tp = []\n",
    "all_fp = []\n",
    "all_fn = []\n",
    "all_gt = []\n",
    "all_detections = [] ### format [file1_detection, file2_detection] -> file1_detection: [(state1, 0), (ts1, ts2), filename]  \n",
    "all_features = []  ### collection of features (corresponding events for anomaly from reference samples)\n",
    "y_pred_all = []\n",
    "y_true_all = []\n",
    "for i in range(0, len(bad_sequence_patterns)):\n",
    "    # bad_sequence_patterns = {}\n",
    "    # print(f\"length of {i} bad file is : {len(bad_sequences[i])}\")\n",
    "    # _patterns, _ = generate_cartesian_event(bad_sequences[i], delta=DELTA, max_len=MAX_PATTERN_LEN)\n",
    "    _patterns = bad_sequence_patterns[i]\n",
    "    _file_name = bad_log_directory[i].split('/')[-1]\n",
    "    print(\"Processing bad file:\", _file_name)\n",
    "\n",
    "    _keys = _patterns.keys()\n",
    "    # print(\"Keys in bad sequence patterns:\", _keys)\n",
    "\n",
    "    #### accumulate all patterns with different lengths\n",
    "    discriminative_patterns_good = []\n",
    "    discriminative_patterns_bad = []\n",
    "    discriminative_support_good = []\n",
    "    discriminative_support_bad = []\n",
    "    for key in _keys:\n",
    "        # print(\"Processing patterns of length:\", key)\n",
    "        bad_patterns_dict = _patterns[key]\n",
    "        good_patterns_dict = merged_good_patterns[key]\n",
    "        good_patterns_keys = list(good_patterns_dict.keys())\n",
    "\n",
    "        # print('B', bad_patterns_dict, 'G', good_patterns_dict)\n",
    "        for bad_pattern, bad_support in bad_patterns_dict.items():\n",
    "            if bad_pattern not in good_patterns_dict:\n",
    "                # print(\"Discriminative Pattern found:\", bad_pattern)\n",
    "                discriminative_patterns_bad.append(bad_pattern)\n",
    "                discriminative_support_bad.append(bad_support)\n",
    "            else:\n",
    "                good_support = good_patterns_dict[bad_pattern]\n",
    "                if abs(bad_support - good_support) > 0.3:\n",
    "                    # print(\"Discriminative Pattern found with different support:\", bad_pattern)\n",
    "                    discriminative_patterns_bad.append(bad_pattern)\n",
    "                    discriminative_support_bad.append(bad_support)\n",
    "                else:\n",
    "                    pass\n",
    "                good_patterns_keys.remove(bad_pattern)\n",
    "        for good_pattern in good_patterns_keys:\n",
    "            good_support = good_patterns_dict[good_pattern]\n",
    "            # print(\"Discriminative Pattern found in good patterns:\", good_pattern)\n",
    "            discriminative_patterns_good.append(good_pattern)\n",
    "            discriminative_support_good.append(good_support)\n",
    "        # break\n",
    "\n",
    "    ### sort discriminative patterns based on respective support\n",
    "    discriminative_patterns_good_sorted = sorted(zip(discriminative_patterns_good, discriminative_support_good), key=lambda x: x[1], reverse=True)\n",
    "    discriminative_patterns_bad_sorted = sorted(zip(discriminative_patterns_bad, discriminative_support_bad), key=lambda x: x[1], reverse=True)\n",
    "    print('Disc pattern count:', len(discriminative_patterns_bad_sorted)) \n",
    "    print('Disc patterns sorted', discriminative_patterns_bad_sorted )\n",
    "\n",
    "    with open(LABEL_PATH, \"r\") as f:\n",
    "        label_map = json.load(f)\n",
    "\n",
    "    gt_seq = label_map[_file_name]  # assuming we take the first ground truth sequence for simplicity\n",
    "    detection = [p for p, _ in discriminative_patterns_bad_sorted[:TOP_K]]\n",
    "\n",
    "    # print(\"Label map loaded:\", label_map)\n",
    "    print('gt_seq:', gt_seq, 'pd_seq:', detection)\n",
    "\n",
    "    correct_pred, rest_pred, false_neg, y_true, y_pred = check_correct_detections(detection, gt_seq, MIN_COVERAGE=MIN_COVERAGE)\n",
    "\n",
    "    print('correct_pred:', correct_pred)\n",
    "    print('rest_pred:', rest_pred)\n",
    "\n",
    "    y_pred_all.extend(y_pred)\n",
    "    y_true_all.extend(y_true)\n",
    "    \n",
    "    label_file = LABEL_PATH\n",
    "    all_detections += [(_file_name, detection, label_file)]  ### used to plot detections\n",
    "    # all_features += [feature]\n",
    "    all_tp += [(_file_name, correct_pred, label_file)]\n",
    "    all_fp += [(_file_name, rest_pred, label_file)]\n",
    "    all_fn += [(_file_name, false_neg, label_file)]\n",
    "    all_gt += [(_file_name, gt_seq, label_file)]\n",
    "    \n",
    "    print('')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47e5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Discriminative Good Patterns:\", discriminative_patterns_good_sorted)\n",
    "# print(\"Discriminative Good Supports:\", discriminative_support_good)\n",
    "print(\"Discriminative Bad Patterns:\", discriminative_patterns_bad_sorted)\n",
    "print(\"Discriminative Bad Supports:\", discriminative_support_bad)\n",
    "\n",
    "print(\"All True Positives:\", all_tp)\n",
    "print(\"All False Positives:\", all_fp)\n",
    "print(\"FP count:\", sum(len(x[1]) for x in all_fp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c24efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, average_precision_score, ConfusionMatrixDisplay, adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_true_all, y_pred_all)\n",
    "print(f'Precision: {precision:.4f}')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_true_all, y_pred_all)\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "# # Calculate average precision\n",
    "# average_precision = average_precision_score(y_true_all, y_pred_all)\n",
    "# print(f'Average Precision: {average_precision:.4f}')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_true_all, y_pred_all)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print('')\n",
    "\n",
    "# ### isntance length mean and std\n",
    "# mean_inst_len = np.mean(inst_len_all)\n",
    "# std_inst_len = np.std(inst_len_all)\n",
    "# print('avg_inst_len:', mean_inst_len)\n",
    "# print('std_inst_len:', std_inst_len)\n",
    "# print('')\n",
    "\n",
    "# ### avg number of anomalies in each detection \n",
    "# mean_gt_in_inst = np.mean(gt_in_inst_all)\n",
    "# std_gt_in_inst = np.std(gt_in_inst_all)\n",
    "# print('avg_gt_in_inst:', mean_gt_in_inst)\n",
    "# print('std_gt_in_inst:', std_gt_in_inst)\n",
    "# print('')\n",
    "\n",
    "# ### avg number of detecions per GT\n",
    "# mean_inst_in_gt = np.mean(num_anomaly_per_gt_all)\n",
    "# std_inst_in_gt = np.std(num_anomaly_per_gt_all)\n",
    "# print('mean_inst_in_gt:', mean_inst_in_gt)\n",
    "# print('std_inst_in_gt:', std_inst_in_gt)\n",
    "# print('')\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_all, y_pred_all)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "if len(conf_matrix) == 1:\n",
    "    conf_matrix = np.array([[0, 0], [0, conf_matrix[0][0]]])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['normal', 'anomaly'])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c101a430",
   "metadata": {},
   "source": [
    "## Classwise Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607d12b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LABEL_PATH = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/diag_subseq/subseq/subseq_labels/subseq_class.json\"\n",
    "\n",
    "with open(CLASS_LABEL_PATH, 'r') as f:\n",
    "    class_mapping = json.load(f)\n",
    "\n",
    "# print('class mapping:', class_mapping)\n",
    "classwise_fn = defaultdict(list)\n",
    "classwise_tp = defaultdict(list)\n",
    "gt_len = 0\n",
    "for file_fn, file_gt in zip(all_fn, all_gt):\n",
    "    fn = file_fn[1]\n",
    "    gt = file_gt[1]\n",
    "    # print('file fn:', file_fn)\n",
    "    # print('file gt:', file_gt)\n",
    "\n",
    "    file_name = file_gt[0].split('.')[0]\n",
    "    # print('file name:', file_name)\n",
    "    classes = class_mapping[file_name]\n",
    "\n",
    "    # print('classes:', classes)\n",
    "    # print('gt:', gt)\n",
    "    # print('fn:', fn)\n",
    "    for label, cl in zip(gt, classes):\n",
    "        # print('label:', label)\n",
    "        if label in fn:\n",
    "            classwise_fn[cl].append(label)\n",
    "        else:\n",
    "            classwise_tp[cl].append(label)\n",
    "            # print('tp:', label)\n",
    "\n",
    "    gt_len += len(gt)\n",
    "    # print('file gt:', len(gt))\n",
    "    # print('file fn:', len(fn))\n",
    "    # print('\\n')\n",
    "    # break\n",
    "\n",
    "total_fn = 0\n",
    "total_tp = 0\n",
    "keys = set(list(classwise_fn.keys()) + list(classwise_tp.keys()))\n",
    "# print('keys:', keys)\n",
    "class_recall = []\n",
    "for key in keys:\n",
    "    print('class:', key)\n",
    "    total_fn += len(classwise_fn[key])\n",
    "    total_tp += len(classwise_tp[key])\n",
    "\n",
    "    crecall = len(classwise_tp[key])/(len(classwise_fn[key])+len(classwise_tp[key]))\n",
    "    crecall = round(crecall, 4)\n",
    "\n",
    "    # print('not detected:', len(classwise_fn[key]))\n",
    "    print('detected:', len(classwise_tp[key]))\n",
    "    print('total anomalies:', len(classwise_fn[key])+len(classwise_tp[key]))\n",
    "    print('Recall (classwise):', crecall)\n",
    "    print('\\n')\n",
    "\n",
    "    class_recall.append(crecall)\n",
    "\n",
    "\n",
    "# print('total fn+tp:', total_fn+total_tp)\n",
    "# print('total gt:', gt_len)\n",
    "assert total_fn+total_tp == gt_len, 'total fn+tp not equal to total gt'\n",
    "print('All class recalls:', class_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15de88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb34b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminative_score = {}\n",
    "\n",
    "# #Here we take all the distinct event patterns from both bad and good logs and store in all_patterns.\n",
    "\n",
    "# if isinstance(merged_good_patterns, list):\n",
    "#     good_seqs = merged_good_patterns[0] \n",
    "# else:\n",
    "#     good_seqs = merged_good_patterns\n",
    "\n",
    "# all_patterns = set(good_seqs.keys())\n",
    "\n",
    "# for d in bad_sequence_patterns:\n",
    "#     all_patterns.update(d.keys())\n",
    "\n",
    "# print(\"all_patterns :\", len(all_patterns))\n",
    "\n",
    "# for pattern in all_patterns:\n",
    "#     support_good = good_seqs.get(pattern, 0.0)\n",
    "#     bad_seq_count = [d[pattern] for d in bad_sequence_patterns if pattern in d]\n",
    "    \n",
    "#     if bad_seq_count:\n",
    "#         support_bad = sum(bad_seq_count) / len(bad_seq_count)\n",
    "#     else:\n",
    "#         support_bad = 0.0\n",
    "\n",
    "#     score = support_good - support_bad\n",
    "#     discriminative_score[pattern] = score\n",
    "\n",
    "# sorted_patterns = sorted(discriminative_score.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "# threshold = 0.30\n",
    "\n",
    "# discriminative_patterns = {}\n",
    "# for pat, score in sorted_patterns:\n",
    "#     if abs(score) >= threshold:\n",
    "#         discriminative_patterns[pat] = {\"support\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debef892",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def normalize_patterns(discrinimative_obj):\n",
    "#     if discrinimative_obj is None:\n",
    "#         return []\n",
    "#     if isinstance(discrinimative_obj, dict):\n",
    "#         seq = discrinimative_obj.keys()\n",
    "#     else:\n",
    "#         seq = discrinimative_obj\n",
    "\n",
    "#     out = []\n",
    "#     for p in seq:\n",
    "#         if isinstance(p, (list, tuple, set)):\n",
    "#             out.append(tuple(p))\n",
    "#     return out\n",
    "\n",
    "# def normalize_gt_sequences(gt_seq_list):\n",
    "#     if not gt_seq_list:\n",
    "#         return []\n",
    "#     if isinstance(gt_seq_list[0], int):\n",
    "#         return [gt_seq_list]\n",
    "#     else:\n",
    "#         return [seq for seq in gt_seq_list if isinstance(seq, list) and len(seq) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4425c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "# import psutil\n",
    "# import time\n",
    "# from collections import defaultdict\n",
    "\n",
    "# process = psutil.Process(os.getpid())\n",
    "# start_mem = process.memory_info().rss / (1024 * 1024)  # in MB\n",
    "# start_time = time.perf_counter() \n",
    "\n",
    "# LABEL_PATH = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/gt_test_data_labels.json\"\n",
    "\n",
    "# MIN_COVERAGE = 0.60 \n",
    "\n",
    "# discriminative_patterns_seq = normalize_patterns(discriminative_patterns)\n",
    "\n",
    "# with open(LABEL_PATH, \"r\") as f:\n",
    "#     label_map = json.load(f)\n",
    "\n",
    "# tp = fp = fn = tn = 0\n",
    "# matched_patterns_global = set()\n",
    "\n",
    "# all_tp = []         \n",
    "# all_fp = []        \n",
    "# all_fn = []        \n",
    "# all_gt = []        \n",
    "# y_true_all = []     \n",
    "# y_pred_all = []     \n",
    "\n",
    "# label_file_path = LABEL_PATH \n",
    "\n",
    "# for test_file_name, raw_gt in label_map.items():\n",
    "#     gt_seqs_list = normalize_gt_sequences(raw_gt)\n",
    "\n",
    "#     all_gt.append((test_file_name, gt_seqs_list, label_file_path))\n",
    "\n",
    "#     correct_pred_file = [] # To store that GT_seq that matched discriminative patterns  \n",
    "#     rest_pred_file = [] # stores which patterns did not match gt sequence     \n",
    "#     false_neg_file = [] # for fn sequence\n",
    "\n",
    "#     total_patterns = len(discriminative_patterns_seq)  # no of patterns mined by dustminer\n",
    "\n",
    "#     for gt_seq in gt_seqs_list:\n",
    "#         y_true_all.append(1)   # appending 1 as gt label for every sequence is 1 fault present\n",
    "\n",
    "#         matched_for_gt = []\n",
    "#         for discriminative_patterns_1 in discriminative_patterns_seq:\n",
    "#             if check_subsequence(discriminative_patterns_1, gt_seq, MIN_COVERAGE): # check if there is a complete match or atleast 60% match \n",
    "#                 matched_for_gt.append(discriminative_patterns_1)\n",
    "#                 matched_patterns_global.add(discriminative_patterns_1) # set of all discriminative_patterns that match at least one gt sequence\n",
    "\n",
    "#         matched_count = len(matched_for_gt) # no of patterns that matched this gt sequence\n",
    "\n",
    "#         if matched_count == 0: # when no patterns match for this particular gt then its FN\n",
    "#             fn += 1\n",
    "#             fp += total_patterns   \n",
    "#             false_neg_file.append(gt_seq)\n",
    "#             rest_pred_file.append((gt_seq, list(discriminative_patterns_seq))) # appending this gt_seq and all patterns as its FN\n",
    "#             y_pred_all.append(0)  # no detection so appending 0\n",
    "\n",
    "#         else: # when atleast one pattern match for this particular gt then its TP\n",
    "#             tp += 1\n",
    "#             fp += (total_patterns - matched_count)\n",
    "\n",
    "#             fp_for_gt = [\n",
    "#                 p for p in discriminative_patterns_seq\n",
    "#                 if p not in matched_for_gt\n",
    "#             ]\n",
    "\n",
    "#             correct_pred_file.append((gt_seq, matched_for_gt))\n",
    "#             rest_pred_file.append((gt_seq, fp_for_gt)) # appending gt and remaining unmatched patterns\n",
    "#             y_pred_all.append(1)  # fault detected so appending 1\n",
    "\n",
    "#     all_tp.append((test_file_name, correct_pred_file, label_file_path))\n",
    "#     all_fp.append((test_file_name, rest_pred_file, label_file_path))\n",
    "#     all_fn.append((test_file_name, false_neg_file, label_file_path))\n",
    "\n",
    "# if tp + fp > 0:\n",
    "#     precision = tp / (tp + fp)\n",
    "# else:\n",
    "#     precision = 0.0\n",
    "\n",
    "# if tp + fn > 0:\n",
    "#     recall = tp / (tp + fn)\n",
    "# else:\n",
    "#     recall = 0.0\n",
    "\n",
    "# if (precision + recall) > 0:\n",
    "#     f1 = (2 * precision * recall) / (precision + recall)\n",
    "# else:\n",
    "#     f1 = 0.0\n",
    "\n",
    "# print(f\"TP={tp} FP={fp} FN={fn} TN={tn}\")\n",
    "# print(f\"precision={precision:.3f}  recall={recall:.3f}  f1={f1:.3f}\")\n",
    "\n",
    "# matched_patterns = sorted(matched_patterns_global, key=lambda x: (len(x), x))\n",
    "# print(f\"\\nMatched {len(matched_patterns)} discriminative patterns with GT sequences:\")\n",
    "\n",
    "# avg_value_length_tp_discriminative = 0\n",
    "# for p in matched_patterns:\n",
    "#     print(p)\n",
    "#     avg_value_length_tp_discriminative += len(p)\n",
    "\n",
    "# avg_value_length_tp_discriminative = (\n",
    "#     avg_value_length_tp_discriminative / len(matched_patterns)\n",
    "#     if matched_patterns else 0\n",
    "# )\n",
    "\n",
    "# end_mem = process.memory_info().rss / (1024 * 1024)\n",
    "# print(f\"Memory used: {end_mem - start_mem:.2f} MB\")\n",
    "\n",
    "# end_time = time.perf_counter()\n",
    "# elapsed_ms = (end_time - start_time) * 1000\n",
    "# print(f\"\\nTime taken: {elapsed_ms:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classwise_fn = defaultdict(list)\n",
    "# classwise_tp = defaultdict(list)\n",
    "# gt_len = 0\n",
    "\n",
    "# CLASS_LABEL_PATH = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/diag_subseq/subseq/subseq_labels/subseq_class.json\"\n",
    "\n",
    "# with open(CLASS_LABEL_PATH, 'r') as f:\n",
    "#     class_mapping = json.load(f)\n",
    "\n",
    "# for file_fn, file_gt in zip(all_fn, all_gt):\n",
    "#     test_filename = file_gt[0].replace('.json', '')\n",
    "    \n",
    "#     fn = file_fn[1] \n",
    "#     gt = file_gt[1] \n",
    "    \n",
    "#     if test_filename in class_mapping:\n",
    "#         class_ids = class_mapping[test_filename]\n",
    "        \n",
    "#         if len(gt) != len(class_ids):\n",
    "#             print(f\"Mismatch in {test_filename}. GT len: {len(gt)}, Class ID len: {len(class_ids)}\")\n",
    "#             continue\n",
    "\n",
    "#         for i, label in enumerate(gt):\n",
    "#             current_class_id = class_ids[i] \n",
    "#             if label in fn:\n",
    "#                 classwise_fn[current_class_id].append(label)\n",
    "#             else:\n",
    "#                 classwise_tp[current_class_id].append(label)\n",
    "#     else:\n",
    "#         print(f\"{test_filename} not found in class mapping JSON.\")\n",
    "\n",
    "#     gt_len += len(gt)\n",
    "#     # print('file gt:', len(gt))\n",
    "#     # print('file fn:', len(fn))\n",
    "#     # print('\\n')\n",
    "#     # break\n",
    "\n",
    "# total_fn = 0\n",
    "# total_tp = 0\n",
    "# keys = set(list(classwise_fn.keys()) + list(classwise_tp.keys()))\n",
    "# # print('keys:', keys)\n",
    "# class_recall = []\n",
    "# for key in keys:\n",
    "#     print('class:', key)\n",
    "#     total_fn += len(classwise_fn[key])\n",
    "#     total_tp += len(classwise_tp[key])\n",
    "\n",
    "#     crecall = len(classwise_tp[key])/(len(classwise_fn[key])+len(classwise_tp[key]))\n",
    "\n",
    "#     # print('not detected:', len(classwise_fn[key]))\n",
    "#     print('detected:', len(classwise_tp[key]))\n",
    "#     print('total anomalies:', len(classwise_fn[key])+len(classwise_tp[key]))\n",
    "#     print('Recall (classwise):', crecall)\n",
    "#     print('\\n')\n",
    "\n",
    "#     class_recall.append(crecall)\n",
    "\n",
    "\n",
    "# # print('total fn+tp:', total_fn+total_tp)\n",
    "# # print('total gt:', gt_len)\n",
    "# # assert total_fn+total_tp == gt_len, 'total fn+tp not equal to total gt'\n",
    "# print('All class recalls:', class_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d84d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
