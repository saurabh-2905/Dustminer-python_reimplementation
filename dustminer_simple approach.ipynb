{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ce3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dustminer Implementation\n",
    "# Loading the normal data as Good pile and Faulty data as Bad pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from libraries.utils import get_paths, read_traces, read_json, mapint2var, is_consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e01a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CODE = 'theft_protection'               ### application (code) theft_protection, mamba2, lora_ducy\n",
    "BEHAVIOUR_FAULTY = 'faulty_data/diag_subseq/subseq/'        ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal/'             ### normal, faulty_data\n",
    "THREAD = 'single'                       ### single, multi\n",
    "VER = 4                                 ### format of data collection\n",
    "\n",
    "base_dir = '../trace_data'              ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(\"Normal base path:\", normalbase_path)\n",
    "print(\"Faulty base path:\", faultybase_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dcff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base_path = os.path.join(normalbase_path, 'train_data') #'diag_refsamples500')\n",
    "print(\"Train base path:\", train_base_path)\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "train_data_path = [os.path.join(train_base_path, x) for x in os.listdir(train_base_path)]\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in os.listdir(normalbase_path) if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "train_data_path = [x for x in train_data_path if '.DS_Store' not in x]\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "paths_log = [x for x in paths_log if '.DS_Store' not in x]\n",
    "paths_traces = [x for x in paths_traces if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "\n",
    "print(\"Number of training data files:\", len(train_data_path))\n",
    "print(\"Number of training varlist files:\", len(train_varlist_path))\n",
    "print(\"Number of faulty log files:\", len(paths_log))\n",
    "print(\"Number of faulty trace files:\", len(paths_traces))\n",
    "print(\"Number of faulty varlist files:\", len(varlist_path))\n",
    "print(\"Number of faulty label files:\", len(paths_label))\n",
    "\n",
    "paths_log.sort()\n",
    "paths_traces.sort()\n",
    "varlist_path.sort()\n",
    "paths_label.sort()\n",
    "\n",
    "test_data_path = paths_traces\n",
    "test_label_path = paths_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33afd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f87f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file in file_paths:\n",
    "        traces = read_traces(file)\n",
    "        if isinstance(traces, list) and len(traces) <=2:\n",
    "            # id_sequence = [trace for trace in traces]\n",
    "            id_sequence = traces[0]\n",
    "            print(\"id_sequence:\", id_sequence)\n",
    "\n",
    "        elif isinstance(traces, list) and len(traces) > 2:\n",
    "            id_sequence = [int(trace[0]) for trace in traces if isinstance(trace, list) and len(trace) >= 2]\n",
    "        \n",
    "        data.append(id_sequence)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_log_directory = train_data_path\n",
    "bad_log_directory = test_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_log_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_log_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sequences = load_data(good_log_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be98d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9281a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(good_sequences)):\n",
    "    print(f\"Good sequence {i} length: {len(good_sequences[i])}\")\n",
    "    count = count + len(good_sequences[i])\n",
    "print(\"Total good sequences length:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cfa59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sequences = load_data(bad_log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12855792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def get_ground_truth_file(path, ground_truth_files):\n",
    "    filename = []\n",
    "    for file in ground_truth_files:\n",
    "        if Path(path).stem in Path(file).stem:\n",
    "            print(\"from function - matched ground truth file is :\", file)\n",
    "            filename = file\n",
    "    return filename\n",
    "\n",
    "def get_trace_info(path):\n",
    "    filename = Path(path).stem\n",
    "    match = re.search(r\"(trace_trial_?\\d+)_(\\d+)-(\\d+)\", filename)\n",
    "    if match:\n",
    "        name = match.group(1)\n",
    "        start = int(match.group(2))\n",
    "        end = int(match.group(3))\n",
    "        test_data_name = name+'_'+str(start)+'-'+str(end)+'.json'\n",
    "        return name, start, end, test_data_name\n",
    "    else:\n",
    "        raise ValueError(\"Filename format not recognized\")\n",
    "\n",
    "def find_sequence_ground_truth(test_data_path, ground_truth):\n",
    "    trace = read_traces(test_data_path)\n",
    "    name, start, end,test_data_name = get_trace_info(test_data_path)\n",
    "    sequence = [int(ev[0]) for ev in trace if isinstance(ev, list) and len(ev) >= 2]\n",
    "    gt_start_end_pair = [[x[0], x[1]] for x in ground_truth]\n",
    "    return sequence, gt_start_end_pair\n",
    "\n",
    "\n",
    "def create_labels(sequence, gt_start_end_pair, test_data_start_index, test_data_end_index):\n",
    "    start_index = test_data_start_index\n",
    "    end_index = test_data_end_index\n",
    "    event_list = []\n",
    "    event_id_list = []\n",
    "    for start, end in gt_start_end_pair:\n",
    "        event_list = []\n",
    "        for event_id in range(start_index, end_index):\n",
    "            if event_id >= start and event_id <= end:\n",
    "                print(\"Event ID {} is in ground truth range ({}, {})\".format(event_id, start, end))\n",
    "                print(\"event_id - start_index:\", event_id , start_index)\n",
    "                event_list.append(sequence[event_id - start_index])\n",
    "        if event_list:\n",
    "            event_id_list.append(event_list)\n",
    "\n",
    "    return event_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.anomaly_detection import discover_test_files, load_ground_truth_dir, build_labels\n",
    "import json \n",
    " \n",
    "gt_path   = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/labels\" # example\n",
    "ground_truth_path = [os.path.join(gt_path, x) for x in os.listdir(gt_path)]\n",
    "\n",
    "print(\"ground truth path:\", ground_truth_path)\n",
    "\n",
    "new_label = {}\n",
    "\n",
    "for test_data in test_data_path:\n",
    "    print(\"Test data file:\", test_data)\n",
    "\n",
    "\n",
    "    print(\"---------------------------------------------------\")\n",
    "    test_data_name_1, test_data_start_index, test_data_end_index, test_data_name= get_trace_info(test_data)\n",
    "    print(\"Test data name is : \", test_data_name_1)\n",
    "    print(\"Test data start index is : \", test_data_start_index)\n",
    "    print(\"Test data end index is : \", test_data_end_index)\n",
    "\n",
    "\n",
    "    ground_truth_filename = get_ground_truth_file(test_data_name_1, ground_truth_path)\n",
    "    if not ground_truth_filename:\n",
    "        print(\"No matching ground truth file found for test data:\", test_data)\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        print(\"Ground truth file name is : \", ground_truth_filename)\n",
    "\n",
    "\n",
    "        ground_truth_raw = read_traces(ground_truth_filename)                                               # read ground truth labels from the label file\n",
    "        ground_truth = ground_truth_raw['labels']                                                # extract labels from dictionary from ground truth data\n",
    "\n",
    "        label_trace_name = list(ground_truth.keys())[0]\n",
    "        ground_truth = ground_truth[label_trace_name]\n",
    "\n",
    "        print(\"ground truth:\", ground_truth)\n",
    "\n",
    "        print(\"The test data file \", test_data, \" is not corresponding to ground truth file : \", ground_truth_filename)\n",
    "        print(\"The test data file \", test_data, \" is corresponding to ground truth file : \", ground_truth_filename)\n",
    "        sequence, gt_start_end_pair = find_sequence_ground_truth(test_data, ground_truth)\n",
    "        print(\"Event ID sequence is : \", sequence)\n",
    "        print(\"Ground truth start-end pairs are : \", gt_start_end_pair)\n",
    "        labels = create_labels(sequence, gt_start_end_pair, test_data_start_index, test_data_end_index)\n",
    "        print(\"Labels are : \", labels)\n",
    "        new_label[test_data_name] = labels\n",
    "        print(\"New label dictionary is : \", new_label)\n",
    "\n",
    "output_dir = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"gt_test_data_labels.json\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_label, f, indent=4)\n",
    "\n",
    "print(f\"\\n Saved to file: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fab146",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde484f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac416e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_max_length(seq, event_id):\n",
    "    max_length = 0\n",
    "    start_index = seq.index(event_id)\n",
    "    end_index = 0\n",
    "    for i in range(start_index + 1 , len(seq)):\n",
    "        if seq[i] == event_id:\n",
    "            end_index = i - start_index\n",
    "            if end_index > max_length:\n",
    "                max_length = end_index\n",
    "            start_index = i\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def max_gap_two_lists(sequences):\n",
    "    max_length = 0\n",
    "    events = []\n",
    "    event_id_max_length = {}\n",
    "\n",
    "    for seq in sequences:\n",
    "        if seq and isinstance(seq[0], list):\n",
    "            inner_sequences = seq\n",
    "        else:\n",
    "            inner_sequences = [seq]  \n",
    "        for inner_seq in inner_sequences:\n",
    "            for i, x in enumerate(inner_seq):\n",
    "                gap = get_max_length(inner_seq, x)\n",
    "                if x not in event_id_max_length:\n",
    "                    event_id_max_length[x] = gap\n",
    "                    events.append(x)\n",
    "                else:\n",
    "                    if gap > event_id_max_length[x]:\n",
    "                        event_id_max_length[x] = gap\n",
    "\n",
    "                if event_id_max_length[x] > max_length:\n",
    "                    max_length = event_id_max_length[x]\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c14571",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequences_1 = good_sequences\n",
    "\n",
    "MAX_PATTERN_LEN = max_gap_two_lists(all_sequences_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98acd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PATTERN_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_same_index(seq):\n",
    "    \"\"\"\"\n",
    "    This function is used to compute next occurence of every element in the sequence.\n",
    "    parameters :\n",
    "        seq - list(int)\n",
    "    Returns:\n",
    "        nxt - list(int) where nxt[i] is the index of the next occurrence of seq[i] in seq and n if none.\n",
    "    \"\"\"\n",
    "    last_pos = {}\n",
    "    n = len(seq)\n",
    "    nxt = [n] * n\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        v = seq[i]\n",
    "        if v in last_pos:\n",
    "            nxt[i] = last_pos[v]\n",
    "        last_pos[v] = i\n",
    "    return nxt\n",
    "\n",
    "\n",
    "def dynamic_window_sequence(sequences):\n",
    "    \"\"\"\n",
    "    This function generates dynamic windows for each sequence in sequences.\n",
    "    For example, for sequence [1,2,3,1,4,2], the dynamic windows are: [1,2,3],[1,4,2]\n",
    "    parameters :\n",
    "        sequences - list of list(int)\n",
    "    Returns:\n",
    "        final_windows - list of list(int) containing all dynamic windows from all sequences.\n",
    "    \"\"\"\n",
    "    final_windows = []\n",
    "    for seq in sequences:\n",
    "        if not seq:\n",
    "            continue\n",
    "        # print(\"Generating dynamic windows for sequence:\", seq)\n",
    "        nxt = next_same_index(seq)\n",
    "        # print(\"Next same index array:\", nxt)\n",
    "        n = len(seq)\n",
    "        for i in range(n):\n",
    "            j = nxt[i]\n",
    "            if j > i:\n",
    "                final_windows.append(seq[i:j])\n",
    "    return final_windows\n",
    "\n",
    "def has_substring(pattern, sequence):\n",
    "    \"\"\"\n",
    "    This function checks if the given pattern is a subsequence of sequence.\n",
    "    parameters : \n",
    "        pattern - list(int)\n",
    "        sequence - list(int)\n",
    "    Returns:\n",
    "        True if the pattern appears as a continous match in sequence.\n",
    "        False if no match\n",
    "    \"\"\"\n",
    "    pattern = tuple(pattern)\n",
    "    sequence = tuple(sequence)\n",
    "    m = len(pattern)\n",
    "    n = len(sequence)\n",
    "    if m == 0:\n",
    "        return True\n",
    "    if m > n:\n",
    "        return False\n",
    "    for i in range(n - m + 1):\n",
    "        if sequence[i:i+m] == pattern:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# def compress_patterns(patterns):\n",
    "#     \"\"\"\n",
    "#     Compress frequent patterns by removing redundant shorter ones.\n",
    "#     For each pattern P, if there exists a longer pattern Q such that:\n",
    "#         - P appears as a CONTIGUOUS substring of Q, and has support(P) == support(Q), then P is dropped (only Q is kept).\n",
    "\n",
    "#     Parameters:\n",
    "#         patterns (dict[tuple[int, ...], int]): Dictionary mapping each pattern tuple to its support count.\n",
    "\n",
    "#     Returns:\n",
    "#         dict[tuple[int, ...], int]: Compressed dictionary containing only longest unique-support patterns.\n",
    "\n",
    "#     For example:\n",
    "#         Input  : {(6,7):3, (6,7,8):3, (6,8):2}\n",
    "#         Output : {(6,7,8):3, (6,8):2} Because (6,7) is a substring of (6,7,8) and both have same support=3.\n",
    "#     \"\"\"\n",
    "#     items = list(patterns.items())\n",
    "#     items.sort(key=lambda x: (-len(x[0]), x[0]))\n",
    "#     print(items)\n",
    "#     compressed_patterns = {}\n",
    "#     for pattern, support in items:\n",
    "#         drop = False\n",
    "#         for other in compressed_patterns.keys():\n",
    "#             if patterns[other] == support and has_substring(pattern, other):\n",
    "#                 drop = True\n",
    "#                 break\n",
    "#         if not drop:\n",
    "#             compressed_patterns[pattern] = support\n",
    "#     return compressed_patterns\n",
    "\n",
    "\n",
    "def compress_patterns(current, next):\n",
    "    \"\"\"\n",
    "    Compress frequent patterns by removing redundant shorter ones.\n",
    "    For each pattern P, if there exists a longer pattern Q such that:\n",
    "        - P appears as a CONTIGUOUS substring of Q, and has support(P) == support(Q), then P is dropped (only Q is kept).\n",
    "\n",
    "    Parameters:\n",
    "        patterns (dict[tuple[int, ...], int]): Dictionary mapping each pattern tuple to its support count.\n",
    "\n",
    "    Returns:\n",
    "        dict[tuple[int, ...], int]: Compressed dictionary containing only longest unique-support patterns.\n",
    "\n",
    "    For example:\n",
    "        Input  : {(6,7):3, (6,7,8):3, (6,8):2}\n",
    "        Output : {(6,7,8):3, (6,8):2} Because (6,7) is a substring of (6,7,8) and both have same support=3.\n",
    "    \"\"\"\n",
    "    current_pats = list(current.keys())\n",
    "    next_pats = list(next.keys())\n",
    "    compressed_patterns = {}\n",
    "    for pattern in current_pats:\n",
    "        drop = False\n",
    "        for other in next_pats:\n",
    "            # print(\"Comparing pattern:\", pattern, \"with other pattern:\", other)\n",
    "            # print(\"Has substring:\", has_substring(pattern, other))\n",
    "            if pattern == other[:-1]:\n",
    "                # print(\"Pattern\", pattern, \"is a substring of\", other, current[pattern], next[other])\n",
    "                if abs(current[pattern] - next[other])<=0.3:\n",
    "                    # print(\"Droped Pattern\", pattern)\n",
    "                    drop = True\n",
    "                    break\n",
    "        if not drop:\n",
    "            compressed_patterns[pattern] = current[pattern]\n",
    "    return compressed_patterns\n",
    "\n",
    "def check_ordered_support(pattern, windows):\n",
    "    if not windows:\n",
    "        return 0.0\n",
    "\n",
    "    # print(\"pattern from check is : \", pattern)\n",
    "    # print(\"pattern from check is : \", pattern[0])\n",
    "\n",
    "    # match_count = 0\n",
    "    # pattern1 = pattern[1:]\n",
    "    # for window in windows:\n",
    "    #     # print(\"Evaluating window:\", window)\n",
    "    #     # print(\"Evaluating pattern1:\", pattern1)\n",
    "    #     i = 1\n",
    "    #     j = 0\n",
    "    #     while i < len(window) and j < len(pattern1):\n",
    "    #         if window[i] == pattern1[j]:\n",
    "    #             j = j + 1\n",
    "    #         i = i + 1\n",
    "\n",
    "    #     if j == len(pattern1):\n",
    "    #         match_count += 1\n",
    "\n",
    "    ###################\n",
    "    pattern_str = ','.join(map(str, pattern))\n",
    "    # print(\"pattern from check is : \", pattern_str, windows_str)\n",
    "    match_count = 0\n",
    "    for window in windows:\n",
    "        window_str = ','.join(map(str, window))\n",
    "        if pattern_str in window_str:\n",
    "            match_count += 1\n",
    "            # print(\"Matched pattern:\", pattern_str, \"in window:\", window_str)\n",
    "\n",
    "    return round(match_count / len(windows), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5774ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def generate_cartesian_event(sequence, delta=0.1, max_len=10):\n",
    "    seq = sequence\n",
    "    print(f\"Sequence: {seq}\")\n",
    "    print(\"Length:\", len(seq))\n",
    "\n",
    "    dynamic_windows = dynamic_window_sequence([seq]) \n",
    "    # print(\"Dynamic windows generated:\", dynamic_windows)\n",
    "    sorted_dynamic_windows = defaultdict(list)\n",
    "    for _win in dynamic_windows:\n",
    "        key = _win[0]\n",
    "        sorted_dynamic_windows[key].append(_win)\n",
    "\n",
    "    # print(\"sorted_dynamic_windows:\", sorted_dynamic_windows)\n",
    "    seq_length = len(seq)\n",
    "    S1 = list(set(seq))\n",
    "    infile_support = {e: seq.count(e)/seq_length for e in S1}\n",
    "\n",
    "    # print(\"S1 is \", infile_support)\n",
    "    \n",
    "    # min_val = min(infile_support.values())\n",
    "    min_val = 0\n",
    "    max_val = max(infile_support.values())\n",
    "    # print(\"Min and Max values are :\", min_val, max_val)\n",
    "    \n",
    "    normalized_infile_value = {}\n",
    "    if min_val == max_val:\n",
    "        normalized_infile_value = {e: 1.0 for e in infile_support}\n",
    "    else:\n",
    "        for e, v in infile_support.items():\n",
    "            normalized_infile_value[e] = round((v - min_val) / (max_val - min_val), 4)\n",
    "    \n",
    "    # print(\"Normalized S1 is \", normalized_infile_value)\n",
    "    s1_new = {k: v for k, v in normalized_infile_value.items() if v >= delta}\n",
    "    event_id_s1 = list(s1_new.keys())\n",
    "    \n",
    "    print(f\"S1 Generated: {len(s1_new)} items\")\n",
    "    # print(\"S1\", s1_new)\n",
    "    print('event_id_s1:', event_id_s1)\n",
    "\n",
    "    S2_candidates = set()\n",
    "    for a in range(len(event_id_s1)):\n",
    "        for b in range(len(event_id_s1)):\n",
    "            if a != b: \n",
    "                S2_candidates.add((event_id_s1[a], event_id_s1[b]))\n",
    "\n",
    "    all_patterns = {}\n",
    "    s2_with_support = {}\n",
    "    \n",
    "    print(f\"Total S2 candidates: {S2_candidates}\")\n",
    "    for pair in S2_candidates:\n",
    "        # print(\"Evaluating pair:\", pair)\n",
    "        # print(\"Evaluating pair:\", pair[0])\n",
    "        support = check_ordered_support(pair, sorted_dynamic_windows[pair[0]])\n",
    "        if support >= delta:\n",
    "            s2_with_support[pair] = support\n",
    "\n",
    "    # print(\"S2 count is :\", len(s2_with_support))\n",
    "    # print(\"S2: \", s2_with_support)\n",
    "    all_patterns[2] = s2_with_support\n",
    "\n",
    "    current_patterns = s2_with_support\n",
    "    S1_items = list(s1_new.keys())\n",
    "    k = 3\n",
    "\n",
    "    while k<=max_len:\n",
    "        candidates = set()\n",
    "        \n",
    "        for pattern in current_patterns.keys(): \n",
    "            for item in S1_items:\n",
    "                if item not in pattern:\n",
    "                    new_cand = tuple(list(pattern) + [item])\n",
    "                    # print('pattern:', pattern)\n",
    "                    # print(\"Generated new candidate pattern:\", new_cand)\n",
    "                    candidates.add(new_cand)\n",
    "                            \n",
    "        next_patterns = {}\n",
    "        for m in candidates:\n",
    "            # support = check_ordered_support(m, dynamic_windows)\n",
    "            support = check_ordered_support(m, sorted_dynamic_windows[m[0]])\n",
    "            if support >= delta:\n",
    "                next_patterns[m] = support\n",
    "        \n",
    "        if not next_patterns:\n",
    "            break\n",
    "        # print('current patterns:', current_patterns)\n",
    "        # print('next_patterns', next_patterns)\n",
    "        compressed_pattern = compress_patterns(current_patterns, next_patterns)\n",
    "        # print(f\"Compressed S{k} patterns:\", compressed_pattern)\n",
    "\n",
    "        # print(f\"S{k}: \", len(compressed_pattern))\n",
    "        # print(f\"S{k}: \", compressed_pattern)\n",
    "\n",
    "        ### storing compressed pattern of previous length\n",
    "        print('k', k)\n",
    "        print('len of', k-1, len(all_patterns[k-1]))\n",
    "        print('compressed pattern length:', len(compressed_pattern))\n",
    "        all_patterns[k-1] = compressed_pattern\n",
    "        all_patterns[k] = next_patterns\n",
    "        current_patterns = next_patterns\n",
    "        k += 1\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return all_patterns, sorted_dynamic_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b23e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PATTERN_LEN = 10\n",
    "\n",
    "good_sequence_patterns = []\n",
    "for i in range(0, len(good_sequences)):\n",
    "    print(f\"length of {i} good file is : {len(good_sequences[i])}\")\n",
    "    _patterns, _ = generate_cartesian_event(good_sequences[i], delta=0.1, max_len=MAX_PATTERN_LEN)\n",
    "    good_sequence_patterns.append(_patterns)\n",
    "    # break\n",
    "\n",
    "\n",
    "# good_sequence_patterns, sorted_dynamic_windows = generate_cartesian_event(good_sequences[1], delta=0.1, max_len=MAX_PATTERN_LEN)\n",
    "# bad_sequence_patterns = generate_cartesian_event(bad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7406f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (14, 13)\n",
    "y = [14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13]\n",
    "\n",
    "x_str = ','.join(map(str, x))\n",
    "y_str = ','.join(map(str, y))\n",
    "print(x_str, y_str)\n",
    "print(x_str in y_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd09d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sequence_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5439375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sequence_patterns(pattern_list_of_dicts):\n",
    "    all_supports = {}\n",
    "    # print(\"Number of sequences to merge:\", len(pattern_list_of_dicts))\n",
    "    for i in range(len(pattern_list_of_dicts)):\n",
    "        _keys = pattern_list_of_dicts[i].keys()\n",
    "        # print(\"Keys in pattern_list_of_dicts:\", _keys)\n",
    "        for key in _keys:\n",
    "            pattern_dict = pattern_list_of_dicts[i][key]\n",
    "            for pattern, support in pattern_dict.items():\n",
    "                # print('p', pattern, 's', support)\n",
    "                if pattern not in all_supports:\n",
    "                    all_supports[pattern] = []\n",
    "                all_supports[pattern].append(support)\n",
    "    # print(\"All supports collected:\", all_supports)\n",
    "    final_patterns = {}\n",
    "    for pattern, support_list in all_supports.items():\n",
    "        average_val = sum(support_list) / len(support_list)\n",
    "        final_patterns[pattern] = round(average_val, 4)\n",
    "        \n",
    "    ### sort them in dict according to length\n",
    "    pattern_keys = list(final_patterns.keys())\n",
    "    pattern_keys.sort(key=lambda x: len(x))\n",
    "    # print(\"Sorted pattern keys by length:\", pattern_keys)\n",
    "    sorted_patterns = defaultdict(dict)\n",
    "    for key in pattern_keys:\n",
    "        key_len = len(key)\n",
    "        sorted_patterns[key_len][key] = final_patterns[key]\n",
    "\n",
    "    return sorted_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_good_patterns = merge_sequence_patterns(good_sequence_patterns)\n",
    "print(\"total Good Patterns : \", len(merged_good_patterns))\n",
    "print(\"Good Patterns : \", merged_good_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634866e4",
   "metadata": {},
   "source": [
    "### Save Good Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b13daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save good patterns as using pandas dataframe\n",
    "import os\n",
    "from genericpath import isdir\n",
    "import json\n",
    "output_dir = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/normal/\"\n",
    "isdir(output_dir)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"dustminer_train\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_path, \"good_patterns.json\")\n",
    "\n",
    "### convert keys from tuple to string for json serialization\n",
    "# _merged_good_patterns = {str(k): v for k, v in merged_good_patterns.items()}\n",
    "\n",
    "serialize_2 = {}\n",
    "for length, patterns in merged_good_patterns.items():\n",
    "    serialize_1 = {}\n",
    "    for pattern, support in patterns.items():\n",
    "        serialize_1[str(pattern)] = support\n",
    "    serialize_2[length] = serialize_1\n",
    "\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(_merged_good_patterns, f, indent=4)\n",
    "# print(f\"\\n Saved to file: {output_path}\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(serialize_2, f, indent=4)\n",
    "print(f\"\\n Saved to file: {output_path}\")\n",
    "\n",
    "def load_good_patterns(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        patterns_str = json.load(f)\n",
    "    # Convert string keys back to tuple\n",
    "    # patterns = {eval(k): v for k, v in patterns_str.items()}\n",
    "    patterns = defaultdict(dict)\n",
    "    for length, pattern_dict in patterns_str.items():\n",
    "        for pattern_str, support in pattern_dict.items():\n",
    "            pattern_tuple = eval(pattern_str)\n",
    "            length_tuple = int(length)\n",
    "            patterns[length_tuple][pattern_tuple] = support\n",
    "            \n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e1d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _patterns = load_good_patterns(output_path)\n",
    "# print(\"Loaded patterns:\", _patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216ee6d",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c882c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PATTERN_LEN = 10\n",
    "\n",
    "bad_sequence_patterns = []\n",
    "for i in range(0, len(bad_sequences)):\n",
    "    print(f\"length of {i} bad file is : {len(bad_sequences[i])}\")\n",
    "    _patterns, _ = generate_cartesian_event(bad_sequences[i], delta=0.1, max_len=MAX_PATTERN_LEN)\n",
    "    bad_sequence_patterns.append(_patterns)\n",
    "    print('')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ace48",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sequence_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a0b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get Discriminative Patterns\n",
    "# good paterns in merged_good_patterns\n",
    "MAX_PATTERN_LEN = 10\n",
    "\n",
    "# ### get all good patterns without support\n",
    "# good_key_patterns = list(merged_good_patterns.keys())\n",
    "# ### sort good patterns based on length\n",
    "# good_key_patterns.sort(key=lambda x: len(x), reverse=True)\n",
    "# print(\"Total Good Patterns before comparison: \", len(good_key_patterns))\n",
    "\n",
    "for i in range(0, len(bad_sequence_patterns)):\n",
    "    # bad_sequence_patterns = {}\n",
    "    # print(f\"length of {i} bad file is : {len(bad_sequences[i])}\")\n",
    "    # _patterns, _ = generate_cartesian_event(bad_sequences[i], delta=0.1, max_len=MAX_PATTERN_LEN)\n",
    "    _patterns = bad_sequence_patterns[i]\n",
    "\n",
    "    _keys = _patterns.keys()\n",
    "    # print(\"Keys in bad sequence patterns:\", _keys)\n",
    "\n",
    "    #### accumulate all patterns with different lengths\n",
    "    discriminative_patterns_good = []\n",
    "    discriminative_patterns_bad = []\n",
    "    discriminative_support_good = []\n",
    "    discriminative_support_bad = []\n",
    "    for key in _keys:\n",
    "        print(\"Processing patterns of length:\", key)\n",
    "        bad_patterns_dict = _patterns[key]\n",
    "        good_patterns_dict = merged_good_patterns[key]\n",
    "        good_patterns_keys = list(good_patterns_dict.keys())\n",
    "\n",
    "        print(bad_patterns_dict, good_patterns_dict)\n",
    "        for bad_pattern, bad_support in bad_patterns_dict.items():\n",
    "            if bad_pattern not in good_patterns_dict:\n",
    "                # print(\"Discriminative Pattern found:\", bad_pattern)\n",
    "                discriminative_patterns_bad.append(bad_pattern)\n",
    "                discriminative_support_bad.append(bad_support)\n",
    "            else:\n",
    "                good_support = good_patterns_dict[bad_pattern]\n",
    "                if abs(bad_support - good_support) > 0.3:\n",
    "                    # print(\"Discriminative Pattern found with different support:\", bad_pattern)\n",
    "                    discriminative_patterns_bad.append(bad_pattern)\n",
    "                    discriminative_support_bad.append(bad_support)\n",
    "                else:\n",
    "                    pass\n",
    "                good_patterns_keys.remove(bad_pattern)\n",
    "        for good_pattern in good_patterns_keys:\n",
    "            good_support = good_patterns_dict[good_pattern]\n",
    "            # print(\"Discriminative Pattern found in good patterns:\", good_pattern)\n",
    "            discriminative_patterns_good.append(good_pattern)\n",
    "            discriminative_support_good.append(good_support)\n",
    "        # break\n",
    "\n",
    "    ### sort discriminative patterns based on respective support\n",
    "    discriminative_patterns_good_sorted = sorted(zip(discriminative_patterns_good, discriminative_support_good), key=lambda x: x[1], reverse=True)\n",
    "    discriminative_patterns_bad_sorted = sorted(zip(discriminative_patterns_bad, discriminative_support_bad), key=lambda x: x[1], reverse=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47e5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Discriminative Good Patterns:\", discriminative_patterns_good_sorted)\n",
    "print(\"Discriminative Good Supports:\", discriminative_support_good)\n",
    "print(\"Discriminative Bad Patterns:\", discriminative_patterns_bad_sorted)\n",
    "print(\"Discriminative Bad Supports:\", discriminative_support_bad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminative_score = {}\n",
    "\n",
    "# #Here we take all the distinct event patterns from both bad and good logs and store in all_patterns.\n",
    "\n",
    "# if isinstance(merged_good_patterns, list):\n",
    "#     good_seqs = merged_good_patterns[0] \n",
    "# else:\n",
    "#     good_seqs = merged_good_patterns\n",
    "\n",
    "# all_patterns = set(good_seqs.keys())\n",
    "\n",
    "# for d in bad_sequence_patterns:\n",
    "#     all_patterns.update(d.keys())\n",
    "\n",
    "# print(\"all_patterns :\", len(all_patterns))\n",
    "\n",
    "# for pattern in all_patterns:\n",
    "#     support_good = good_seqs.get(pattern, 0.0)\n",
    "#     bad_seq_count = [d[pattern] for d in bad_sequence_patterns if pattern in d]\n",
    "    \n",
    "#     if bad_seq_count:\n",
    "#         support_bad = sum(bad_seq_count) / len(bad_seq_count)\n",
    "#     else:\n",
    "#         support_bad = 0.0\n",
    "\n",
    "#     score = support_good - support_bad\n",
    "#     discriminative_score[pattern] = score\n",
    "\n",
    "# sorted_patterns = sorted(discriminative_score.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "# threshold = 0.30\n",
    "\n",
    "# discriminative_patterns = {}\n",
    "# for pat, score in sorted_patterns:\n",
    "#     if abs(score) >= threshold:\n",
    "#         discriminative_patterns[pat] = {\"support\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debef892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4425c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "start_mem = process.memory_info().rss / (1024 * 1024)  # in MB\n",
    "start_time = time.perf_counter() \n",
    "\n",
    "LABEL_PATH = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/gt_test_data_labels.json\"\n",
    "\n",
    "MIN_COVERAGE = 0.60 \n",
    "\n",
    "def normalize_patterns(discrinimative_obj):\n",
    "    if discrinimative_obj is None:\n",
    "        return []\n",
    "    if isinstance(discrinimative_obj, dict):\n",
    "        seq = discrinimative_obj.keys()\n",
    "    else:\n",
    "        seq = discrinimative_obj\n",
    "\n",
    "    out = []\n",
    "    for p in seq:\n",
    "        if isinstance(p, (list, tuple, set)):\n",
    "            out.append(tuple(p))\n",
    "    return out\n",
    "\n",
    "def normalize_gt_sequences(gt_seq_list):\n",
    "    if not gt_seq_list:\n",
    "        return []\n",
    "    if isinstance(gt_seq_list[0], int):\n",
    "        return [gt_seq_list]\n",
    "    else:\n",
    "        return [seq for seq in gt_seq_list if isinstance(seq, list) and len(seq) > 0]\n",
    "\n",
    "with open(LABEL_PATH, \"r\") as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "discriminative_patterns_seq = normalize_patterns(discriminative_patterns)\n",
    "\n",
    "def check_subsequence(discriminative_pat, gt_seq, MIN_COVERAGE):\n",
    "    m = len(discriminative_pat)\n",
    "    n = len(gt_seq)\n",
    "\n",
    "    if m == 0 or m > n:\n",
    "        return False\n",
    "\n",
    "    for i in range(n - m + 1):\n",
    "        if gt_seq[i:i+m] == list(discriminative_pat):\n",
    "            coverage = m / n\n",
    "            return coverage >= MIN_COVERAGE\n",
    "\n",
    "    return False\n",
    "\n",
    "tp = fp = fn = tn = 0\n",
    "matched_patterns_global = set()\n",
    "\n",
    "all_tp = []         \n",
    "all_fp = []        \n",
    "all_fn = []        \n",
    "all_gt = []        \n",
    "y_true_all = []     \n",
    "y_pred_all = []     \n",
    "\n",
    "label_file_path = LABEL_PATH \n",
    "\n",
    "for test_file_name, raw_gt in label_map.items():\n",
    "    gt_seqs_list = normalize_gt_sequences(raw_gt)\n",
    "\n",
    "    all_gt.append((test_file_name, gt_seqs_list, label_file_path))\n",
    "\n",
    "    correct_pred_file = [] # To store that GT_seq that matched discriminative patterns  \n",
    "    rest_pred_file = [] # stores which patterns did not match gt sequence     \n",
    "    false_neg_file = [] # for fn sequence\n",
    "\n",
    "    total_patterns = len(discriminative_patterns_seq)  # no of patterns mined by dustminer\n",
    "\n",
    "    for gt_seq in gt_seqs_list:\n",
    "        y_true_all.append(1)   # appending 1 as gt label for every sequence is 1 fault present\n",
    "\n",
    "        matched_for_gt = []\n",
    "        for discriminative_patterns_1 in discriminative_patterns_seq:\n",
    "            if check_subsequence(discriminative_patterns_1, gt_seq, MIN_COVERAGE): # check if there is a complete match or atleast 60% match \n",
    "                matched_for_gt.append(discriminative_patterns_1)\n",
    "                matched_patterns_global.add(discriminative_patterns_1) # set of all discriminative_patterns that match at least one gt sequence\n",
    "\n",
    "        matched_count = len(matched_for_gt) # no of patterns that matched this gt sequence\n",
    "\n",
    "        if matched_count == 0: # when no patterns match for this particular gt then its FN\n",
    "            fn += 1\n",
    "            fp += total_patterns   \n",
    "            false_neg_file.append(gt_seq)\n",
    "            rest_pred_file.append((gt_seq, list(discriminative_patterns_seq))) # appending this gt_seq and all patterns as its FN\n",
    "            y_pred_all.append(0)  # no detection so appending 0\n",
    "\n",
    "        else: # when atleast one pattern match for this particular gt then its TP\n",
    "            tp += 1\n",
    "            fp += (total_patterns - matched_count)\n",
    "\n",
    "            fp_for_gt = [\n",
    "                p for p in discriminative_patterns_seq\n",
    "                if p not in matched_for_gt\n",
    "            ]\n",
    "\n",
    "            correct_pred_file.append((gt_seq, matched_for_gt))\n",
    "            rest_pred_file.append((gt_seq, fp_for_gt)) # appending gt and remaining unmatched patterns\n",
    "            y_pred_all.append(1)  # fault detected so appending 1\n",
    "\n",
    "    all_tp.append((test_file_name, correct_pred_file, label_file_path))\n",
    "    all_fp.append((test_file_name, rest_pred_file, label_file_path))\n",
    "    all_fn.append((test_file_name, false_neg_file, label_file_path))\n",
    "\n",
    "if tp + fp > 0:\n",
    "    precision = tp / (tp + fp)\n",
    "else:\n",
    "    precision = 0.0\n",
    "\n",
    "if tp + fn > 0:\n",
    "    recall = tp / (tp + fn)\n",
    "else:\n",
    "    recall = 0.0\n",
    "\n",
    "if (precision + recall) > 0:\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "else:\n",
    "    f1 = 0.0\n",
    "\n",
    "print(f\"TP={tp} FP={fp} FN={fn} TN={tn}\")\n",
    "print(f\"precision={precision:.3f}  recall={recall:.3f}  f1={f1:.3f}\")\n",
    "\n",
    "matched_patterns = sorted(matched_patterns_global, key=lambda x: (len(x), x))\n",
    "print(f\"\\nMatched {len(matched_patterns)} discriminative patterns with GT sequences:\")\n",
    "\n",
    "avg_value_length_tp_discriminative = 0\n",
    "for p in matched_patterns:\n",
    "    print(p)\n",
    "    avg_value_length_tp_discriminative += len(p)\n",
    "\n",
    "avg_value_length_tp_discriminative = (\n",
    "    avg_value_length_tp_discriminative / len(matched_patterns)\n",
    "    if matched_patterns else 0\n",
    ")\n",
    "\n",
    "end_mem = process.memory_info().rss / (1024 * 1024)\n",
    "print(f\"Memory used: {end_mem - start_mem:.2f} MB\")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_ms = (end_time - start_time) * 1000\n",
    "print(f\"\\nTime taken: {elapsed_ms:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "classwise_fn = defaultdict(list)\n",
    "classwise_tp = defaultdict(list)\n",
    "gt_len = 0\n",
    "\n",
    "CLASS_LABEL_PATH = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/diag_subseq/subseq/subseq_labels/subseq_class.json\"\n",
    "\n",
    "with open(CLASS_LABEL_PATH, 'r') as f:\n",
    "    class_mapping = json.load(f)\n",
    "\n",
    "for file_fn, file_gt in zip(all_fn, all_gt):\n",
    "    test_filename = file_gt[0].replace('.json', '')\n",
    "    \n",
    "    fn = file_fn[1] \n",
    "    gt = file_gt[1] \n",
    "    \n",
    "    if test_filename in class_mapping:\n",
    "        class_ids = class_mapping[test_filename]\n",
    "        \n",
    "        if len(gt) != len(class_ids):\n",
    "            print(f\"Mismatch in {test_filename}. GT len: {len(gt)}, Class ID len: {len(class_ids)}\")\n",
    "            continue\n",
    "\n",
    "        for i, label in enumerate(gt):\n",
    "            current_class_id = class_ids[i] \n",
    "            if label in fn:\n",
    "                classwise_fn[current_class_id].append(label)\n",
    "            else:\n",
    "                classwise_tp[current_class_id].append(label)\n",
    "    else:\n",
    "        print(f\"{test_filename} not found in class mapping JSON.\")\n",
    "\n",
    "    gt_len += len(gt)\n",
    "    # print('file gt:', len(gt))\n",
    "    # print('file fn:', len(fn))\n",
    "    # print('\\n')\n",
    "    # break\n",
    "\n",
    "total_fn = 0\n",
    "total_tp = 0\n",
    "keys = set(list(classwise_fn.keys()) + list(classwise_tp.keys()))\n",
    "# print('keys:', keys)\n",
    "class_recall = []\n",
    "for key in keys:\n",
    "    print('class:', key)\n",
    "    total_fn += len(classwise_fn[key])\n",
    "    total_tp += len(classwise_tp[key])\n",
    "\n",
    "    crecall = len(classwise_tp[key])/(len(classwise_fn[key])+len(classwise_tp[key]))\n",
    "\n",
    "    # print('not detected:', len(classwise_fn[key]))\n",
    "    print('detected:', len(classwise_tp[key]))\n",
    "    print('total anomalies:', len(classwise_fn[key])+len(classwise_tp[key]))\n",
    "    print('Recall (classwise):', crecall)\n",
    "    print('\\n')\n",
    "\n",
    "    class_recall.append(crecall)\n",
    "\n",
    "\n",
    "# print('total fn+tp:', total_fn+total_tp)\n",
    "# print('total gt:', gt_len)\n",
    "# assert total_fn+total_tp == gt_len, 'total fn+tp not equal to total gt'\n",
    "print('All class recalls:', class_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3841073",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b08f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_seq_path = (f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/diag_subseq/subseq\")\n",
    "\n",
    "def load_detection_seq(json_path):\n",
    "    \"\"\"\n",
    "    load the test data file from the path mentioned in detection_seq_path. The format of data is [event_id, timestamp] and we take only event_id.\n",
    "    Parameters: json_path -> string\n",
    "    Returns: event_ids -> [6,7,8,6,7]\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if isinstance(data, list) and len(data) > 0 and isinstance(data[0], list):\n",
    "        return [row[0] for row in data]\n",
    "\n",
    "    return []\n",
    "\n",
    "detection_seq_map = {}\n",
    "\n",
    "# Iterating over the gt_labels file we created with test_file name\n",
    "for test_file_name in label_map.keys():                         # label_map is the GT labels we have created\n",
    "    detection_path = os.path.join(detection_seq_path, test_file_name)   # appending the test_filename to detection_path to take the complete event_d seq for that file\n",
    "    if os.path.exists(detection_path):\n",
    "        detection_seq_map[test_file_name] = load_detection_seq(detection_path)  # Filters out the timestamp and returns only event_ids as lists\n",
    "    else:\n",
    "        detection_seq_map[test_file_name] = []\n",
    "\n",
    "def count_occurrences(subseq, pattern):\n",
    "    \"\"\"\n",
    "    This function returns the number of occurences of a pattern in the larger sequence of event_id. Here pattern is the gt_seq.\n",
    "    Parameters: subseq -> list[int] [6, 7, 8, 6, 7, 8, 9]\n",
    "                pattern -> list[int] [6, 7, 8]\n",
    "    Returns: int -> count of pattern in subseq\n",
    "    \"\"\"\n",
    "    if subseq is None:\n",
    "        return None\n",
    "    count = 0\n",
    "    L = len(pattern)\n",
    "    for i in range(len(subseq) - L + 1):\n",
    "        if subseq[i:i+L] == pattern:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def analyze_fault(gt_seqs, detection_subseq, matched_patterns):\n",
    "    results = []\n",
    "\n",
    "    matched_set = {tuple(p) for p in matched_patterns}\n",
    "\n",
    "    for gt in gt_seqs:\n",
    "        gt_tuple = tuple(gt)\n",
    "        occ = count_occurrences(detection_subseq, gt)\n",
    "\n",
    "        status = \"TP\" if gt_tuple in matched_set else \"FN\"\n",
    "\n",
    "        results.append({\n",
    "            \"gt_seq\": gt,\n",
    "            \"occurrences_in_detection\": occ,\n",
    "            \"status\": status\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "fault_details = []\n",
    "\n",
    "# all_gt is of format \n",
    "# [('trace_trial1_1090-1140.json', [[12, 6, 7, 8, 9, 6]], './trace_data/theft_protection/single_thread/version_4/faulty_data/gt_test_data_labels.json'),\n",
    "\n",
    "for file_name, gt_seqs_list, label_file_path in all_gt:\n",
    "    detection_subseq = detection_seq_map.get(file_name, [])\n",
    "    results = analyze_fault(gt_seqs_list, detection_subseq, matched_patterns_global) # matched_patterns_global is the set of all discriminative pattern that matched at least one GT sequence\n",
    "    fault_details.append((file_name, results))\n",
    "\n",
    "# for file_name, result_list in fault_details:\n",
    "#     print(f\"\\nFile: {file_name}\")\n",
    "#     for r in result_list:\n",
    "#         print(r)\n",
    "\n",
    "all_entries = []\n",
    "for file_name, results in fault_details:\n",
    "    for r in results:\n",
    "        all_entries.append(r)\n",
    "\n",
    "fn_list = []\n",
    "tp_list = []\n",
    "\n",
    "for i in all_entries:\n",
    "    if i[\"status\"] == \"FN\" and i[\"occurrences_in_detection\"] is not None:\n",
    "        fn_list.append(i)\n",
    "    elif i[\"status\"] == \"TP\" and i[\"occurrences_in_detection\"] is not None:\n",
    "        tp_list.append(i)\n",
    "\n",
    "fn_list_with_single_occurence = []\n",
    "tp_list_with_more_occurence = []\n",
    "\n",
    "for i in fn_list:\n",
    "    if i[\"occurrences_in_detection\"] == 1:\n",
    "        fn_list_with_single_occurence.append(i)\n",
    "\n",
    "for i in tp_list:\n",
    "    if i[\"occurrences_in_detection\"] >= 2:\n",
    "        tp_list_with_more_occurence.append(i)\n",
    "\n",
    "print(f\"Total GT sequences with detection : {len(all_entries)}\")\n",
    "print(f\"TP count : {len(tp_list)}\")\n",
    "print(f\"FN count : {len(fn_list)}\")\n",
    "print(f\"FN with exactly 1 occurrence : {len(fn_list_with_single_occurence)}\")\n",
    "print(f\"TP with more than 1 occurrences : {len(tp_list_with_more_occurence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d84d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
