{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ce3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dustminer Implementation\n",
    "# Loading the normal data as Good pile and Faulty data as Bad pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from libraries.utils import get_paths, read_traces, read_json, mapint2var, is_consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e01a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CODE = 'theft_protection'               ### application (code) theft_protection, mamba2, lora_ducy\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'        ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'             ### normal, faulty_data\n",
    "THREAD = 'single'                       ### single, multi\n",
    "VER = 3                                 ### format of data collection\n",
    "\n",
    "base_dir = './trace_data'              ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(\"Normal base path:\", normalbase_path)\n",
    "print(\"Faulty base path:\", faultybase_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dcff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base_path = os.path.join(normalbase_path, 'train_data')\n",
    "print(\"Train base path:\", train_base_path)\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "train_data_path = [os.path.join(train_base_path, x) for x in os.listdir(train_base_path)]\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in os.listdir(normalbase_path) if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "train_data_path = [x for x in train_data_path if '.DS_Store' not in x]\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "paths_log = [x for x in paths_log if '.DS_Store' not in x]\n",
    "paths_traces = [x for x in paths_traces if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "\n",
    "paths_log.sort()\n",
    "paths_traces.sort()\n",
    "varlist_path.sort()\n",
    "paths_label.sort()\n",
    "\n",
    "test_data_path = paths_traces\n",
    "test_label_path = paths_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8974bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check consistency\n",
    "if VER == 3:\n",
    "    check_con, _ = is_consistent([train_varlist_path[0]] + varlist_path)\n",
    "    if check_con:\n",
    "        to_number = read_json(varlist_path[0])\n",
    "        from_number = mapint2var(to_number)\n",
    "    else:\n",
    "        to_number = read_json(train_varlist_path[0])\n",
    "        from_number = mapint2var(to_number)\n",
    "\n",
    "sorted_keys = list(from_number.keys())\n",
    "sorted_keys.sort()\n",
    "var_list = [from_number[key] for key in sorted_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e10e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the event sequences\n",
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file in file_paths:\n",
    "        traces = read_traces(file)\n",
    "        if isinstance(traces, list):\n",
    "            id_sequence = [int(trace[0]) for trace in traces if isinstance(trace, list) and len(trace) >= 2]\n",
    "            data.append(id_sequence)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "MIN_SUP = 2\n",
    "SEGMENT_WIDTH = 50\n",
    "TOP_K_SEGMENTS = 5\n",
    "NORMALIZE_SUPPORT = True\n",
    "\n",
    "good_log_directory = train_data_path\n",
    "bad_log_directory = test_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_log_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_log_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sequences = load_data(good_log_directory)\n",
    "bad_sequences = load_data(bad_log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844dfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def max_gap_two_lists(sequences):\n",
    "    \"\"\"\n",
    "    Two-list method:\n",
    "      - events: ordered list of unique event IDs (first-seen order)\n",
    "      - main:   global best gap per event across all sequences\n",
    "      - seq_best (2nd list): per-sequence best gap; on each repeat of an event,\n",
    "                             update seq_best[event] and then propagate to main[event]\n",
    "    Returns (events, main, global_max)\n",
    "    \"\"\"\n",
    "    events = []          # unique event IDs in first-seen order\n",
    "    idx_of = {}          # event -> index into lists\n",
    "    main = []            # global best gap per event\n",
    "\n",
    "    for seq in sequences:\n",
    "        # per-sequence working state (the \"second list\" you described)\n",
    "        seq_best = [0] * len(events)   # best gaps within this sequence\n",
    "        last_seen = [-1] * len(events) # last index in this sequence\n",
    "\n",
    "        for i, x in enumerate(seq):\n",
    "            if x not in idx_of:\n",
    "                # first time we ever see this event: add a new slot everywhere\n",
    "                idx_of[x] = len(events)\n",
    "                events.append(x)\n",
    "                main.append(0)\n",
    "                seq_best.append(0)\n",
    "                last_seen.append(-1)\n",
    "\n",
    "            k = idx_of[x]\n",
    "            if last_seen[k] != -1:\n",
    "                gap = i - last_seen[k]       # distance between consecutive occurrences\n",
    "                if gap > seq_best[k]:\n",
    "                    seq_best[k] = gap        # update 2nd list\n",
    "                if seq_best[k] > main[k]:\n",
    "                    main[k] = seq_best[k]    # propagate to main list\n",
    "\n",
    "            last_seen[k] = i                 # update last position for this event\n",
    "\n",
    "        # (seq_best is reset fresh for each sequence by reinitializing above)\n",
    "\n",
    "    global_max = max(main) if main else 0\n",
    "    return events, main, global_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b739372",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequences_1 = good_sequences + bad_sequences\n",
    "event_list, corr_event, MAX_PATTERN_LEN = max_gap_two_lists(all_sequences_1)\n",
    "# MAX_PATTERN_LEN = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e200cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PATTERN_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c514c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a72ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ba59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def is_subsequence(small, big):\n",
    "    it = iter(big)\n",
    "    return all(c in it for c in small)\n",
    "\n",
    "def windows_dynamic(seq, e1):\n",
    "    \"\"\"\n",
    "    dynamic windows:\n",
    "    for each occurrence of e1 at index i, window = [i, next_e1_index) or [i, len(seq))\n",
    "    \"\"\"\n",
    "    idxs = [i for i, x in enumerate(seq) if x == e1]\n",
    "    for k, i in enumerate(idxs):\n",
    "        j = idxs[k+1] if k+1 < len(idxs) else len(seq)\n",
    "        yield i, j\n",
    "\n",
    "def pattern_in_window(seq, s, e, pattern):\n",
    "    \"\"\"\n",
    "    Check if the pattern appears as an ordered subsequence inside seq[s:e].\n",
    "    \"\"\"\n",
    "    it = iter(seq[s:e])\n",
    "    return all(p in it for p in pattern)\n",
    "\n",
    "def support_dynamic(sequences, pattern):\n",
    "    \"\"\"\n",
    "    Support counted via dynamic windows keyed by the pattern's first item.\n",
    "    \"\"\"\n",
    "    if not pattern:\n",
    "        return 0\n",
    "    e1 = pattern[0]\n",
    "    supp = 0\n",
    "    for seq in sequences:\n",
    "        for s, e in windows_dynamic(seq, e1):\n",
    "            if pattern_in_window(seq, s, e, pattern):\n",
    "                supp += 1\n",
    "    return supp\n",
    "\n",
    "def seq_pattern_mining(p, q):\n",
    "    return p + (q[-1],) if p[1:] == q[:-1] else None\n",
    "\n",
    "def subseqs_kminus1(pat):\n",
    "    k = len(pat)\n",
    "    for i in range(k):\n",
    "        yield pat[:i] + pat[i+1:]\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_frequent_patterns(sequences, min_sup, max_len=27):\n",
    "    \"\"\"\n",
    "    Frequent patterns with dynamic windows, printed step-by-step.\n",
    "    Candidate generation uses combinations-only:\n",
    "      - k=2: (a,b) with a < b\n",
    "      - k>=3: Apriori join with increasing constraint (p[1:]==q[:-1] and p[-1] < q[-1])\n",
    "    Returns: {pattern(tuple): support(int)}\n",
    "    \"\"\"\n",
    "\n",
    "    item_counts = defaultdict(int)\n",
    "    for seq in sequences:\n",
    "        for e in seq:\n",
    "            item_counts[(e,)] += 1\n",
    "\n",
    "    print(\"Step 1 length=1: counts\")\n",
    "    print(\"  \" + \", \".join(f\"{str(p[0])} - {c}\" for p, c in sorted(item_counts.items())))\n",
    "    fre_pattern = {p for p, c in item_counts.items() if c >= min_sup}\n",
    "    supports = {p: item_counts[p] for p in fre_pattern}\n",
    "    print(f\"S1 = {{{', '.join(str(p[0]) for p in sorted(fre_pattern))}}}\\n\")\n",
    "\n",
    "    k = 2\n",
    "    while fre_pattern and k <= max_len:\n",
    "        candidates = set()\n",
    "\n",
    "        if k == 2:\n",
    "            singles = sorted(p[0] for p in fre_pattern)\n",
    "            for i in range(len(singles)):\n",
    "                for j in range(i + 1, len(singles)):\n",
    "                    candidates.add((singles[i], singles[j]))\n",
    "        else:\n",
    "            prev = sorted(fre_pattern)\n",
    "            for p in prev:\n",
    "                for q in prev:\n",
    "                    if p[1:] == q[:-1] and p[-1] < q[-1]:\n",
    "                        cand = p + (q[-1],)\n",
    "                        if all(sub in fre_pattern for sub in subseqs_kminus1(cand)):\n",
    "                            candidates.add(cand)\n",
    "\n",
    "        print(f\"Step {k} length={k}: sets\")  \n",
    "        if not candidates:\n",
    "            print(\"  no sets, stopping.\\n\")\n",
    "            break\n",
    "        print(\"  \" + \", \".join(\"-\".join(map(str, c)) for c in sorted(candidates)))\n",
    "\n",
    "        counts = {cand: support_dynamic(sequences, cand) for cand in sorted(candidates)}\n",
    "        print(\"  support value: \" + \", \".join(f\"{'-'.join(map(str, c))} – {v}\" for c, v in counts.items()))\n",
    "\n",
    "        fre_pattern = {p for p, c in counts.items() if c >= min_sup}\n",
    "        if fre_pattern:\n",
    "            supports.update({p: counts[p] for p in fre_pattern})\n",
    "            print(f\"S{k} = {{{', '.join('-'.join(map(str, p)) for p in sorted(fre_pattern))}}}\\n\")\n",
    "        else:\n",
    "            print(f\"no patterns satisfy minSup ({min_sup}) at length {k}\\n\")\n",
    "            break\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    # printing the final frequent patterns\n",
    "    print(\"Final frequent patterns:\")\n",
    "    by_len = defaultdict(list)\n",
    "    for p, c in supports.items():\n",
    "        by_len[len(p)].append((p, c))\n",
    "    for L in sorted(by_len):\n",
    "        items = \", \".join(\"-\".join(map(str, p)) for p, _ in sorted(by_len[L]))\n",
    "        print(f\"Length {L}: {items}\")\n",
    "\n",
    "    return supports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c890f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_subsequence(small, big):\n",
    "    it = iter(big)\n",
    "    return all(c in it for c in small)\n",
    "\n",
    "def compress_patterns(patterns):\n",
    "    \"\"\"\n",
    "    Remove redundant subsequences from a dictionary of patterns.\n",
    "    This function keeps only the longest unique patterns and removes those that are subsequences of another pattern with the same support. \n",
    "    This avoids duplication in the frequent pattern set.\n",
    "    Parameters - patterns : dict - A dictionary where keys are patterns as tuples and values are their support counts (int).\n",
    "    Returns - dict - A compressed dictionary of patterns, with redundant subsequences removed.\n",
    "    eg - patterns = {\n",
    "    (1, 2): 3,\n",
    "    (1, 2, 3): 3,\n",
    "    (2, 3): 2\n",
    "    }\n",
    "    output - {(1, 2, 3): 3, (2, 3): 2}\n",
    "    \"\"\"\n",
    "    compressed = {}\n",
    "    # Sort patterns by descending length\n",
    "    for pattern, support in sorted(patterns.items(), key=lambda x: (-len(x[0]), x[0])):\n",
    "        is_subseq = False\n",
    "        # If pattern is a subsequence of an already kept pattern with same support then we skip it\n",
    "        for other_pattern in compressed:\n",
    "            if is_subsequence(pattern, other_pattern) and patterns[other_pattern] == support:\n",
    "                is_subseq = True\n",
    "                break\n",
    "        # Keeping the pattern if not redundant    \n",
    "        if not is_subseq:\n",
    "            compressed[pattern] = support\n",
    "    return compressed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb98077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_full_counts(sequences, max_len):\n",
    "    return generate_frequent_patterns(sequences, min_sup=1, max_len=max_len)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize(patterns, sequences, max_len=27):\n",
    "    \"\"\"\n",
    "    Chain-rule normalization:\n",
    "      len=1: P(e) = count(e) / total_events\n",
    "      len>1: P(e1..ek) = P(e1) * Π P(e_{i+1} | e1..ei),\n",
    "                         with P(next|prefix) = count(prefix+next) / Σ_x count(prefix+x)\n",
    "    Returns {pattern: probability}\n",
    "    \"\"\"\n",
    "    full_counts = build_full_counts(sequences, max_len=max_len)\n",
    "    total_events = sum(len(s) for s in sequences) or 1\n",
    "\n",
    "    prefix_out_sum = defaultdict(int)\n",
    "    for q, c in full_counts.items():\n",
    "        if len(q) >= 2:\n",
    "            prefix = q[:-1]\n",
    "            prefix_out_sum[prefix] += c\n",
    "\n",
    "    def supp(p): \n",
    "        return full_counts.get(p, 0)\n",
    "\n",
    "    probs = {}\n",
    "    for pat in patterns:\n",
    "        k = len(pat)\n",
    "        if k == 1:\n",
    "            probs[pat] = supp(pat) / total_events\n",
    "            continue\n",
    "\n",
    "        p = supp((pat[0],)) / total_events\n",
    "        if p == 0.0:\n",
    "            probs[pat] = 0.0\n",
    "            continue\n",
    "\n",
    "        is_success = True\n",
    "        for i in range(1, k):\n",
    "            prefix = pat[:i]\n",
    "            num = supp(pat[:i+1])\n",
    "            den = prefix_out_sum.get(prefix, 0)\n",
    "            if num == 0 or den == 0:\n",
    "                p = 0.0\n",
    "                is_success = False\n",
    "                break\n",
    "            p *= (num / den)\n",
    "        probs[pat] = p if is_success else 0.0\n",
    "    return probs\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f203a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE_SUPPORT = True \n",
    "\n",
    "def mine_discriminative_patterns(good_seqs, bad_seqs, min_sup):\n",
    "    \"\"\"\n",
    "    Sequential (paper-style) discriminative mining:\n",
    "      - Mine frequent ordered patterns with dynamic windows in good & bad\n",
    "      - Sequence-compress (drop redundant subseqs with same support)\n",
    "      - normalize per pile\n",
    "      - Return patterns with higher prominence in bad than good\n",
    "    \"\"\"\n",
    "    # Stage-1 frequent patterns\n",
    "    good_patterns = generate_frequent_patterns(good_seqs, min_sup, max_len=MAX_PATTERN_LEN)\n",
    "    bad_patterns  = generate_frequent_patterns(bad_seqs,  min_sup, max_len=MAX_PATTERN_LEN)\n",
    "\n",
    "    # Compression\n",
    "    good_patterns = compress_patterns(good_patterns)\n",
    "    bad_patterns  = compress_patterns(bad_patterns)\n",
    "\n",
    "    # Normalization\n",
    "    if NORMALIZE_SUPPORT:\n",
    "        good_patterns = normalize(good_patterns, good_seqs, max_len=MAX_PATTERN_LEN)\n",
    "        bad_patterns  = normalize(bad_patterns,  bad_seqs,  max_len=MAX_PATTERN_LEN)\n",
    "\n",
    "    # Discriminative diff\n",
    "    discriminative = {}\n",
    "    for pattern, bad_val in bad_patterns.items():\n",
    "        good_val = good_patterns.get(pattern, 0)\n",
    "        diff = bad_val - good_val\n",
    "        if diff > 0:\n",
    "            discriminative[pattern] = {'bad': bad_val, 'good': good_val, 'delta': diff}\n",
    "\n",
    "    return discriminative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c8a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENT_WIDTH = 50\n",
    "TOP_K_SEGMENTS = 5\n",
    "\n",
    "def segment_log(sequence, width):\n",
    "    return [sequence[i:i+width] for i in range(0, len(sequence), width)]\n",
    "\n",
    "def _score_segment(seg, discr_keys):\n",
    "    return sum(1 for p in discr_keys if is_subsequence(p, seg))\n",
    "\n",
    "def segment_and_mine(bad_seqs, discriminative_patterns, seg_width=SEGMENT_WIDTH, top_k=TOP_K_SEGMENTS, max_len=MAX_PATTERN_LEN):\n",
    "    discr_keys = list(discriminative_patterns.keys()) if isinstance(discriminative_patterns, dict) else list(discriminative_patterns)\n",
    "\n",
    "    segments, scores = [], []\n",
    "    for seq in bad_seqs:\n",
    "        for seg in segment_log(seq, seg_width):\n",
    "            segments.append(seg)\n",
    "            scores.append(_score_segment(seg, discr_keys))\n",
    "\n",
    "    if not segments:\n",
    "        return {}\n",
    "\n",
    "    top_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "    top_segments = [segments[i] for i in top_idx]\n",
    "\n",
    "    raw = generate_frequent_patterns(top_segments, min_sup=1, max_len=max_len)\n",
    "    return compress_patterns(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5de8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminative_patterns = mine_discriminative_patterns(good_sequences, bad_sequences, MIN_SUP)\n",
    "# discriminative_patterns = mine_discriminative_patterns(good_20, bad_20, min_sup=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminative_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad88606",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_patterns = segment_and_mine(bad_sequences, discriminative_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d507d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da3bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([\n",
    "    {'Pattern': pattern, 'Bad Support': v['bad'], 'Good Support': v['good'], 'Difference': v['delta']}\n",
    "    for pattern, v in sorted(discriminative_patterns.items(), key=lambda x: -x[1]['delta'])\n",
    "])\n",
    "\n",
    "df2 = pd.DataFrame([\n",
    "    {'Pattern': pattern, 'Support': support}\n",
    "    for pattern, support in sorted(stage2_patterns.items(), key=lambda x: -x[1])\n",
    "])\n",
    "\n",
    "from IPython.display import display\n",
    "print(\"Stage 1: Discriminative Patterns\")\n",
    "display(df1)\n",
    "\n",
    "print(\"\\nStage 2: Infrequent Root-Cause Patterns (from top-K segments)\")\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c5fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def test_single(file_path):\n",
    "    detections = []\n",
    "\n",
    "    # Read the test trace (list of [event_id, timestamp])\n",
    "    trace = read_traces(file_path)\n",
    "    sequence = [int(ev[0]) for ev in trace if isinstance(ev, list) and len(ev) >= 2]\n",
    "    timestamps = [int(ev[1]) for ev in trace if isinstance(ev, list) and len(ev) >= 2]\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    pattern_list = list(discriminative_patterns.keys()) + list(stage2_patterns.keys())\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(len(sequence)):\n",
    "        for pattern in pattern_list:\n",
    "            pattern_len = len(pattern)\n",
    "            if i + pattern_len <= len(sequence) and tuple(sequence[i:i+pattern_len]) == pattern:\n",
    "                start_idx = i\n",
    "                end_idx = i + pattern_len - 1\n",
    "                start_ts = timestamps[start_idx]\n",
    "                end_ts = timestamps[end_idx]\n",
    "                detections.append([\n",
    "                    (start_idx, end_idx),\n",
    "                    (start_ts, end_ts),\n",
    "                    filename\n",
    "                ])\n",
    "\n",
    "    inference_time = (time.time() - start_time) * 1000 \n",
    "\n",
    "    return detections, inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b20970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.anomaly_detection import merge_detections, get_correct_detections\n",
    "\n",
    "\n",
    "## checking the detections against the ground truth\n",
    "DIFF_VAL = 0 \n",
    "all_detections = []         # To store detections for each file\n",
    "y_pred_all = []             # To store the predicted labels\n",
    "y_true_all = []             # To store the ground truth labels\n",
    "all_tp = []                 # To store all true positives\n",
    "all_fp = []                 # To store all false positives\n",
    "all_fn = []                 # To store all false negatives\n",
    "all_gt = []                 # To store the ground truth\n",
    "\n",
    "\n",
    "# Iterating through each test data file and label file\n",
    "for test_data, test_label in zip(test_data_path, test_label_path):\n",
    "    detection, inference_time = test_single(test_data)            # Detecting anomalies in the test data\n",
    "    print(\"Detection : \", detection)\n",
    "\n",
    "    all_detections.append((test_data, detection, test_label))\n",
    "    merge_detection, agg_ts = merge_detections(detection, diff_val=DIFF_VAL)\n",
    "\n",
    "    print(\"Merge detection : \", merge_detection)\n",
    "    \n",
    "    ground_truth_raw = read_traces(test_label)                                               # read ground truth labels from the label file\n",
    "    ground_truth = ground_truth_raw['labels']                                                # extract labels from dictionary from ground truth data\n",
    "\n",
    "    label_trace_name = list(ground_truth.keys())[0]\n",
    "    ground_truth = ground_truth[label_trace_name]\n",
    "\n",
    "    correct_pred, rest_pred, y_pred, y_true, false_neg = get_correct_detections(merge_detection, ground_truth)  # Comparing detected anomaly with ground truth\n",
    "\n",
    "    y_pred_all.extend(y_pred)          # predicted labels\n",
    "    y_true_all.extend(y_true)          # actual ground truth labels\n",
    "    all_tp.append((test_data, correct_pred, test_label))\n",
    "    all_fp.append((test_data, rest_pred, test_label))\n",
    "    all_fn.append((test_data, false_neg, test_label))\n",
    "    all_gt.append((test_data, ground_truth, test_label))\n",
    "\n",
    "    print(\"Inference time : \", (inference_time/32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a080cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred_all = np.array(y_pred_all)\n",
    "y_true_all = np.array(y_true_all)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision = precision_score(y_true_all, y_pred_all)\n",
    "recall = recall_score(y_true_all, y_pred_all)\n",
    "f1 = f1_score(y_true_all, y_pred_all)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
