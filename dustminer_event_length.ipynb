{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ce3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dustminer Implementation\n",
    "# Loading the normal data as Good pile and Faulty data as Bad pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from libraries.utils import get_paths, read_traces, read_json, mapint2var, is_consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e01a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CODE = 'mamba2'               ### application (code) theft_protection, mamba2, lora_ducy\n",
    "BEHAVIOUR_FAULTY = 'faulty_data/diag_subseq/subseq/'        ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal/'             ### normal, faulty_data\n",
    "THREAD = 'single'                       ### single, multi\n",
    "VER = 4                                 ### format of data collection\n",
    "\n",
    "base_dir = './trace_data'              ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(\"Normal base path:\", normalbase_path)\n",
    "print(\"Faulty base path:\", faultybase_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dcff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base_path = os.path.join(normalbase_path, 'train_data') #'diag_refsamples500')\n",
    "print(\"Train base path:\", train_base_path)\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "train_data_path = [os.path.join(train_base_path, x) for x in os.listdir(train_base_path)]\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in os.listdir(normalbase_path) if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "train_data_path = [x for x in train_data_path if '.DS_Store' not in x]\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "paths_log = [x for x in paths_log if '.DS_Store' not in x]\n",
    "paths_traces = [x for x in paths_traces if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "\n",
    "paths_log.sort()\n",
    "paths_traces.sort()\n",
    "varlist_path.sort()\n",
    "paths_label.sort()\n",
    "\n",
    "test_data_path = paths_traces\n",
    "test_label_path = paths_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f87f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8974bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check consistency\n",
    "# if VER == 3:\n",
    "#     check_con, _ = is_consistent([train_varlist_path[0]] + varlist_path)\n",
    "#     if check_con:\n",
    "#         to_number = read_json(varlist_path[0])\n",
    "#         from_number = mapint2var(to_number)\n",
    "#     else:\n",
    "#         to_number = read_json(train_varlist_path[0])\n",
    "#         from_number = mapint2var(to_number)\n",
    "\n",
    "# sorted_keys = list(from_number.keys())\n",
    "# sorted_keys.sort()\n",
    "# var_list = [from_number[key] for key in sorted_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file in file_paths:\n",
    "        traces = read_traces(file)\n",
    "        if isinstance(traces, list) and len(traces) <=2:\n",
    "            # id_sequence = [trace for trace in traces]\n",
    "            id_sequence = traces[0]\n",
    "            print(\"id_sequence:\", id_sequence)\n",
    "\n",
    "        elif isinstance(traces, list) and len(traces) > 2:\n",
    "            id_sequence = [int(trace[0]) for trace in traces if isinstance(trace, list) and len(trace) >= 2]\n",
    "        \n",
    "        data.append(id_sequence)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "MIN_SUP = 2\n",
    "SEGMENT_WIDTH = 50\n",
    "TOP_K_SEGMENTS = 5\n",
    "NORMALIZE_SUPPORT = True\n",
    "\n",
    "good_log_directory = train_data_path\n",
    "bad_log_directory = test_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_log_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_log_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sequences_original = load_data(good_log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be98d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_sequences_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_sequences = [[1,2,3,4,5,6,7,8],[10,11,12,13,14,15],[20,21,22,23,24,25,26],[30,31,32,33,34,35,36,37,38]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1256d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_id_length = 25000\n",
    "\n",
    "remaining_count = event_id_length\n",
    "good_sequences = []\n",
    "for i in range(len(good_sequences_original)):\n",
    "    print(f\"Good sequence {i} length: {len(good_sequences_original[i])}\")\n",
    "    if len(good_sequences_original[i]) > remaining_count:\n",
    "        good_sequences.append(good_sequences_original[i][:remaining_count])\n",
    "        break\n",
    "    else:\n",
    "        good_sequences.append(good_sequences_original[i])\n",
    "        remaining_count = remaining_count - len(good_sequences_original[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6731b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4165c8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cfa59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sequences = load_data(bad_log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844dfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_max_length(seq, event_id):\n",
    "    max_length = 0\n",
    "    start_index = seq.index(event_id)\n",
    "    end_index = 0\n",
    "    for i in range(start_index + 1 , len(seq)):\n",
    "        if seq[i] == event_id:\n",
    "            end_index = i - start_index\n",
    "            if end_index > max_length:\n",
    "                max_length = end_index\n",
    "            start_index = i\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def max_gap_two_lists(sequences):\n",
    "    max_length = 0\n",
    "    events = []\n",
    "    event_id_max_length = {}\n",
    "\n",
    "    for seq in sequences:\n",
    "        if seq and isinstance(seq[0], list):\n",
    "            inner_sequences = seq\n",
    "        else:\n",
    "            inner_sequences = [seq]  \n",
    "        for inner_seq in inner_sequences:\n",
    "            for i, x in enumerate(inner_seq):\n",
    "                gap = get_max_length(inner_seq, x)\n",
    "                if x not in event_id_max_length:\n",
    "                    event_id_max_length[x] = gap\n",
    "                    events.append(x)\n",
    "                else:\n",
    "                    if gap > event_id_max_length[x]:\n",
    "                        event_id_max_length[x] = gap\n",
    "\n",
    "                if event_id_max_length[x] > max_length:\n",
    "                    max_length = event_id_max_length[x]\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b739372",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequences_1 = good_sequences\n",
    "\n",
    "MAX_PATTERN_LEN = max_gap_two_lists(all_sequences_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PATTERN_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12855792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def get_ground_truth_file(path, ground_truth_files):\n",
    "    filename = []\n",
    "    for file in ground_truth_files:\n",
    "        if Path(path).stem in Path(file).stem:\n",
    "            print(\"from function - matched ground truth file is :\", file)\n",
    "            filename = file\n",
    "    return filename\n",
    "\n",
    "def get_trace_info(path):\n",
    "    filename = Path(path).stem\n",
    "    match = re.search(r\"(trace_trial_?\\d+)_(\\d+)-(\\d+)\", filename)\n",
    "    if match:\n",
    "        name = match.group(1)\n",
    "        start = int(match.group(2))\n",
    "        end = int(match.group(3))\n",
    "        test_data_name = name+'_'+str(start)+'-'+str(end)+'.json'\n",
    "        return name, start, end, test_data_name\n",
    "    else:\n",
    "        raise ValueError(\"Filename format not recognized\")\n",
    "\n",
    "def find_sequence_ground_truth(test_data_path, ground_truth):\n",
    "    trace = read_traces(test_data_path)\n",
    "    name, start, end,test_data_name = get_trace_info(test_data_path)\n",
    "    sequence = [int(ev[0]) for ev in trace if isinstance(ev, list) and len(ev) >= 2]\n",
    "    gt_start_end_pair = [[x[0], x[1]] for x in ground_truth]\n",
    "    return sequence, gt_start_end_pair\n",
    "\n",
    "\n",
    "def create_labels(sequence, gt_start_end_pair, test_data_start_index, test_data_end_index):\n",
    "    start_index = test_data_start_index\n",
    "    end_index = test_data_end_index\n",
    "    event_list = []\n",
    "    event_id_list = []\n",
    "    for start, end in gt_start_end_pair:\n",
    "        event_list = []\n",
    "        for event_id in range(start_index, end_index):\n",
    "            if event_id >= start and event_id <= end:\n",
    "                print(\"Event ID {} is in ground truth range ({}, {})\".format(event_id, start, end))\n",
    "                print(\"event_id - start_index:\", event_id , start_index)\n",
    "                event_list.append(sequence[event_id - start_index])\n",
    "        if event_list:\n",
    "            event_id_list.append(event_list)\n",
    "\n",
    "    return event_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.anomaly_detection import discover_test_files, load_ground_truth_dir, build_labels\n",
    "import json \n",
    " \n",
    "gt_path   = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/labels\" # example\n",
    "ground_truth_path = [os.path.join(gt_path, x) for x in os.listdir(gt_path)]\n",
    "\n",
    "print(\"ground truth path:\", ground_truth_path)\n",
    "\n",
    "new_label = {}\n",
    "\n",
    "for test_data in test_data_path:\n",
    "    print(\"Test data file:\", test_data)\n",
    "\n",
    "\n",
    "    print(\"---------------------------------------------------\")\n",
    "    test_data_name_1, test_data_start_index, test_data_end_index, test_data_name= get_trace_info(test_data)\n",
    "    print(\"Test data name is : \", test_data_name_1)\n",
    "    print(\"Test data start index is : \", test_data_start_index)\n",
    "    print(\"Test data end index is : \", test_data_end_index)\n",
    "\n",
    "\n",
    "    ground_truth_filename = get_ground_truth_file(test_data_name_1, ground_truth_path)\n",
    "    if not ground_truth_filename:\n",
    "        print(\"No matching ground truth file found for test data:\", test_data)\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        print(\"Ground truth file name is : \", ground_truth_filename)\n",
    "\n",
    "\n",
    "        ground_truth_raw = read_traces(ground_truth_filename)                                               # read ground truth labels from the label file\n",
    "        ground_truth = ground_truth_raw['labels']                                                # extract labels from dictionary from ground truth data\n",
    "\n",
    "        label_trace_name = list(ground_truth.keys())[0]\n",
    "        ground_truth = ground_truth[label_trace_name]\n",
    "\n",
    "        print(\"ground truth:\", ground_truth)\n",
    "\n",
    "        print(\"The test data file \", test_data, \" is not corresponding to ground truth file : \", ground_truth_filename)\n",
    "        print(\"The test data file \", test_data, \" is corresponding to ground truth file : \", ground_truth_filename)\n",
    "        sequence, gt_start_end_pair = find_sequence_ground_truth(test_data, ground_truth)\n",
    "        print(\"Event ID sequence is : \", sequence)\n",
    "        print(\"Ground truth start-end pairs are : \", gt_start_end_pair)\n",
    "        labels = create_labels(sequence, gt_start_end_pair, test_data_start_index, test_data_end_index)\n",
    "        print(\"Labels are : \", labels)\n",
    "        new_label[test_data_name] = labels\n",
    "        print(\"New label dictionary is : \", new_label)\n",
    "\n",
    "output_dir = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"gt_test_data_labels.json\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_label, f, indent=4)\n",
    "\n",
    "print(f\"\\n Saved to file: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ffa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "from math import ceil\n",
    "\n",
    "def has_substring(pattern, sequence):\n",
    "    \"\"\"\n",
    "    This function checks if the given pattern is a subsequence of sequence.\n",
    "    parameters : \n",
    "        pattern - list(int)\n",
    "        sequence - list(int)\n",
    "    Returns:\n",
    "        True if the pattern appears as a continous match in sequence.\n",
    "        False if no match\n",
    "    \"\"\"\n",
    "    pattern = tuple(pattern)\n",
    "    sequence = tuple(sequence)\n",
    "    m = len(pattern)\n",
    "    n = len(sequence)\n",
    "    if m == 0:\n",
    "        return True\n",
    "    if m > n:\n",
    "        return False\n",
    "    for i in range(n - m + 1):\n",
    "        if sequence[i:i+m] == pattern:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def count_substring(pattern, sequence):\n",
    "    \"\"\"\n",
    "    This function counts the number of times the given pattern appears as continuous match in sequence.\n",
    "    parameters : \n",
    "        pattern - list(int)\n",
    "        sequence - list(int)\n",
    "    Returns:\n",
    "        count - (int) the count of occurrences of pattern in sequence.\n",
    "    \"\"\"\n",
    "    pattern = tuple(pattern)\n",
    "    sequence = tuple(sequence)\n",
    "    m = len(pattern)\n",
    "    n = len(sequence)\n",
    "    if m == 0 or m > n:\n",
    "        return 0\n",
    "    count = 0\n",
    "    for i in range(n - m + 1):\n",
    "        if sequence[i:i+m] == pattern:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def is_subsequence(small, big):\n",
    "    it = iter(big)\n",
    "    return all(x in it for x in small)\n",
    "\n",
    "def next_same_index(seq):\n",
    "    \"\"\"\"\n",
    "    This function is used to compute next occurence of every element in the sequence.\n",
    "    parameters :\n",
    "        seq - list(int)\n",
    "    Returns:\n",
    "        nxt - list(int) where nxt[i] is the index of the next occurrence of seq[i] in seq and n if none.\n",
    "    \"\"\"\n",
    "    last_pos = {}\n",
    "    n = len(seq)\n",
    "    nxt = [n] * n\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        v = seq[i]\n",
    "        if v in last_pos:\n",
    "            nxt[i] = last_pos[v]\n",
    "        last_pos[v] = i\n",
    "    return nxt\n",
    "\n",
    "def dynamic_window_sequence(sequences):\n",
    "    \"\"\"\n",
    "    This function generates dynamic windows for each sequence in sequences.\n",
    "    For example, for sequence [1,2,3,1,4,2], the dynamic windows are: [1,2,3],[1,4,2]\n",
    "    parameters :\n",
    "        sequences - list of list(int)\n",
    "    Returns:\n",
    "        final_windows - list of list(int) containing all dynamic windows from all sequences.\n",
    "    \"\"\"\n",
    "    final_windows = []\n",
    "    for seq in sequences:\n",
    "        if not seq:\n",
    "            continue\n",
    "        nxt = next_same_index(seq)\n",
    "        n = len(seq)\n",
    "        for i in range(n):\n",
    "            j = nxt[i]\n",
    "            if j > i:\n",
    "                final_windows.append(seq[i:j])\n",
    "    return final_windows\n",
    "\n",
    "def windows_by_anchor(sequences):\n",
    "    \"\"\"\n",
    "    This function groups dynamic windows by their anchor first element.\n",
    "    parameters :\n",
    "        sequences - list of list(int)\n",
    "    Returns:\n",
    "        window - dict where window[a] is the list of windows starting with anchor a.\n",
    "\n",
    "        For example: For sequence [1,2,3,1,4,2], the dynamic windows are: [1,2,3],[1,4,2]\n",
    "        Output: 1 : [[1,2,3],[1,4,2]]\n",
    "    \"\"\"\n",
    "    window = {}\n",
    "    for w in dynamic_window_sequence(sequences):\n",
    "        if not w:\n",
    "            continue\n",
    "        a = w[0]\n",
    "        if a not in window:\n",
    "            window[a] = []\n",
    "        window[a].append(w)\n",
    "    return window\n",
    "\n",
    "def candidate_allowed(pattern, x, repeated_event_id):\n",
    "    \"\"\"\n",
    "    This function is used to check if at any stage of cartesian we are able to append that with x.\n",
    "    Parameters:\n",
    "        pattern - tuple(int) - (6,7,8)\n",
    "        x - int - event_id in the sequence 6 or 7 or 8\n",
    "        repeated_event_id - bool - flag to allow or disallow repeated event ids in the pattern.\n",
    "    Returns:\n",
    "        True if appending x to pattern is allowed, False otherwise.\n",
    "    \n",
    "        For example : (6,7,8), x=6 - This return true if repeated_event_id flag is True else False\n",
    "                      (6,7,8), x=8 - This return false in both cases as adjacent duplicates are not allowed.\n",
    "    \"\"\"\n",
    "    if not pattern: \n",
    "        return True\n",
    "    if x == pattern[-1]:\n",
    "        return False \n",
    "    if repeated_event_id is False and x in pattern:\n",
    "        return False \n",
    "    return True\n",
    "\n",
    "def generate_frequent_patterns(sequences, min_sup, max_len=None, repeated_event_id=False):\n",
    "    \"\"\"\n",
    "    This function helps to mine frequent continuous patterns over dynamic windows indexed by anchor.\n",
    "\n",
    "    For each candidate pattern p = (a, x, y, ...), support is the number of dynamic windows that START with anchor 'a' and contain p as a contiguous\n",
    "    substring. Candidates are generated Apriori-style via right-extensions and filtered by `candidate_allowed`:\n",
    "      - No adjacent duplicates (always).\n",
    "      - If `repeated_event_id` is False: no repeated symbols anywhere in p.\n",
    "      - If `repeated_event_id` is True : repeated symbols allowed if non-adjacent.\n",
    "\n",
    "    Parameters:\n",
    "        sequences -  (list[list[int]]): sequences of event IDs.\n",
    "        min_sup (int): Minimum window-support to keep a pattern if greater than or equal to min_sup.\n",
    "        max_len (int or None): Maximum pattern length K to explore. If None, expand until no further candidates survive.\n",
    "        repeated_event_id (bool): Whether to allow non-adjacent repeats in patterns.\n",
    "\n",
    "    Returns:\n",
    "        dict[tuple[int, ...], int]: Map from pattern tuple to its window-support.\n",
    "\n",
    "    \"\"\"\n",
    "    by_anchor = windows_by_anchor(sequences)\n",
    "\n",
    "    all_patterns = {}\n",
    "    S1 = set()                          # For S1 cartesian\n",
    "    for a, wins in by_anchor.items():\n",
    "        c = len(wins)\n",
    "        if c >= min_sup:\n",
    "            all_patterns[(a,)] = c\n",
    "            S1.add(a)\n",
    "\n",
    "    cur_level = {}\n",
    "    if len(S1) > 0:\n",
    "        candidate_len2 = set()\n",
    "        for i in S1:\n",
    "            for j in S1:\n",
    "                if not candidate_allowed((i,), j, repeated_event_id):\n",
    "                    continue\n",
    "                candidate_len2.add((i, j))\n",
    "        if candidate_len2:\n",
    "            counts2 = defaultdict(int)\n",
    "            for cand in candidate_len2:\n",
    "                a = cand[0]\n",
    "                for w in by_anchor.get(a, []):\n",
    "                    if has_substring(cand, w):\n",
    "                        counts2[cand] += 1\n",
    "            for p, c in counts2.items():\n",
    "                if c >= min_sup:\n",
    "                    cur_level[p] = c\n",
    "                    all_patterns[p] = c\n",
    "\n",
    "    level = 3           # From S3\n",
    "    while cur_level:\n",
    "        if max_len is not None and level > max_len:\n",
    "            break\n",
    "\n",
    "        candidates = set()\n",
    "        for p in cur_level.keys():\n",
    "            for x in S1:\n",
    "                if not candidate_allowed(p, x, repeated_event_id):\n",
    "                    continue\n",
    "                candidates.add(p + (x,))\n",
    "\n",
    "        if not candidates:\n",
    "            break\n",
    "\n",
    "        counts_k = defaultdict(int)\n",
    "        for cand in candidates:\n",
    "            a = cand[0]\n",
    "            for w in by_anchor.get(a, []):\n",
    "                if has_substring(cand, w):\n",
    "                    counts_k[cand] += 1\n",
    "\n",
    "        next_level = {}\n",
    "        for p, c in counts_k.items():\n",
    "            if c >= min_sup:\n",
    "                next_level[p] = c\n",
    "                all_patterns[p] = c\n",
    "\n",
    "        if not next_level:\n",
    "            break\n",
    "\n",
    "        cur_level = next_level\n",
    "        level += 1\n",
    "\n",
    "    return all_patterns\n",
    "\n",
    "\n",
    "def compress_patterns(patterns):\n",
    "    \"\"\"\n",
    "    Compress frequent patterns by removing redundant shorter ones.\n",
    "    For each pattern P, if there exists a longer pattern Q such that:\n",
    "        - P appears as a CONTIGUOUS substring of Q, and has support(P) == support(Q), then P is dropped (only Q is kept).\n",
    "\n",
    "    Parameters:\n",
    "        patterns (dict[tuple[int, ...], int]): Dictionary mapping each pattern tuple to its support count.\n",
    "\n",
    "    Returns:\n",
    "        dict[tuple[int, ...], int]: Compressed dictionary containing only longest unique-support patterns.\n",
    "\n",
    "    For example:\n",
    "        Input  : {(6,7):3, (6,7,8):3, (6,8):2}\n",
    "        Output : {(6,7,8):3, (6,8):2} Because (6,7) is a substring of (6,7,8) and both have same support=3.\n",
    "    \"\"\"\n",
    "    items = list(patterns.items())\n",
    "    items.sort(key=lambda x: (-len(x[0]), x[0]))\n",
    "    compressed_patterns = {}\n",
    "    for pattern, support in items:\n",
    "        drop = False\n",
    "        for other in compressed_patterns.keys():\n",
    "            if patterns[other] == support and has_substring(pattern, other):\n",
    "                drop = True\n",
    "                break\n",
    "        if not drop:\n",
    "            compressed_patterns[pattern] = support\n",
    "    return compressed_patterns\n",
    "\n",
    "def support_patterns(sequences, patterns):\n",
    "    \"\"\"\n",
    "    Compute per-sequence support for continuous patterns using dynamic windows.\n",
    "    For each pattern p (a tuple of event IDs), compute:\n",
    "      - across: number of sequences in which p occurs at least once as a continuous substring inside any dynamic window.\n",
    "      - in_file_avg: average number of continous occurrences per sequence, computed ONLY over sequence where p occurs (current sequence).\n",
    "\n",
    "    Computing support:\n",
    "      1) For each file (sequence), we build dynamic windows and group them by their anchor (window[0]).\n",
    "      2) For a pattern p = (a, ...), we look only at windows anchored at 'a' and sum count_substring(p, window) across those windows.\n",
    "      3) If the summed count for that sequence > 0, the sequence contributes to 'across' and contributes its count toward the present-only average.\n",
    "\n",
    "    Parameters:\n",
    "        sequences (list[list[int]]): List traces as sequences of event IDs.\n",
    "        patterns (Iterable[tuple[int, ...]]): Patterns to measure.\n",
    "\n",
    "    Returns:\n",
    "        dict[tuple[int, ...], dict]: For each pattern p, a dict with:\n",
    "            {\n",
    "              \"across\": int,\n",
    "              \"in_file_avg\": float\n",
    "            }\n",
    "    \"\"\"\n",
    "    support = {}\n",
    "    if not patterns:\n",
    "        return support\n",
    "\n",
    "    sequence_window_by_anchor = []\n",
    "    for seq in sequences:\n",
    "        seq_1 = {}\n",
    "        for w in dynamic_window_sequence([seq]):\n",
    "            if not w:\n",
    "                continue\n",
    "            a = w[0]\n",
    "            seq_1.setdefault(a, []).append(w)\n",
    "        sequence_window_by_anchor.append(seq_1)\n",
    "\n",
    "    per_sequence_counts = {p: [0]*len(sequences) for p in patterns}\n",
    "\n",
    "    for i, t in enumerate(sequence_window_by_anchor):\n",
    "        for p in patterns:\n",
    "            if not p:\n",
    "                continue\n",
    "            a = p[0]\n",
    "            cnt = 0\n",
    "            for w in t.get(a, []):\n",
    "                cnt += count_substring(p, w)          \n",
    "            per_sequence_counts[p][i] = cnt\n",
    "\n",
    "    for p, vec in per_sequence_counts.items():\n",
    "        present = sum(1 for v in vec if v > 0)\n",
    "        total = sum(v for v in vec if v > 0)\n",
    "        support[p] = {\"across\": present, \"in_file_avg\": (float(total) / float(present)) if present > 0 else 0.0}\n",
    "    return support\n",
    "\n",
    "def generateFrequentSubSequences(sequences, K, Scommon, min_sup, repeated_event_id=False):\n",
    "    \"\"\"\n",
    "    Generate all frequent continous patterns of exact length K from a set of sequences, using dynamic windows grouped by anchor.\n",
    "    This function is used for progressive discriminative mining process. At each iteration, it mines only the K-length patterns (not all lengths) \n",
    "    by extending the (K-1)-length patterns from the previous step.\n",
    "\n",
    "    Candidate generation:\n",
    "      - For K=1, each anchor (window first element) is a 1-gram.\n",
    "      - For K=2, base = all frequent 1-grams (anchors) with support ≥ min_sup.\n",
    "      - For K>2, base = patterns from `Scommon` having length K-1.\n",
    "      - Each base pattern is extended on the right by one symbol from S1, the set of frequent anchors.\n",
    "\n",
    "    Support counting:\n",
    "      - Support(p) = number of dynamic windows that start with anchor p[0] and contain pattern p contiguously at least once.\n",
    "      - Candidates with support ≥ min_sup are retained as frequent K-grams.\n",
    "\n",
    "    repeated_event_id = False → discards all repeated symbols and adjacent duplicates\n",
    "    repeated_event_id = True  → allows non-adjacent repeats but still discards adjacent duplicates\n",
    "\n",
    "    Parameters:\n",
    "        sequences : list[list[int]] - Input event sequences\n",
    "        K : int - pattern length to mine\n",
    "        Scommon : set[tuple[int, ...]] - Set of frequent patterns common to both good and bad sequences from the previous iteration\n",
    "        min_sup : int - Minimum window-support threshold patterns with support ≥ min_sup\n",
    "        repeated_event_id : bool, default=False Whether to allow repeated event IDs within a pattern\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[tuple[int, ...], int] - Dictionary mapping each frequent pattern tuple of event IDs of length K to its support count.\n",
    "\n",
    "    For example\n",
    "    -------\n",
    "    sequences = [[6,7,8,6,7,8,9]]\n",
    "    GenerateFrequentSubSequences(sequences, K=2, Scommon=set(), min_sup=1)\n",
    "    output - {(6,7): 2, (7,8): 2, (8,6): 1, (8,9): 1}\n",
    "    \"\"\"\n",
    "    by_anchor = windows_by_anchor(sequences)\n",
    "\n",
    "    if K == 1:\n",
    "        result = {}\n",
    "        for a, wins in by_anchor.items():\n",
    "            c = len(wins)\n",
    "            if c >= min_sup:\n",
    "                result[(a,)] = c\n",
    "        return result\n",
    "\n",
    "    base = set()\n",
    "    if Scommon and len(Scommon) > 0:\n",
    "        for pat in Scommon:\n",
    "            if len(pat) == K - 1:\n",
    "                base.add(pat)\n",
    "    else:\n",
    "        if K == 2:\n",
    "            for a, wins in by_anchor.items():\n",
    "                if len(wins) >= min_sup:\n",
    "                    base.add((a,))\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    S1 = set()\n",
    "    for a, wins in by_anchor.items():\n",
    "        if len(wins) >= min_sup:\n",
    "            S1.add(a)\n",
    "\n",
    "    candidates = set()\n",
    "    for p in base:\n",
    "        for x in S1:\n",
    "            if not candidate_allowed(p, x, repeated_event_id):\n",
    "                continue\n",
    "            candidates.add(p + (x,))\n",
    "\n",
    "    if not candidates:\n",
    "        return {}\n",
    "\n",
    "    counts_k = defaultdict(int)\n",
    "    for cand in candidates:\n",
    "        a = cand[0]\n",
    "        for w in by_anchor.get(a, []):\n",
    "            if has_substring(cand, w):\n",
    "                counts_k[cand] += 1\n",
    "\n",
    "    result = {}\n",
    "    for p, c in counts_k.items():\n",
    "        if c >= min_sup:\n",
    "            result[p] = c\n",
    "    return result\n",
    "\n",
    "def findCommon(good_pattern_seq, bad_pattern_seq):\n",
    "    \"\"\"\n",
    "    Function to find patterns that are common to both good and bad sequence groups at the current pattern length K.\n",
    "    These common patterns are then used as the base for generating longer candidate patterns (K+1) in the next iteration.\n",
    "\n",
    "    Parameters:\n",
    "        good_pattern_seq : dict[tuple[int, ...], int] - Frequent continous patterns of length K mined from the good sequences.\n",
    "        bad_pattern_seq : dict[tuple[int, ...], int] - Frequent continous patterns of length K mined from the bad sequences.\n",
    "\n",
    "    Returns:\n",
    "        set[tuple[int, ...]] - Set of pattern tuples that are present in both good and bad pattern sets.\n",
    "\n",
    "    For example::\n",
    "    good_pattern_seq = {(6,7,8): 4, (7,8,9): 3}\n",
    "    bad_pattern_seq  = {(6,7,8): 2, (8,9,10): 5}\n",
    "    findCommon(good_pattern_seq, bad_pattern_seq)\n",
    "    output - {(6,7,8)}\n",
    "    \"\"\"\n",
    "    common = set()\n",
    "    if good_pattern_seq and bad_pattern_seq:\n",
    "        for p in good_pattern_seq:\n",
    "            if p in bad_pattern_seq:\n",
    "                common.add(p)\n",
    "    return common\n",
    "\n",
    "\n",
    "\n",
    "def FindDiscriminative(A, B, num_A, num_B, theta=0.8, delta=0.8):\n",
    "    \"\"\"\n",
    "    Identify discriminative patterns that occur significantly more often in one group of sequences (A) than in another group (B), based on ratio thresholds.\n",
    "    A pattern p is marked as discriminative if any of the following hold:\n",
    "    1. B has zero evidence of the pattern: - B_stats[p][\"across\"] == 0, or B_stats[p][\"in_file_avg\"] == 0\n",
    "    2. The ratio of presence across sequences exceeds theta: (across_A / across_B) >= theta\n",
    "    3. The ratio of average in-sequence occurrences exceeds delta: (in_file_avg_A / in_file_avg_B) >= delta\n",
    "\n",
    "    Parameters:\n",
    "        A : dict[tuple[int, ...], dict] - Support statistics for patterns in group A (e.g., bad or good sequences).\n",
    "            Each entry has:\n",
    "                {\n",
    "                \"across\": int,        # number of sequences where pattern appears\n",
    "                \"in_seq_avg\": float   # avg. number of continous occurrences per sequence\n",
    "                }\n",
    "        B : dict[tuple[int, ...], dict] - Equivalent support statistics for patterns in group B.\n",
    "        num_A : int - Number of sequences in group A.\n",
    "        num_B : int - Number of sequences in group B.\n",
    "        theta : float, default=0.8 - Threshold for \"across-sequence\" ratio test (presence ratio).\n",
    "        delta : float, default=0.8 - Threshold for \"in-sequence\" ratio test (average frequency ratio).\n",
    "\n",
    "    Returns:\n",
    "        set[tuple[int, ...]] - Set of patterns that are discriminative for group A compared to B.\n",
    "\n",
    "    for exmaple::\n",
    "        A_stats = { (6,7,8): {\"across\": 5, \"in_file_avg\": 2.3} }\n",
    "        B_stats = { (6,7,8): {\"across\": 1, \"in_file_avg\": 0.4} }\n",
    "        FindDiscriminative(A_stats, B_stats, nA=10, nB=10, theta=0.8, delta=0.8)\n",
    "        output - {(6,7,8)}\n",
    "    \"\"\"\n",
    "    discriminative = set()\n",
    "    for p in A:\n",
    "        support_a = A[p]\n",
    "        if p not in B:\n",
    "            discriminative.add(p)\n",
    "            continue\n",
    "        support_b = B[p]\n",
    "\n",
    "        seq_a = float(support_a[\"across\"]) / float(num_A) if num_A > 0 else 0.0\n",
    "        seq_b = float(support_b[\"across\"]) / float(num_B) if num_B > 0 else 0.0\n",
    "        per_seq_count_A = support_a[\"in_file_avg\"]\n",
    "        per_seq_count_B = support_b[\"in_file_avg\"]\n",
    "\n",
    "        if seq_b == 0.0 or per_seq_count_B == 0.0:\n",
    "            discriminative.add(p)\n",
    "            continue\n",
    "\n",
    "        if seq_b > 0.0 and (seq_a / seq_b) >= theta:\n",
    "            discriminative.add(p)\n",
    "            continue\n",
    "        if per_seq_count_B > 0.0 and (per_seq_count_A / per_seq_count_B) >= delta:\n",
    "            discriminative.add(p)\n",
    "            continue\n",
    "    return discriminative\n",
    "\n",
    "\n",
    "def mine_discriminative_patterns_progressive(good_seqs,bad_seqs,min_sup,theta=0.8,delta=0.8,k_start=1,k_max=10,stop_when_found=True,normalize=True,repeated_event_id=False):\n",
    "    \"\"\"\n",
    "    Mine and score discriminative continous patterns between two groups of sequences good vs bad using a two-stage DustMiner-style pipeline.\n",
    "    Stage 1 — Progressive discriminative mining\n",
    "      • For K = k_start..k_max:\n",
    "          - Generate frequent K-grams separately from good and bad sequences via GenerateFrequentSubSequences.\n",
    "          - Compute per-pattern support stats in each group for both across and in seqeunce group.\n",
    "          - Mark patterns discriminative via FindDiscriminative using θ (across ratio) and δ (in-sequence ratio).\n",
    "          - Update Scommon = intersection of frequent K-grams (good ∩ bad) for next K.\n",
    "      • At the end of Stage 1, we keep patterns discriminative for the bad group.\n",
    "\n",
    "    Stage 2 — Quantitative scoring\n",
    "      • Determine longest discriminative length (max_len_found).\n",
    "      • Build full frequent pattern lattices up to max_len_found for each group via generate_frequent_patterns(...) to get supports for all prefixes.\n",
    "      • Compute normalized probabilities P_bad(p) and P_good(p) using a chain rule over continous extensions; score each pattern with delta = P_bad - P_good.\n",
    "      • Return patterns with positive delta, sorted by (delta, p, length p).\n",
    "\n",
    "    Parameters:\n",
    "        good_seqs : list[list[int]] - Traces labeled as good (normal data).\n",
    "        bad_seqs : list[list[int]] - Traces labeled as bad (faulty).\n",
    "        min_sup : int - Minimum window-support threshold used in Stage 1\n",
    "        theta : float, default=0.8 - Across-sequence presence ratio threshold (A/B) in FindDiscriminative.\n",
    "        delta : float, default=0.8 - In-sequence average count ratio threshold (A/B) in FindDiscriminative.\n",
    "        k_start : int, default=1 - Initial pattern length for progressive mining.\n",
    "        k_max : int - Maximum pattern length to mine during Stage 1.\n",
    "        stop_when_found : bool, default=True - If True, break the mining as soon as any discriminative set sequenceGood and sequencebad is non-empty.\n",
    "        normalize : bool, default=True - If True, compute Stage-2 chain probabilities; if False, fall back to across-sequence rates.\n",
    "        repeated_event_id : bool, default=False - allow non-adjacent repeats if True and adjacent duplicates are always disallowed.\n",
    "\n",
    "    Returns:\n",
    "        dict[tuple[int, ...], dict] - Mapping each discriminative pattern p to: {\"bad\": float, \"good\": float, \"delta\": float} sorted by decreasing delta\n",
    "\n",
    "    \"\"\"\n",
    "    Scommon = set()\n",
    "    good_discriminative = set()\n",
    "    bad_discriminative = set()\n",
    "    K = k_start\n",
    "    length_good_seq = len(good_seqs)\n",
    "    length_bad_seq = len(bad_seqs)\n",
    "\n",
    "    while True:\n",
    "        if K > k_max:\n",
    "            break\n",
    "\n",
    "        k_frequent_good_patterns = generateFrequentSubSequences(good_seqs, K, Scommon, min_sup, repeated_event_id=repeated_event_id)\n",
    "        k_frequent_bad_patterns = generateFrequentSubSequences(bad_seqs,  K, Scommon, min_sup, repeated_event_id=repeated_event_id)\n",
    "        print(\"Good sequence mine patterns at stage : \",K,\" is : \", k_frequent_good_patterns)\n",
    "        print(\"Bad sequence mine patterns at stage :\",K, \" is :\", k_frequent_bad_patterns)\n",
    "\n",
    "        if not k_frequent_good_patterns and not k_frequent_bad_patterns:\n",
    "            break\n",
    "\n",
    "        good_seq_support = support_patterns(good_seqs, set(k_frequent_good_patterns.keys()) if k_frequent_good_patterns else set())\n",
    "        bad_seq_support = support_patterns(bad_seqs,  set(k_frequent_bad_patterns.keys())  if k_frequent_bad_patterns else set())\n",
    "\n",
    "        if k_frequent_good_patterns:\n",
    "            discriminative_good = FindDiscriminative(good_seq_support, bad_seq_support, length_good_seq, length_bad_seq, theta=theta, delta=delta)\n",
    "            for p in discriminative_good:\n",
    "                if p in k_frequent_good_patterns:\n",
    "                    good_discriminative.add(p)\n",
    "\n",
    "        if k_frequent_bad_patterns:\n",
    "            discriminative_bad = FindDiscriminative(bad_seq_support, good_seq_support, length_bad_seq, length_good_seq, theta=theta, delta=delta)\n",
    "            for p in discriminative_bad:\n",
    "                if p in k_frequent_bad_patterns:\n",
    "                    bad_discriminative.add(p)\n",
    "\n",
    "        Scommon = findCommon(k_frequent_good_patterns, k_frequent_bad_patterns)\n",
    "        K += 1\n",
    "        if stop_when_found:\n",
    "            if len(good_discriminative) > 0 or len(bad_discriminative) > 0:\n",
    "                break\n",
    "\n",
    "    patterns = set(bad_discriminative)\n",
    "    if not patterns:\n",
    "        return {}\n",
    "\n",
    "    max_len_found = 1\n",
    "    for p in patterns:\n",
    "        if len(p) > max_len_found:\n",
    "            max_len_found = len(p)\n",
    "\n",
    "    full_bad  = generate_frequent_patterns(bad_seqs,  min_sup, max_len=max_len_found, repeated_event_id=repeated_event_id)\n",
    "    full_good = generate_frequent_patterns(good_seqs, min_sup, max_len=max_len_found, repeated_event_id=repeated_event_id)\n",
    "\n",
    "    def prefix_out_sum(full):\n",
    "        i = defaultdict(int)\n",
    "        for q, c in full.items():\n",
    "            if len(q) >= 2:\n",
    "                i[q[:-1]] += c\n",
    "        return i\n",
    "\n",
    "    bad_out  = prefix_out_sum(full_bad)\n",
    "    good_out = prefix_out_sum(full_good)\n",
    "\n",
    "    def prob_chain(pattern, full, out, total_events):\n",
    "        if total_events <= 0:\n",
    "            return 0.0\n",
    "        if len(pattern) == 1:\n",
    "            return float(full.get(pattern, 0)) / float(total_events)\n",
    "        val = float(full.get((pattern[0],), 0)) / float(total_events)\n",
    "        if val == 0.0:\n",
    "            return 0.0\n",
    "        for i in range(1, len(pattern)):\n",
    "            pref = pattern[:i]\n",
    "            n = float(full.get(pattern[:i+1], 0))\n",
    "            d = float(out.get(pref, 0))\n",
    "            if n == 0.0 or d == 0.0:\n",
    "                return 0.0\n",
    "            val *= (n / d)\n",
    "        return val\n",
    "\n",
    "    total_bad  = sum(len(s) for s in bad_seqs)\n",
    "    total_good = sum(len(s) for s in good_seqs)\n",
    "\n",
    "    discriminative = {}\n",
    "    for p in patterns:\n",
    "        if normalize:\n",
    "            b = prob_chain(p, full_bad,  bad_out,  total_bad)\n",
    "            g = prob_chain(p, full_good, good_out, total_good)\n",
    "        else:\n",
    "            bad_support = support_patterns(bad_seqs,  [p])\n",
    "            good_support = support_patterns(good_seqs, [p])\n",
    "            b = float(bad_support.get(p, {\"across\":0})[\"across\"]) / float(max(1, length_bad_seq))\n",
    "            g = float(good_support.get(p, {\"across\":0})[\"across\"]) / float(max(1, length_good_seq))\n",
    "        d = b - g\n",
    "        if d > 0:\n",
    "            discriminative[p] = {\"bad\": b, \"good\": g, \"delta\": d} \n",
    "\n",
    "    discriminative_sorted = dict(sorted(discriminative.items(), key=lambda x: (-x[1][\"delta\"], -len(x[0]), x[0])))\n",
    "    return discriminative_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "start_time = time.time()\n",
    "process = psutil.Process(os.getpid())\n",
    "start_mem = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "\n",
    "S1 = {e for seq in (good_sequences + bad_sequences) for e in seq}\n",
    "k_max_1 = len(S1)\n",
    "\n",
    "print(\"MAX PATTERN LENGTH:\", k_max_1)\n",
    "\n",
    "\n",
    "discriminative_patterns = mine_discriminative_patterns_progressive(\n",
    "    good_sequences,\n",
    "    bad_sequences,\n",
    "    min_sup=2,\n",
    "    theta=0.8, \n",
    "    delta=0.8, \n",
    "    k_start=1,\n",
    "    k_max=k_max_1,\n",
    "    stop_when_found=False,\n",
    "    normalize=True,\n",
    "    repeated_event_id=True,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "end_mem = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Memory used: {end_mem - start_mem:.2f} MB\")\n",
    "\n",
    "# discriminative_patterns = mine_discriminative_patterns_progressive(\n",
    "#     [[6,7,8,9,6,7,8,9,10,11,12],[6,7,8,9,10,11,12,13,6,7,8,6,7,8,9]],\n",
    "#     [[6,7,8,9,6,7,8,7,9,10],[6,7,8,9,10,11,12]],\n",
    "#     min_sup=2,\n",
    "#     theta=0.8,\n",
    "#     delta=0.8,\n",
    "#     k_start=1,\n",
    "#     k_max=10,\n",
    "#     stop_when_found=False,\n",
    "#     normalize=True,\n",
    "#     repeated_event_id=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab0a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminative_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(discriminative_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c2ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import ceil\n",
    "\n",
    "def segment_fixed_width(sequence, width, overlap=False):\n",
    "    \"\"\"\n",
    "    Function splits a sequence into fixed-width segments. Each segment is represented as a tuple (start_index, end_index, subsequence)\n",
    "    where the indices refer to the original sequence.\n",
    "\n",
    "    Parameters:\n",
    "        sequence : list or tuple - input sequence\n",
    "        width : int - segment window size. If width = 0, returns an empty list.\n",
    "        overlap : bool, If False - Create non-overlapping segments of size width. If True - Create overlapping segments with stride = width - 1 \n",
    "        sliding by one new element each time\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[int, int, list]] - list of tuples, where each tuple contains: (start_index, end_index, subsequence) representing the start and end indices \n",
    "        and the actual slice of the sequence.\n",
    "\n",
    "    for eg:\n",
    "        sequence = [1, 2, 3, 4, 5, 6]\n",
    "        segment_fixed_width(sequence, width=3, overlap=False)\n",
    "        output - [(0, 3, [1, 2, 3]), (3, 6, [4, 5, 6])]\n",
    "\n",
    "        segment_fixed_width(sequence, width=3, overlap=True)\n",
    "        output - [(0, 3, [1, 2, 3]), (2, 5, [3, 4, 5]), (4, 6, [5, 6])]\n",
    "    \"\"\"\n",
    "    if width <= 0:\n",
    "        return []\n",
    "    segment = []\n",
    "    n = len(sequence)\n",
    "    if not overlap:\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            j = min(i + width, n)\n",
    "            segment.append((i, j, sequence[i:j]))\n",
    "            i += width\n",
    "    else:\n",
    "        stride = max(1, width - 1)  \n",
    "        i = 0\n",
    "        while i < n:\n",
    "            j = min(i + width, n)\n",
    "            segment.append((i, j, sequence[i:j]))\n",
    "            if j == n:\n",
    "                break\n",
    "            i += stride\n",
    "    return segment\n",
    "\n",
    "def score_segment(seg_seq, discrinimative_keys):\n",
    "    \"\"\"\n",
    "    Compute a discriminative score for a segment based on pattern matches.\n",
    "    For each discriminative pattern p in discriminative_keys, check whether p appears as a subsequence within the given segment `seg_seq`.\n",
    "    The score is the total number of discriminative patterns that match.\n",
    "\n",
    "    Parameters:\n",
    "        seg_seq : list[int] - segment\n",
    "        discriminative_keys : [tuple[int]] - discriminative patterns where each pattern is a tuple of event IDs.\n",
    "\n",
    "    Returns:\n",
    "        int - number of discriminative patterns that appear as subsequences within the segment.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for p in discrinimative_keys:\n",
    "        if is_subsequence(p, seg_seq):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def merge_consecutive_segments(segs_1):\n",
    "    \"\"\"\n",
    "    Merge consecutive segments from the same sequence into larger segments.\n",
    "    This function takes a list of segment dictionaries each describing a subsequence with start/end positions, a score, and the originating sequence and merges adjacent segments that:\n",
    "    Belong to the same sequence, and they are continous in position (the end of one equals the start of the next), and also Both have positive scores.\n",
    "    When merged, the resulting segments: 'end' is extended to the new segments end, 'seq' is concatenated, 'score' is summed.\n",
    "\n",
    "    Parameters:\n",
    "        segs_1 : list[dict] - List of segment dictionaries. Each dictionary should contain at least:\n",
    "\n",
    "    Returns:\n",
    "        list[dict] - List of merged segment dictionaries with adjacent segments combined.\n",
    "\n",
    "    for eg:\n",
    "        segs_1 = [{\"file\": \"A\", \"start\": 0, \"end\": 3, \"seq\": [6,7,8], \"score\": 2},{\"file\": \"A\", \"start\": 3, \"end\": 6, \"seq\": [9,10,11], \"score\": 1},\n",
    "        {\"file\": \"A\", \"start\": 6, \"end\": 9, \"seq\": [12,13,14], \"score\": 0},\n",
    "        {\"file\": \"B\", \"start\": 0, \"end\": 3, \"seq\": [1,2,3], \"score\": 1}]\n",
    "        merge_consecutive_segments(segs_1)\n",
    "        output - [ {\"file\": \"A\", \"start\": 0, \"end\": 6, \"seq\": [6,7,8,9,10,11], \"score\": 3}, {\"file\": \"A\", \"start\": 6, \"end\": 9, \"seq\": [12,13,14], \"score\": 0},\n",
    "        {\"file\": \"B\", \"start\": 0, \"end\": 3, \"seq\": [1,2,3], \"score\": 1}]\n",
    "    \"\"\"\n",
    "    if not segs_1:\n",
    "        return []\n",
    "    segs_1.sort(key=lambda d: (d[\"file\"], d[\"start\"]))\n",
    "    merged = []\n",
    "    current = None\n",
    "    for i in segs_1:\n",
    "        if current is None:\n",
    "            current = dict(i)\n",
    "        else:\n",
    "            same_file = (i[\"file\"] == current[\"file\"])\n",
    "            contiguous = (i[\"start\"] == current[\"end\"])\n",
    "            if same_file and contiguous and current[\"score\"] > 0 and i[\"score\"] > 0:\n",
    "                current[\"end\"] = i[\"end\"]\n",
    "                current[\"seq\"] = current[\"seq\"] + i[\"seq\"]\n",
    "                current[\"score\"] = current[\"score\"] + i[\"score\"]\n",
    "            else:\n",
    "                merged.append(current)\n",
    "                current = dict(i)\n",
    "    if current is not None:\n",
    "        merged.append(current)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def stage2_mining(bad_seqs,discriminative_patterns,seg_width=50,top_k=5,max_len_stage2=None,overlap=False):\n",
    "    \"\"\"\n",
    "    Stage-2 mining:\n",
    "    This stage performs localized pattern mining on the highest-scoring faulty regions identified using discriminative patterns from Stage 1. \n",
    "    It then aggregates patterns that are consistent across all selected segments.\n",
    "\n",
    "    Segment each bad sequence into fixed-width windows.\n",
    "    Score each segment by counting how many discriminative patterns appear.\n",
    "    Merge adjacent high-scoring segments from the same file.\n",
    "    Select top-K merged segments based on score and length.\n",
    "    Mine frequent contiguous patterns within each selected segment.\n",
    "    Keep only patterns that appear in all top segments.\n",
    "\n",
    "    Parameters:\n",
    "        bad_seqs : list[list[int]] - List of faulty (bad) sequences.\n",
    "        discriminative_patterns : dict or iterable[tuple[int]] - Discriminative patterns discovered in Stage 1 (keys are pattern tuples).\n",
    "        seg_width : int, default=50 - Segment size for splitting each sequence.\n",
    "        top_k : int, default=5 - Maximum number of top-scoring merged segments to mine.\n",
    "        max_len_stage2 : int or None, default=None - Maximum pattern length mined within each selected segment.\n",
    "        overlap : bool, default=False - If True generate overlapping windows during segmentation.\n",
    "\n",
    "    Returns:\n",
    "        dict[tuple[int], int] - Final dictionary of Stage-2 frequent patterns that appear in all selected top-K faulty segments with aggregated support counts.\n",
    "    \"\"\"\n",
    "    if isinstance(discriminative_patterns, dict):\n",
    "        discriminative_keys = list(discriminative_patterns.keys())\n",
    "    else:\n",
    "        discriminative_keys = list(discriminative_patterns)\n",
    "\n",
    "    if not discriminative_keys:\n",
    "        return {}\n",
    "\n",
    "    segs = []\n",
    "    for fidx, seq in enumerate(bad_seqs):\n",
    "        pieces = segment_fixed_width(seq, seg_width, overlap=overlap)\n",
    "        for (s, e, seg) in pieces:\n",
    "            sc = score_segment(seg, discriminative_keys)\n",
    "            segs.append({\"file\": fidx, \"start\": s, \"end\": e, \"seq\": seg, \"score\": sc})\n",
    "\n",
    "    if not segs:\n",
    "        return {}\n",
    "\n",
    "    merged = merge_consecutive_segments(segs)\n",
    "    if not merged:\n",
    "        return {}\n",
    "\n",
    "    order = list(range(len(merged)))\n",
    "    order.sort(key=lambda i: (merged[i][\"score\"], len(merged[i][\"seq\"])), reverse=True)\n",
    "\n",
    "    selected = []\n",
    "    if top_k is None or top_k <= 0:\n",
    "        for i in order:\n",
    "            selected.append(merged[i])\n",
    "    else:\n",
    "        for i in order:\n",
    "            if merged[i][\"score\"] <= 0:\n",
    "                continue\n",
    "            selected.append(merged[i])\n",
    "            if len(selected) >= top_k:\n",
    "                break\n",
    "        if len(selected) == 0:\n",
    "            take = min(top_k, len(order))\n",
    "            for i in order[:take]:\n",
    "                selected.append(merged[i])\n",
    "\n",
    "    if not selected:\n",
    "        return {}\n",
    "\n",
    "    per_seg_fp = []\n",
    "    for i in selected:\n",
    "        mined = generate_frequent_patterns([i[\"seq\"]], min_sup=1, max_len=max_len_stage2)\n",
    "        mined = compress_patterns(mined)\n",
    "        per_seg_fp.append(mined)\n",
    "\n",
    "    if not per_seg_fp:\n",
    "        return {}\n",
    "\n",
    "    K = len(per_seg_fp)\n",
    "    need = K # max(1, ceil(1.0 * K))\n",
    "\n",
    "    appear_in = defaultdict(int)\n",
    "    support_sum = defaultdict(int)\n",
    "    for d in per_seg_fp:\n",
    "        for p, c in d.items():\n",
    "            appear_in[p] += 1\n",
    "            support_sum[p] += c\n",
    "\n",
    "    final_seq = {}\n",
    "    for p, kcount in appear_in.items():\n",
    "        if kcount >= need:\n",
    "            final_seq[p] = support_sum[p]\n",
    "\n",
    "    final_seq = compress_patterns(final_seq)\n",
    "    return final_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad88606",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_patterns = stage2_mining(\n",
    "    bad_seqs=bad_sequences,\n",
    "    discriminative_patterns=discriminative_patterns,\n",
    "    seg_width=50,\n",
    "    top_k=5,\n",
    "    max_len_stage2=None,\n",
    "    overlap=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d507d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([\n",
    "    {'Pattern': pattern, 'Bad Support': v['bad'], 'Good Support': v['good'], 'Difference': v['delta']}\n",
    "    for pattern, v in sorted(discriminative_patterns.items(), key=lambda x: -x[1]['delta'])\n",
    "])\n",
    "\n",
    "df2 = pd.DataFrame([\n",
    "    {'Pattern': pattern, 'Support': support}\n",
    "    for pattern, support in sorted(stage2_patterns.items(), key=lambda x: -x[1])\n",
    "])\n",
    "\n",
    "from IPython.display import display\n",
    "print(\"Stage 1: Discriminative Patterns\")\n",
    "display(df1)\n",
    "\n",
    "print(\"\\nStage 2: Infrequent Root-Cause Patterns (from top-K segments)\")\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_pattern_sorted = sorted(stage2_patterns.items(), key=lambda x: (-x[1], -len(x[0]), x[0]))\n",
    "top5_stage2_patterns = stage2_pattern_sorted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b569bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_stage2_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from math import ceil\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "start_mem = process.memory_info().rss / (1024 * 1024)  # in MB\n",
    "start_time = time.perf_counter() \n",
    "\n",
    "LABEL_PATH = './trace_data/mamba2/single_thread/version_4/faulty_data/gt_test_data_labels.json'\n",
    "MIN_COVERAGE = 0.60 \n",
    "\n",
    "def normalize_patterns(discrinimative_obj):\n",
    "    if discrinimative_obj is None:\n",
    "        return []\n",
    "    if isinstance(discrinimative_obj, dict):\n",
    "        seq = discrinimative_obj.keys()\n",
    "    else:\n",
    "        seq = discrinimative_obj\n",
    "\n",
    "    out = []\n",
    "    for p in seq:\n",
    "        if isinstance(p, (list, tuple, set)):\n",
    "            out.append(tuple(p))\n",
    "    return out\n",
    "\n",
    "def normalize_gt_sequences(gt_seq_list):\n",
    "    if not gt_seq_list:\n",
    "        return []\n",
    "    if isinstance(gt_seq_list[0], int):\n",
    "        return [gt_seq_list]\n",
    "    else:\n",
    "        return [seq for seq in gt_seq_list if isinstance(seq, list) and len(seq) > 0]\n",
    "\n",
    "with open(LABEL_PATH, \"r\") as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "discriminative_patterns_seq = normalize_patterns(discriminative_patterns)\n",
    "\n",
    "tp = fp = fn = tn = 0\n",
    "matched_patterns = set()   \n",
    "\n",
    "count = 0\n",
    "\n",
    "def check_subsequence(discriminative_pat, gt_seq, MIN_COVERAGE):\n",
    "    m = len(discriminative_pat)\n",
    "    n = len(gt_seq)\n",
    "\n",
    "    if m == 0 or m > n:\n",
    "        return False\n",
    "\n",
    "    for i in range(n - m + 1):\n",
    "        if gt_seq[i:i+m] == list(discriminative_pat):\n",
    "            coverage = m / n\n",
    "            return coverage >= MIN_COVERAGE\n",
    "\n",
    "    return False\n",
    "\n",
    "has_one_tp = False\n",
    "count_fp = 0\n",
    "\n",
    "matched_count = 0\n",
    "\n",
    "for fname, raw_gt in label_map.items():\n",
    "    gt_seqs_list = normalize_gt_sequences(raw_gt)\n",
    "    actual_positive = len(gt_seqs_list) > 0\n",
    "    predicted_positive = False\n",
    "\n",
    "\n",
    "    count = 0 \n",
    "    if actual_positive:\n",
    "\n",
    "        for gt_seqs in gt_seqs_list:\n",
    "            count_fp = 0\n",
    "            predicted_positive = False\n",
    "            has_one_tp = False\n",
    "            total_patterns = len(discriminative_patterns_seq)\n",
    "            matched_count = 0\n",
    "\n",
    "            for discriminative_pat in discriminative_patterns_seq:\n",
    "                is_match = check_subsequence(discriminative_pat, gt_seqs, MIN_COVERAGE)\n",
    "                if is_match:\n",
    "                    if not has_one_tp:\n",
    "                        tp = tp + 1\n",
    "                        has_one_tp = True\n",
    "                        matched_patterns.add(discriminative_pat)\n",
    "                    matched_count += 1\n",
    "\n",
    "            if not has_one_tp:\n",
    "                fn = fn + 1\n",
    "                fp = fp + total_patterns\n",
    "\n",
    "            else:\n",
    "                fp = fp + (total_patterns - matched_count)\n",
    "\n",
    "\n",
    "if tp + fp > 0:\n",
    "    precision = tp / (tp + fp)\n",
    "else:\n",
    "    precision = 0.0\n",
    "\n",
    "if tp + fn > 0:\n",
    "    recall = tp / (tp + fn)\n",
    "else:\n",
    "    recall = 0.0\n",
    "\n",
    "if (precision + recall) > 0:\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "else:\n",
    "    f1 = 0.0\n",
    "\n",
    "\n",
    "print(f\"TP={tp} FP={fp} FN={fn} TN={tn}\")\n",
    "print(f\"precision={precision:.3f}  recall={recall:.3f}  f1={f1:.3f}\")\n",
    "\n",
    "matched_patterns = sorted(matched_patterns, key=lambda x: (len(x), x))\n",
    "print(f\"\\nMatched {len(matched_patterns)} discriminative patterns with GT sequences:\")\n",
    "\n",
    "avg_value_length_tp_discriminative = 0\n",
    "for p in matched_patterns:\n",
    "    print(p)\n",
    "    avg_value_length_tp_discriminative += len(p)\n",
    "\n",
    "avg_value_length_tp_discriminative = avg_value_length_tp_discriminative / len(matched_patterns) if matched_patterns else 0\n",
    "\n",
    "end_mem = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "print(f\"Memory used: {end_mem - start_mem:.2f} MB\")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_ms = (end_time - start_time) * 1000\n",
    "print(f\"\\nTime taken: {elapsed_ms:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4132242",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_value_length_tp_discriminative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916fb6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
