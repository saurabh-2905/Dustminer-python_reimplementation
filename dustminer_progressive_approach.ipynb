{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ce3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dustminer Implementation\n",
    "# Loading the normal data as Good pile and Faulty data as Bad pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from libraries.utils import get_paths, read_traces, read_json, mapint2var, is_consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e01a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CODE = 'theft_protection'               ### application (code) theft_protection, mamba2, lora_ducy\n",
    "BEHAVIOUR_FAULTY = 'faulty_data/diag_subseq/subseq/'        ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal/'             ### normal, faulty_data\n",
    "THREAD = 'single'                       ### single, multi\n",
    "VER = 4                                 ### format of data collection\n",
    "\n",
    "base_dir = './trace_data'              ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(\"Normal base path:\", normalbase_path)\n",
    "print(\"Faulty base path:\", faultybase_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dcff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base_path = os.path.join(normalbase_path, 'train_data') #'diag_refsamples500')\n",
    "print(\"Train base path:\", train_base_path)\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "train_data_path = [os.path.join(train_base_path, x) for x in os.listdir(train_base_path)]\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in os.listdir(normalbase_path) if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "train_data_path = [x for x in train_data_path if '.DS_Store' not in x]\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "paths_log = [x for x in paths_log if '.DS_Store' not in x]\n",
    "paths_traces = [x for x in paths_traces if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "\n",
    "print(\"Number of training data files:\", len(train_data_path))\n",
    "print(\"Number of training varlist files:\", len(train_varlist_path))\n",
    "print(\"Number of faulty log files:\", len(paths_log))\n",
    "print(\"Number of faulty trace files:\", len(paths_traces))\n",
    "print(\"Number of faulty varlist files:\", len(varlist_path))\n",
    "print(\"Number of faulty label files:\", len(paths_label))\n",
    "\n",
    "paths_log.sort()\n",
    "paths_traces.sort()\n",
    "varlist_path.sort()\n",
    "paths_label.sort()\n",
    "\n",
    "test_data_path = paths_traces\n",
    "test_label_path = paths_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33afd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f87f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file in file_paths:\n",
    "        traces = read_traces(file)\n",
    "        if isinstance(traces, list) and len(traces) <=2:\n",
    "            # id_sequence = [trace for trace in traces]\n",
    "            id_sequence = traces[0]\n",
    "            print(\"id_sequence:\", id_sequence)\n",
    "\n",
    "        elif isinstance(traces, list) and len(traces) > 2:\n",
    "            id_sequence = [int(trace[0]) for trace in traces if isinstance(trace, list) and len(trace) >= 2]\n",
    "        \n",
    "        data.append(id_sequence)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_log_directory = train_data_path\n",
    "bad_log_directory = test_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_log_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_log_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sequences = load_data(good_log_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be98d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9281a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(good_sequences)):\n",
    "    print(f\"Good sequence {i} length: {len(good_sequences[i])}\")\n",
    "    count = count + len(good_sequences[i])\n",
    "print(\"Total good sequences length:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cfa59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sequences = load_data(bad_log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12855792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def get_ground_truth_file(path, ground_truth_files):\n",
    "    filename = []\n",
    "    for file in ground_truth_files:\n",
    "        if Path(path).stem in Path(file).stem:\n",
    "            print(\"from function - matched ground truth file is :\", file)\n",
    "            filename = file\n",
    "    return filename\n",
    "\n",
    "def get_trace_info(path):\n",
    "    filename = Path(path).stem\n",
    "    match = re.search(r\"(trace_trial_?\\d+)_(\\d+)-(\\d+)\", filename)\n",
    "    if match:\n",
    "        name = match.group(1)\n",
    "        start = int(match.group(2))\n",
    "        end = int(match.group(3))\n",
    "        test_data_name = name+'_'+str(start)+'-'+str(end)+'.json'\n",
    "        return name, start, end, test_data_name\n",
    "    else:\n",
    "        raise ValueError(\"Filename format not recognized\")\n",
    "\n",
    "def find_sequence_ground_truth(test_data_path, ground_truth):\n",
    "    trace = read_traces(test_data_path)\n",
    "    name, start, end,test_data_name = get_trace_info(test_data_path)\n",
    "    sequence = [int(ev[0]) for ev in trace if isinstance(ev, list) and len(ev) >= 2]\n",
    "    gt_start_end_pair = [[x[0], x[1]] for x in ground_truth]\n",
    "    return sequence, gt_start_end_pair\n",
    "\n",
    "\n",
    "def create_labels(sequence, gt_start_end_pair, test_data_start_index, test_data_end_index):\n",
    "    start_index = test_data_start_index\n",
    "    end_index = test_data_end_index\n",
    "    event_list = []\n",
    "    event_id_list = []\n",
    "    for start, end in gt_start_end_pair:\n",
    "        event_list = []\n",
    "        for event_id in range(start_index, end_index):\n",
    "            if event_id >= start and event_id <= end:\n",
    "                print(\"Event ID {} is in ground truth range ({}, {})\".format(event_id, start, end))\n",
    "                print(\"event_id - start_index:\", event_id , start_index)\n",
    "                event_list.append(sequence[event_id - start_index])\n",
    "        if event_list:\n",
    "            event_id_list.append(event_list)\n",
    "\n",
    "    return event_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.anomaly_detection import discover_test_files, load_ground_truth_dir, build_labels\n",
    "import json \n",
    " \n",
    "gt_path   = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/labels\" # example\n",
    "ground_truth_path = [os.path.join(gt_path, x) for x in os.listdir(gt_path)]\n",
    "\n",
    "print(\"ground truth path:\", ground_truth_path)\n",
    "\n",
    "new_label = {}\n",
    "\n",
    "for test_data in test_data_path:\n",
    "    print(\"Test data file:\", test_data)\n",
    "\n",
    "\n",
    "    print(\"---------------------------------------------------\")\n",
    "    test_data_name_1, test_data_start_index, test_data_end_index, test_data_name= get_trace_info(test_data)\n",
    "    print(\"Test data name is : \", test_data_name_1)\n",
    "    print(\"Test data start index is : \", test_data_start_index)\n",
    "    print(\"Test data end index is : \", test_data_end_index)\n",
    "\n",
    "\n",
    "    ground_truth_filename = get_ground_truth_file(test_data_name_1, ground_truth_path)\n",
    "    if not ground_truth_filename:\n",
    "        print(\"No matching ground truth file found for test data:\", test_data)\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        print(\"Ground truth file name is : \", ground_truth_filename)\n",
    "\n",
    "\n",
    "        ground_truth_raw = read_traces(ground_truth_filename)                                               # read ground truth labels from the label file\n",
    "        ground_truth = ground_truth_raw['labels']                                                # extract labels from dictionary from ground truth data\n",
    "\n",
    "        label_trace_name = list(ground_truth.keys())[0]\n",
    "        ground_truth = ground_truth[label_trace_name]\n",
    "\n",
    "        print(\"ground truth:\", ground_truth)\n",
    "\n",
    "        print(\"The test data file \", test_data, \" is not corresponding to ground truth file : \", ground_truth_filename)\n",
    "        print(\"The test data file \", test_data, \" is corresponding to ground truth file : \", ground_truth_filename)\n",
    "        sequence, gt_start_end_pair = find_sequence_ground_truth(test_data, ground_truth)\n",
    "        print(\"Event ID sequence is : \", sequence)\n",
    "        print(\"Ground truth start-end pairs are : \", gt_start_end_pair)\n",
    "        labels = create_labels(sequence, gt_start_end_pair, test_data_start_index, test_data_end_index)\n",
    "        print(\"Labels are : \", labels)\n",
    "        new_label[test_data_name] = labels\n",
    "        print(\"New label dictionary is : \", new_label)\n",
    "\n",
    "output_dir = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"gt_test_data_labels.json\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_label, f, indent=4)\n",
    "\n",
    "print(f\"\\n Saved to file: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fab146",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde484f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa048be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_max_length(seq, event_id):\n",
    "    max_length = 0\n",
    "    start_index = seq.index(event_id)\n",
    "    end_index = 0\n",
    "    for i in range(start_index + 1 , len(seq)):\n",
    "        if seq[i] == event_id:\n",
    "            end_index = i - start_index\n",
    "            if end_index > max_length:\n",
    "                max_length = end_index\n",
    "            start_index = i\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def max_gap_two_lists(sequences):\n",
    "    max_length = 0\n",
    "    events = []\n",
    "    event_id_max_length = {}\n",
    "\n",
    "    for seq in sequences:\n",
    "        if seq and isinstance(seq[0], list):\n",
    "            inner_sequences = seq\n",
    "        else:\n",
    "            inner_sequences = [seq]  \n",
    "        for inner_seq in inner_sequences:\n",
    "            for i, x in enumerate(inner_seq):\n",
    "                gap = get_max_length(inner_seq, x)\n",
    "                if x not in event_id_max_length:\n",
    "                    event_id_max_length[x] = gap\n",
    "                    events.append(x)\n",
    "                else:\n",
    "                    if gap > event_id_max_length[x]:\n",
    "                        event_id_max_length[x] = gap\n",
    "\n",
    "                if event_id_max_length[x] > max_length:\n",
    "                    max_length = event_id_max_length[x]\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a26b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequences_1 = good_sequences\n",
    "\n",
    "MAX_PATTERN_LEN = max_gap_two_lists(all_sequences_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac25f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PATTERN_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_same_index(seq):\n",
    "    \"\"\"\"\n",
    "    This function is used to compute next occurence of every element in the sequence.\n",
    "    parameters :\n",
    "        seq - list(int)\n",
    "    Returns:\n",
    "        nxt - list(int) where nxt[i] is the index of the next occurrence of seq[i] in seq and n if none.\n",
    "    \"\"\"\n",
    "    last_pos = {}\n",
    "    n = len(seq)\n",
    "    nxt = [n] * n\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        v = seq[i]\n",
    "        if v in last_pos:\n",
    "            nxt[i] = last_pos[v]\n",
    "        last_pos[v] = i\n",
    "    return nxt\n",
    "\n",
    "\n",
    "def dynamic_window_sequence(sequences):\n",
    "    \"\"\"\n",
    "    This function generates dynamic windows for each sequence in sequences.\n",
    "    For example, for sequence [1,2,3,1,4,2], the dynamic windows are: [1,2,3],[1,4,2]\n",
    "    parameters :\n",
    "        sequences - list of list(int)\n",
    "    Returns:\n",
    "        final_windows - list of list(int) containing all dynamic windows from all sequences.\n",
    "    \"\"\"\n",
    "    final_windows = []\n",
    "    for seq in sequences:\n",
    "        if not seq:\n",
    "            continue\n",
    "        nxt = next_same_index(seq)\n",
    "        n = len(seq)\n",
    "        for i in range(n):\n",
    "            j = nxt[i]\n",
    "            if j > i:\n",
    "                final_windows.append(seq[i:j])\n",
    "    return final_windows\n",
    "\n",
    "def has_substring(pattern, sequence):\n",
    "    \"\"\"\n",
    "    This function checks if the given pattern is a subsequence of sequence.\n",
    "    parameters : \n",
    "        pattern - list(int)\n",
    "        sequence - list(int)\n",
    "    Returns:\n",
    "        True if the pattern appears as a continous match in sequence.\n",
    "        False if no match\n",
    "    \"\"\"\n",
    "    pattern = tuple(pattern)\n",
    "    sequence = tuple(sequence)\n",
    "    m = len(pattern)\n",
    "    n = len(sequence)\n",
    "    if m == 0:\n",
    "        return True\n",
    "    if m > n:\n",
    "        return False\n",
    "    for i in range(n - m + 1):\n",
    "        if sequence[i:i+m] == pattern:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def compress_patterns(patterns):\n",
    "    \"\"\"\n",
    "    Compress frequent patterns by removing redundant shorter ones.\n",
    "    For each pattern P, if there exists a longer pattern Q such that:\n",
    "        - P appears as a CONTIGUOUS substring of Q, and has support(P) == support(Q), then P is dropped (only Q is kept).\n",
    "\n",
    "    Parameters:\n",
    "        patterns (dict[tuple[int, ...], int]): Dictionary mapping each pattern tuple to its support count.\n",
    "\n",
    "    Returns:\n",
    "        dict[tuple[int, ...], int]: Compressed dictionary containing only longest unique-support patterns.\n",
    "\n",
    "    For example:\n",
    "        Input  : {(6,7):3, (6,7,8):3, (6,8):2}\n",
    "        Output : {(6,7,8):3, (6,8):2} Because (6,7) is a substring of (6,7,8) and both have same support=3.\n",
    "    \"\"\"\n",
    "    items = list(patterns.items())\n",
    "    items.sort(key=lambda x: (-len(x[0]), x[0]))\n",
    "    compressed_patterns = {}\n",
    "    for pattern, support in items:\n",
    "        drop = False\n",
    "        for other in compressed_patterns.keys():\n",
    "            if patterns[other] == support and has_substring(pattern, other):\n",
    "                drop = True\n",
    "                break\n",
    "        if not drop:\n",
    "            compressed_patterns[pattern] = support\n",
    "    return compressed_patterns\n",
    "\n",
    "def check_ordered_support(pattern, windows):\n",
    "    if not windows:\n",
    "        return 0.0\n",
    "\n",
    "    print(\"pattern from check is : \", pattern)\n",
    "    print(\"pattern from check is : \", pattern[0])\n",
    "\n",
    "    match_count = 0\n",
    "    pattern1 = pattern[1:]\n",
    "    for window in windows:\n",
    "        i = 1\n",
    "        j = 0\n",
    "        while i < len(window) and j < len(pattern1):\n",
    "            if window[i] == pattern1[j]:\n",
    "                j = j + 1\n",
    "            i = i + 1\n",
    "\n",
    "        if j == len(pattern1):\n",
    "            match_count += 1\n",
    "            \n",
    "    return match_count / len(windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5774ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def generate_cartesian_event(sequence, delta=0.0):\n",
    "    seq = sequence\n",
    "    print(f\"Sequence: {seq}\")\n",
    "    print(\"Length:\", len(seq))\n",
    "\n",
    "    dynamic_windows = dynamic_window_sequence([seq]) \n",
    "    # print(\"Dynamic windows generated:\", dynamic_windows)\n",
    "    sorted_dynamic_windows = defaultdict(list)\n",
    "    for _win in dynamic_windows:\n",
    "        key = _win[0]\n",
    "        sorted_dynamic_windows[key].append(_win)\n",
    "\n",
    "    print(\"sorted_dynamic_windows:\", sorted_dynamic_windows)\n",
    "    seq_length = len(seq)\n",
    "    S1 = list(set(seq))\n",
    "    infile_support = {e: seq.count(e)/seq_length for e in S1}\n",
    "\n",
    "    print(\"S1 is \", infile_support)\n",
    "    \n",
    "    min_val = min(infile_support.values())\n",
    "    max_val = max(infile_support.values())\n",
    "    \n",
    "    normalized_infile_value = {}\n",
    "    if min_val == max_val:\n",
    "        normalized_infile_value = {e: 1.0 for e in infile_support}\n",
    "    else:\n",
    "        for e, v in infile_support.items():\n",
    "            normalized_infile_value[e] = (v - min_val) / (max_val - min_val)\n",
    "    \n",
    "    print(\"Normalized S1 is \", normalized_infile_value)\n",
    "    s1_new = {k: v for k, v in normalized_infile_value.items() if v >= delta}\n",
    "    event_id_s1 = list(s1_new.keys())\n",
    "    \n",
    "    print(f\"S1 Generated: {len(s1_new)} items\")\n",
    "    print(\"S1\", s1_new)\n",
    "    print('event_id_s1:', event_id_s1)\n",
    "\n",
    "    S2_candidates = set()\n",
    "    for a in range(len(event_id_s1)):\n",
    "        for b in range(len(event_id_s1)):\n",
    "            if a != b: \n",
    "                S2_candidates.add((event_id_s1[a], event_id_s1[b]))\n",
    "\n",
    "    all_patterns = {}\n",
    "    s2_with_support = {}\n",
    "    \n",
    "    print(f\"Total S2 candidates: {S2_candidates}\")\n",
    "    for pair in S2_candidates:\n",
    "        print(\"Evaluating pair:\", pair)\n",
    "        print(\"Evaluating pair:\", pair[0])\n",
    "        support = check_ordered_support(pair, sorted_dynamic_windows[pair[0]])\n",
    "        if support >= delta:\n",
    "            s2_with_support[pair] = support\n",
    "\n",
    "    print(\"S2 count is :\", len(s2_with_support))\n",
    "    print(\"S2: \", s2_with_support)\n",
    "    all_patterns[2] = s2_with_support\n",
    "\n",
    "    current_patterns = s2_with_support\n",
    "    S1_items = list(s1_new.keys())\n",
    "    k = 3\n",
    "\n",
    "    while k<=MAX_PATTERN_LEN:\n",
    "        candidates = set()\n",
    "        \n",
    "        for pattern in current_patterns.keys(): \n",
    "            for item in S1_items:\n",
    "                if item not in pattern:\n",
    "                    new_cand = tuple(list(pattern) + [item])\n",
    "                    candidates.add(new_cand)\n",
    "                            \n",
    "        next_patterns = {}\n",
    "        for m in candidates:\n",
    "            # support = check_ordered_support(m, dynamic_windows)\n",
    "            support = check_ordered_support(m, sorted_dynamic_windows[m[0]])\n",
    "            if support >= delta:\n",
    "                next_patterns[m] = support\n",
    "        \n",
    "        if not next_patterns:\n",
    "            break\n",
    "\n",
    "        compressed_pattern = compress_patterns(next_patterns)\n",
    "\n",
    "        print(f\"S{k}: \", len(compressed_pattern))\n",
    "        print(f\"S{k}: \", compressed_pattern)\n",
    "\n",
    "        all_patterns[k] = compressed_pattern\n",
    "        current_patterns = compressed_pattern   \n",
    "        k += 1\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return all_patterns, sorted_dynamic_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f40fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def support_patterns(sequences, patterns):\n",
    "    \"\"\"\n",
    "    Compute per-sequence support for continuous patterns using dynamic windows.\n",
    "    For each pattern p (a tuple of event IDs), compute:\n",
    "      - across: number of sequences in which p occurs at least once as a continuous substring inside any dynamic window.\n",
    "      - in_file_avg: average number of continous occurrences per sequence, computed ONLY over sequence where p occurs (current sequence).\n",
    "\n",
    "    Computing support:\n",
    "      1) For each file (sequence), we build dynamic windows and group them by their anchor (window[0]).\n",
    "      2) For a pattern p = (a, ...), we look only at windows anchored at 'a' and sum count_substring(p, window) across those windows.\n",
    "      3) If the summed count for that sequence > 0, the sequence contributes to 'across' and contributes its count toward the present-only average.\n",
    "\n",
    "    Parameters:\n",
    "        sequences (list[list[int]]): List traces as sequences of event IDs.\n",
    "        patterns (Iterable[tuple[int, ...]]): Patterns to measure.\n",
    "\n",
    "    Returns:\n",
    "        dict[tuple[int, ...], dict]: For each pattern p, a dict with:\n",
    "            {\n",
    "              \"across\": int,\n",
    "              \"in_file_avg\": float\n",
    "            }\n",
    "    \"\"\"\n",
    "    support = {}\n",
    "    if not patterns:\n",
    "        return support\n",
    "\n",
    "    sequence_window_by_anchor = []\n",
    "    for seq in sequences:\n",
    "        seq_1 = {}\n",
    "        for w in dynamic_window_sequence([seq]):\n",
    "            if not w:\n",
    "                continue\n",
    "            a = w[0]\n",
    "            seq_1.setdefault(a, []).append(w)\n",
    "        sequence_window_by_anchor.append(seq_1)\n",
    "\n",
    "    per_sequence_counts = {p: [0]*len(sequences) for p in patterns}\n",
    "\n",
    "    for i, t in enumerate(sequence_window_by_anchor):\n",
    "        for p in patterns:\n",
    "            if not p:\n",
    "                continue\n",
    "            a = p[0]\n",
    "            cnt = 0\n",
    "            for w in t.get(a, []):\n",
    "                cnt += count_substring(p, w)          \n",
    "            per_sequence_counts[p][i] = cnt\n",
    "\n",
    "    for p, vec in per_sequence_counts.items():\n",
    "        present = sum(1 for v in vec if v > 0)\n",
    "        total = sum(v for v in vec if v > 0)\n",
    "        support[p] = {\"across\": present, \"in_file_avg\": (float(total) / float(present)) if present > 0 else 0.0}\n",
    "    return support\n",
    "\n",
    "def FindDiscriminative(A, B, num_A, num_B, theta=0.8, delta=0.8):\n",
    "    \"\"\"\n",
    "    Identify discriminative patterns that occur significantly more often in one group of sequences (A) than in another group (B), based on ratio thresholds.\n",
    "    A pattern p is marked as discriminative if any of the following hold:\n",
    "    1. B has zero evidence of the pattern: - B_stats[p][\"across\"] == 0, or B_stats[p][\"in_file_avg\"] == 0\n",
    "    2. The ratio of presence across sequences exceeds theta: (across_A / across_B) >= theta\n",
    "    3. The ratio of average in-sequence occurrences exceeds delta: (in_file_avg_A / in_file_avg_B) >= delta\n",
    "\n",
    "    Parameters:\n",
    "        A : dict[tuple[int, ...], dict] - Support statistics for patterns in group A (e.g., bad or good sequences).\n",
    "            Each entry has:\n",
    "                {\n",
    "                \"across\": int,        # number of sequences where pattern appears\n",
    "                \"in_seq_avg\": float   # avg. number of continous occurrences per sequence\n",
    "                }\n",
    "        B : dict[tuple[int, ...], dict] - Equivalent support statistics for patterns in group B.\n",
    "        num_A : int - Number of sequences in group A.\n",
    "        num_B : int - Number of sequences in group B.\n",
    "        theta : float, default=0.8 - Threshold for \"across-sequence\" ratio test (presence ratio).\n",
    "        delta : float, default=0.8 - Threshold for \"in-sequence\" ratio test (average frequency ratio).\n",
    "\n",
    "    Returns:\n",
    "        set[tuple[int, ...]] - Set of patterns that are discriminative for group A compared to B.\n",
    "\n",
    "    for exmaple::\n",
    "        A_stats = { (6,7,8): {\"across\": 5, \"in_file_avg\": 2.3} }\n",
    "        B_stats = { (6,7,8): {\"across\": 1, \"in_file_avg\": 0.4} }\n",
    "        FindDiscriminative(A_stats, B_stats, nA=10, nB=10, theta=0.8, delta=0.8)\n",
    "        output - {(6,7,8)}\n",
    "    \"\"\"\n",
    "    discriminative = set()\n",
    "    for p in A:\n",
    "        support_a = A[p]\n",
    "        if p not in B:\n",
    "            discriminative.add(p)\n",
    "            continue\n",
    "        support_b = B[p]\n",
    "\n",
    "        seq_a = float(support_a[\"across\"]) / float(num_A) if num_A > 0 else 0.0\n",
    "        seq_b = float(support_b[\"across\"]) / float(num_B) if num_B > 0 else 0.0\n",
    "        per_seq_count_A = support_a[\"in_file_avg\"]\n",
    "        per_seq_count_B = support_b[\"in_file_avg\"]\n",
    "\n",
    "        if seq_b == 0.0 or per_seq_count_B == 0.0:\n",
    "            discriminative.add(p)\n",
    "            continue\n",
    "\n",
    "        if seq_b > 0.0 and (seq_a / seq_b) >= theta:\n",
    "            discriminative.add(p)\n",
    "            continue\n",
    "        if per_seq_count_B > 0.0 and (per_seq_count_A / per_seq_count_B) >= delta:\n",
    "            discriminative.add(p)\n",
    "            continue\n",
    "    return discriminative\n",
    "\n",
    "def mine_discriminative_patterns_progressive(good_seqs,bad_seqs,min_sup,theta=0.8,delta=0.8,k_start=1,k_max=10,stop_when_found=True,normalize=True,repeated_event_id=False):\n",
    "    \"\"\"\n",
    "    Mine and score discriminative continous patterns between two groups of sequences good vs bad using a two-stage DustMiner-style pipeline.\n",
    "    Stage 1 — Progressive discriminative mining\n",
    "      • For K = k_start..k_max:\n",
    "          - Generate frequent K-grams separately from good and bad sequences via GenerateFrequentSubSequences.\n",
    "          - Compute per-pattern support stats in each group for both across and in seqeunce group.\n",
    "          - Mark patterns discriminative via FindDiscriminative using θ (across ratio) and δ (in-sequence ratio).\n",
    "          - Update Scommon = intersection of frequent K-grams (good ∩ bad) for next K.\n",
    "      • At the end of Stage 1, we keep patterns discriminative for the bad group.\n",
    "\n",
    "    Stage 2 — Quantitative scoring\n",
    "      • Determine longest discriminative length (max_len_found).\n",
    "      • Build full frequent pattern lattices up to max_len_found for each group via generate_frequent_patterns(...) to get supports for all prefixes.\n",
    "      • Compute normalized probabilities P_bad(p) and P_good(p) using a chain rule over continous extensions; score each pattern with delta = P_bad - P_good.\n",
    "      • Return patterns with positive delta, sorted by (delta, p, length p).\n",
    "\n",
    "    Parameters:\n",
    "        good_seqs : list[list[int]] - Traces labeled as good (normal data).\n",
    "        bad_seqs : list[list[int]] - Traces labeled as bad (faulty).\n",
    "        min_sup : int - Minimum window-support threshold used in Stage 1\n",
    "        theta : float, default=0.8 - Across-sequence presence ratio threshold (A/B) in FindDiscriminative.\n",
    "        delta : float, default=0.8 - In-sequence average count ratio threshold (A/B) in FindDiscriminative.\n",
    "        k_start : int, default=1 - Initial pattern length for progressive mining.\n",
    "        k_max : int - Maximum pattern length to mine during Stage 1.\n",
    "        stop_when_found : bool, default=True - If True, break the mining as soon as any discriminative set sequenceGood and sequencebad is non-empty.\n",
    "        normalize : bool, default=True - If True, compute Stage-2 chain probabilities; if False, fall back to across-sequence rates.\n",
    "        repeated_event_id : bool, default=False - allow non-adjacent repeats if True and adjacent duplicates are always disallowed.\n",
    "\n",
    "    Returns:\n",
    "        dict[tuple[int, ...], dict] - Mapping each discriminative pattern p to: {\"bad\": float, \"good\": float, \"delta\": float} sorted by decreasing delta\n",
    "\n",
    "    \"\"\"\n",
    "    Scommon = set()\n",
    "    good_discriminative = set()\n",
    "    bad_discriminative = set()\n",
    "    K = k_start\n",
    "    length_good_seq = len(good_seqs)\n",
    "    length_bad_seq = len(bad_seqs)\n",
    "\n",
    "    while True:\n",
    "        if K > k_max:\n",
    "            break\n",
    "\n",
    "        k_frequent_good_patterns = generateFrequentSubSequences(good_seqs, K, Scommon, min_sup, repeated_event_id=repeated_event_id)\n",
    "        k_frequent_bad_patterns = generateFrequentSubSequences(bad_seqs,  K, Scommon, min_sup, repeated_event_id=repeated_event_id)\n",
    "        print(\"Good sequence mine patterns at stage : \",K,\" is : \", k_frequent_good_patterns)\n",
    "        print(\"Bad sequence mine patterns at stage :\",K, \" is :\", k_frequent_bad_patterns)\n",
    "\n",
    "        if not k_frequent_good_patterns and not k_frequent_bad_patterns:\n",
    "            break\n",
    "\n",
    "        good_seq_support = support_patterns(good_seqs, set(k_frequent_good_patterns.keys()) if k_frequent_good_patterns else set())\n",
    "        bad_seq_support = support_patterns(bad_seqs,  set(k_frequent_bad_patterns.keys())  if k_frequent_bad_patterns else set())\n",
    "\n",
    "        if k_frequent_good_patterns:\n",
    "            discriminative_good = FindDiscriminative(good_seq_support, bad_seq_support, length_good_seq, length_bad_seq, theta=theta, delta=delta)\n",
    "            for p in discriminative_good:\n",
    "                if p in k_frequent_good_patterns:\n",
    "                    good_discriminative.add(p)\n",
    "\n",
    "        if k_frequent_bad_patterns:\n",
    "            discriminative_bad = FindDiscriminative(bad_seq_support, good_seq_support, length_bad_seq, length_good_seq, theta=theta, delta=delta)\n",
    "            for p in discriminative_bad:\n",
    "                if p in k_frequent_bad_patterns:\n",
    "                    bad_discriminative.add(p)\n",
    "\n",
    "        Scommon = findCommon(k_frequent_good_patterns, k_frequent_bad_patterns)\n",
    "        K += 1\n",
    "        if stop_when_found:\n",
    "            if len(good_discriminative) > 0 or len(bad_discriminative) > 0:\n",
    "                break\n",
    "\n",
    "    patterns = set(bad_discriminative)\n",
    "    if not patterns:\n",
    "        return {}\n",
    "\n",
    "    max_len_found = 1\n",
    "    for p in patterns:\n",
    "        if len(p) > max_len_found:\n",
    "            max_len_found = len(p)\n",
    "\n",
    "    full_bad  = generate_frequent_patterns(bad_seqs,  min_sup, max_len=max_len_found, repeated_event_id=repeated_event_id)\n",
    "    full_good = generate_frequent_patterns(good_seqs, min_sup, max_len=max_len_found, repeated_event_id=repeated_event_id)\n",
    "\n",
    "    def prefix_out_sum(full):\n",
    "        i = defaultdict(int)\n",
    "        for q, c in full.items():\n",
    "            if len(q) >= 2:\n",
    "                i[q[:-1]] += c\n",
    "        return i\n",
    "\n",
    "    bad_out  = prefix_out_sum(full_bad)\n",
    "    good_out = prefix_out_sum(full_good)\n",
    "\n",
    "    def prob_chain(pattern, full, out, total_events):\n",
    "        if total_events <= 0:\n",
    "            return 0.0\n",
    "        if len(pattern) == 1:\n",
    "            return float(full.get(pattern, 0)) / float(total_events)\n",
    "        val = float(full.get((pattern[0],), 0)) / float(total_events)\n",
    "        if val == 0.0:\n",
    "            return 0.0\n",
    "        for i in range(1, len(pattern)):\n",
    "            pref = pattern[:i]\n",
    "            n = float(full.get(pattern[:i+1], 0))\n",
    "            d = float(out.get(pref, 0))\n",
    "            if n == 0.0 or d == 0.0:\n",
    "                return 0.0\n",
    "            val *= (n / d)\n",
    "        return val\n",
    "\n",
    "    total_bad  = sum(len(s) for s in bad_seqs)\n",
    "    total_good = sum(len(s) for s in good_seqs)\n",
    "\n",
    "    discriminative = {}\n",
    "    for p in patterns:\n",
    "        if normalize:\n",
    "            b = prob_chain(p, full_bad,  bad_out,  total_bad)\n",
    "            g = prob_chain(p, full_good, good_out, total_good)\n",
    "        else:\n",
    "            bad_support = support_patterns(bad_seqs,  [p])\n",
    "            good_support = support_patterns(good_seqs, [p])\n",
    "            b = float(bad_support.get(p, {\"across\":0})[\"across\"]) / float(max(1, length_bad_seq))\n",
    "            g = float(good_support.get(p, {\"across\":0})[\"across\"]) / float(max(1, length_good_seq))\n",
    "        d = b - g\n",
    "        if d > 0:\n",
    "            discriminative[p] = {\"bad\": b, \"good\": g, \"delta\": d} \n",
    "\n",
    "    discriminative_sorted = dict(sorted(discriminative.items(), key=lambda x: (-x[1][\"delta\"], -len(x[0]), x[0])))\n",
    "    return discriminative_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b23e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sequence_patterns = []\n",
    "for i in range(0, len(good_sequences)):\n",
    "    print(f\"length of {i} good file is : {len(good_sequences[i])}\")\n",
    "    good_sequence_patterns.append(generate_cartesian_event(good_sequences[i]))\n",
    "\n",
    "\n",
    "# bad_sequence_patterns = []\n",
    "# for i in range(0, len(bad_sequences)):\n",
    "#     print(f\"length of {i} bad file is : {len(bad_sequences[i])}\")\n",
    "#     bad_sequence_patterns.append(generate_cartesian_event(bad_sequences[i]))\n",
    "\n",
    "\n",
    "good_sequence_patterns = generate_cartesian_event(good_sequences[0])\n",
    "# bad_sequence_patterns = generate_cartesian_event(bad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5439375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sequence_patterns(pattern_list_of_dicts):\n",
    "    all_supports = {}\n",
    "    \n",
    "    for seq_patterns in pattern_list_of_dicts:\n",
    "        for pattern, support in seq_patterns.items():\n",
    "            if pattern not in all_supports:\n",
    "                all_supports[pattern] = []\n",
    "            \n",
    "            all_supports[pattern].append(support)\n",
    "\n",
    "    final_patterns = {}\n",
    "    for pattern, support_list in all_supports.items():\n",
    "        average_val = sum(support_list) / len(support_list)\n",
    "        final_patterns[pattern] = average_val\n",
    "        \n",
    "    return final_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_good_patterns = merge_sequence_patterns(good_sequence_patterns)\n",
    "print(\"total Good Patterns : \", len(merged_good_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e1d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_good_patterns = [merged_good_patterns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a0b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sequence_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminative_score = {}\n",
    "\n",
    "#Here we take all the distinct event patterns from both bad and good logs and store in all_patterns.\n",
    "\n",
    "if isinstance(merged_good_patterns, list):\n",
    "    good_seqs = merged_good_patterns[0] \n",
    "else:\n",
    "    good_seqs = merged_good_patterns\n",
    "\n",
    "all_patterns = set(good_seqs.keys())\n",
    "\n",
    "for d in bad_sequence_patterns:\n",
    "    all_patterns.update(d.keys())\n",
    "\n",
    "print(\"all_patterns :\", len(all_patterns))\n",
    "\n",
    "for pattern in all_patterns:\n",
    "    support_good = good_seqs.get(pattern, 0.0)\n",
    "    bad_seq_count = [d[pattern] for d in bad_sequence_patterns if pattern in d]\n",
    "    \n",
    "    if bad_seq_count:\n",
    "        support_bad = sum(bad_seq_count) / len(bad_seq_count)\n",
    "    else:\n",
    "        support_bad = 0.0\n",
    "\n",
    "    score = support_good - support_bad\n",
    "    discriminative_score[pattern] = score\n",
    "\n",
    "sorted_patterns = sorted(discriminative_score.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "threshold = 0.30\n",
    "\n",
    "discriminative_patterns = {}\n",
    "for pat, score in sorted_patterns:\n",
    "    if abs(score) >= threshold:\n",
    "        discriminative_patterns[pat] = {\"support\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debef892",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminative_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4425c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "start_mem = process.memory_info().rss / (1024 * 1024)  # in MB\n",
    "start_time = time.perf_counter() \n",
    "\n",
    "LABEL_PATH = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/gt_test_data_labels.json\"\n",
    "\n",
    "MIN_COVERAGE = 0.60 \n",
    "\n",
    "def normalize_patterns(discrinimative_obj):\n",
    "    if discrinimative_obj is None:\n",
    "        return []\n",
    "    if isinstance(discrinimative_obj, dict):\n",
    "        seq = discrinimative_obj.keys()\n",
    "    else:\n",
    "        seq = discrinimative_obj\n",
    "\n",
    "    out = []\n",
    "    for p in seq:\n",
    "        if isinstance(p, (list, tuple, set)):\n",
    "            out.append(tuple(p))\n",
    "    return out\n",
    "\n",
    "def normalize_gt_sequences(gt_seq_list):\n",
    "    if not gt_seq_list:\n",
    "        return []\n",
    "    if isinstance(gt_seq_list[0], int):\n",
    "        return [gt_seq_list]\n",
    "    else:\n",
    "        return [seq for seq in gt_seq_list if isinstance(seq, list) and len(seq) > 0]\n",
    "\n",
    "with open(LABEL_PATH, \"r\") as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "discriminative_patterns_seq = normalize_patterns(discriminative_patterns)\n",
    "\n",
    "def check_subsequence(discriminative_pat, gt_seq, MIN_COVERAGE):\n",
    "    m = len(discriminative_pat)\n",
    "    n = len(gt_seq)\n",
    "\n",
    "    if m == 0 or m > n:\n",
    "        return False\n",
    "\n",
    "    for i in range(n - m + 1):\n",
    "        if gt_seq[i:i+m] == list(discriminative_pat):\n",
    "            coverage = m / n\n",
    "            return coverage >= MIN_COVERAGE\n",
    "\n",
    "    return False\n",
    "\n",
    "tp = fp = fn = tn = 0\n",
    "matched_patterns_global = set()\n",
    "\n",
    "all_tp = []         \n",
    "all_fp = []        \n",
    "all_fn = []        \n",
    "all_gt = []        \n",
    "y_true_all = []     \n",
    "y_pred_all = []     \n",
    "\n",
    "label_file_path = LABEL_PATH \n",
    "\n",
    "for test_file_name, raw_gt in label_map.items():\n",
    "    gt_seqs_list = normalize_gt_sequences(raw_gt)\n",
    "\n",
    "    all_gt.append((test_file_name, gt_seqs_list, label_file_path))\n",
    "\n",
    "    correct_pred_file = [] # To store that GT_seq that matched discriminative patterns  \n",
    "    rest_pred_file = [] # stores which patterns did not match gt sequence     \n",
    "    false_neg_file = [] # for fn sequence\n",
    "\n",
    "    total_patterns = len(discriminative_patterns_seq)  # no of patterns mined by dustminer\n",
    "\n",
    "    for gt_seq in gt_seqs_list:\n",
    "        y_true_all.append(1)   # appending 1 as gt label for every sequence is 1 fault present\n",
    "\n",
    "        matched_for_gt = []\n",
    "        for discriminative_patterns_1 in discriminative_patterns_seq:\n",
    "            if check_subsequence(discriminative_patterns_1, gt_seq, MIN_COVERAGE): # check if there is a complete match or atleast 60% match \n",
    "                matched_for_gt.append(discriminative_patterns_1)\n",
    "                matched_patterns_global.add(discriminative_patterns_1) # set of all discriminative_patterns that match at least one gt sequence\n",
    "\n",
    "        matched_count = len(matched_for_gt) # no of patterns that matched this gt sequence\n",
    "\n",
    "        if matched_count == 0: # when no patterns match for this particular gt then its FN\n",
    "            fn += 1\n",
    "            fp += total_patterns   \n",
    "            false_neg_file.append(gt_seq)\n",
    "            rest_pred_file.append((gt_seq, list(discriminative_patterns_seq))) # appending this gt_seq and all patterns as its FN\n",
    "            y_pred_all.append(0)  # no detection so appending 0\n",
    "\n",
    "        else: # when atleast one pattern match for this particular gt then its TP\n",
    "            tp += 1\n",
    "            fp += (total_patterns - matched_count)\n",
    "\n",
    "            fp_for_gt = [\n",
    "                p for p in discriminative_patterns_seq\n",
    "                if p not in matched_for_gt\n",
    "            ]\n",
    "\n",
    "            correct_pred_file.append((gt_seq, matched_for_gt))\n",
    "            rest_pred_file.append((gt_seq, fp_for_gt)) # appending gt and remaining unmatched patterns\n",
    "            y_pred_all.append(1)  # fault detected so appending 1\n",
    "\n",
    "    all_tp.append((test_file_name, correct_pred_file, label_file_path))\n",
    "    all_fp.append((test_file_name, rest_pred_file, label_file_path))\n",
    "    all_fn.append((test_file_name, false_neg_file, label_file_path))\n",
    "\n",
    "if tp + fp > 0:\n",
    "    precision = tp / (tp + fp)\n",
    "else:\n",
    "    precision = 0.0\n",
    "\n",
    "if tp + fn > 0:\n",
    "    recall = tp / (tp + fn)\n",
    "else:\n",
    "    recall = 0.0\n",
    "\n",
    "if (precision + recall) > 0:\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "else:\n",
    "    f1 = 0.0\n",
    "\n",
    "print(f\"TP={tp} FP={fp} FN={fn} TN={tn}\")\n",
    "print(f\"precision={precision:.3f}  recall={recall:.3f}  f1={f1:.3f}\")\n",
    "\n",
    "matched_patterns = sorted(matched_patterns_global, key=lambda x: (len(x), x))\n",
    "print(f\"\\nMatched {len(matched_patterns)} discriminative patterns with GT sequences:\")\n",
    "\n",
    "avg_value_length_tp_discriminative = 0\n",
    "for p in matched_patterns:\n",
    "    print(p)\n",
    "    avg_value_length_tp_discriminative += len(p)\n",
    "\n",
    "avg_value_length_tp_discriminative = (\n",
    "    avg_value_length_tp_discriminative / len(matched_patterns)\n",
    "    if matched_patterns else 0\n",
    ")\n",
    "\n",
    "end_mem = process.memory_info().rss / (1024 * 1024)\n",
    "print(f\"Memory used: {end_mem - start_mem:.2f} MB\")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_ms = (end_time - start_time) * 1000\n",
    "print(f\"\\nTime taken: {elapsed_ms:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "classwise_fn = defaultdict(list)\n",
    "classwise_tp = defaultdict(list)\n",
    "gt_len = 0\n",
    "\n",
    "CLASS_LABEL_PATH = f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/diag_subseq/subseq/subseq_labels/subseq_class.json\"\n",
    "\n",
    "with open(CLASS_LABEL_PATH, 'r') as f:\n",
    "    class_mapping = json.load(f)\n",
    "\n",
    "for file_fn, file_gt in zip(all_fn, all_gt):\n",
    "    test_filename = file_gt[0].replace('.json', '')\n",
    "    \n",
    "    fn = file_fn[1] \n",
    "    gt = file_gt[1] \n",
    "    \n",
    "    if test_filename in class_mapping:\n",
    "        class_ids = class_mapping[test_filename]\n",
    "        \n",
    "        if len(gt) != len(class_ids):\n",
    "            print(f\"Mismatch in {test_filename}. GT len: {len(gt)}, Class ID len: {len(class_ids)}\")\n",
    "            continue\n",
    "\n",
    "        for i, label in enumerate(gt):\n",
    "            current_class_id = class_ids[i] \n",
    "            if label in fn:\n",
    "                classwise_fn[current_class_id].append(label)\n",
    "            else:\n",
    "                classwise_tp[current_class_id].append(label)\n",
    "    else:\n",
    "        print(f\"{test_filename} not found in class mapping JSON.\")\n",
    "\n",
    "    gt_len += len(gt)\n",
    "    # print('file gt:', len(gt))\n",
    "    # print('file fn:', len(fn))\n",
    "    # print('\\n')\n",
    "    # break\n",
    "\n",
    "total_fn = 0\n",
    "total_tp = 0\n",
    "keys = set(list(classwise_fn.keys()) + list(classwise_tp.keys()))\n",
    "# print('keys:', keys)\n",
    "class_recall = []\n",
    "for key in keys:\n",
    "    print('class:', key)\n",
    "    total_fn += len(classwise_fn[key])\n",
    "    total_tp += len(classwise_tp[key])\n",
    "\n",
    "    crecall = len(classwise_tp[key])/(len(classwise_fn[key])+len(classwise_tp[key]))\n",
    "\n",
    "    # print('not detected:', len(classwise_fn[key]))\n",
    "    print('detected:', len(classwise_tp[key]))\n",
    "    print('total anomalies:', len(classwise_fn[key])+len(classwise_tp[key]))\n",
    "    print('Recall (classwise):', crecall)\n",
    "    print('\\n')\n",
    "\n",
    "    class_recall.append(crecall)\n",
    "\n",
    "\n",
    "# print('total fn+tp:', total_fn+total_tp)\n",
    "# print('total gt:', gt_len)\n",
    "# assert total_fn+total_tp == gt_len, 'total fn+tp not equal to total gt'\n",
    "print('All class recalls:', class_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3841073",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b08f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_seq_path = (f\"{base_dir}/{CODE}/{THREAD}_thread/version_{VER}/faulty_data/diag_subseq/subseq\")\n",
    "\n",
    "def load_detection_seq(json_path):\n",
    "    \"\"\"\n",
    "    load the test data file from the path mentioned in detection_seq_path. The format of data is [event_id, timestamp] and we take only event_id.\n",
    "    Parameters: json_path -> string\n",
    "    Returns: event_ids -> [6,7,8,6,7]\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if isinstance(data, list) and len(data) > 0 and isinstance(data[0], list):\n",
    "        return [row[0] for row in data]\n",
    "\n",
    "    return []\n",
    "\n",
    "detection_seq_map = {}\n",
    "\n",
    "# Iterating over the gt_labels file we created with test_file name\n",
    "for test_file_name in label_map.keys():                         # label_map is the GT labels we have created\n",
    "    detection_path = os.path.join(detection_seq_path, test_file_name)   # appending the test_filename to detection_path to take the complete event_d seq for that file\n",
    "    if os.path.exists(detection_path):\n",
    "        detection_seq_map[test_file_name] = load_detection_seq(detection_path)  # Filters out the timestamp and returns only event_ids as lists\n",
    "    else:\n",
    "        detection_seq_map[test_file_name] = []\n",
    "\n",
    "def count_occurrences(subseq, pattern):\n",
    "    \"\"\"\n",
    "    This function returns the number of occurences of a pattern in the larger sequence of event_id. Here pattern is the gt_seq.\n",
    "    Parameters: subseq -> list[int] [6, 7, 8, 6, 7, 8, 9]\n",
    "                pattern -> list[int] [6, 7, 8]\n",
    "    Returns: int -> count of pattern in subseq\n",
    "    \"\"\"\n",
    "    if subseq is None:\n",
    "        return None\n",
    "    count = 0\n",
    "    L = len(pattern)\n",
    "    for i in range(len(subseq) - L + 1):\n",
    "        if subseq[i:i+L] == pattern:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def analyze_fault(gt_seqs, detection_subseq, matched_patterns):\n",
    "    results = []\n",
    "\n",
    "    matched_set = {tuple(p) for p in matched_patterns}\n",
    "\n",
    "    for gt in gt_seqs:\n",
    "        gt_tuple = tuple(gt)\n",
    "        occ = count_occurrences(detection_subseq, gt)\n",
    "\n",
    "        status = \"TP\" if gt_tuple in matched_set else \"FN\"\n",
    "\n",
    "        results.append({\n",
    "            \"gt_seq\": gt,\n",
    "            \"occurrences_in_detection\": occ,\n",
    "            \"status\": status\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "fault_details = []\n",
    "\n",
    "# all_gt is of format \n",
    "# [('trace_trial1_1090-1140.json', [[12, 6, 7, 8, 9, 6]], './trace_data/theft_protection/single_thread/version_4/faulty_data/gt_test_data_labels.json'),\n",
    "\n",
    "for file_name, gt_seqs_list, label_file_path in all_gt:\n",
    "    detection_subseq = detection_seq_map.get(file_name, [])\n",
    "    results = analyze_fault(gt_seqs_list, detection_subseq, matched_patterns_global) # matched_patterns_global is the set of all discriminative pattern that matched at least one GT sequence\n",
    "    fault_details.append((file_name, results))\n",
    "\n",
    "# for file_name, result_list in fault_details:\n",
    "#     print(f\"\\nFile: {file_name}\")\n",
    "#     for r in result_list:\n",
    "#         print(r)\n",
    "\n",
    "all_entries = []\n",
    "for file_name, results in fault_details:\n",
    "    for r in results:\n",
    "        all_entries.append(r)\n",
    "\n",
    "fn_list = []\n",
    "tp_list = []\n",
    "\n",
    "for i in all_entries:\n",
    "    if i[\"status\"] == \"FN\" and i[\"occurrences_in_detection\"] is not None:\n",
    "        fn_list.append(i)\n",
    "    elif i[\"status\"] == \"TP\" and i[\"occurrences_in_detection\"] is not None:\n",
    "        tp_list.append(i)\n",
    "\n",
    "fn_list_with_single_occurence = []\n",
    "tp_list_with_more_occurence = []\n",
    "\n",
    "for i in fn_list:\n",
    "    if i[\"occurrences_in_detection\"] == 1:\n",
    "        fn_list_with_single_occurence.append(i)\n",
    "\n",
    "for i in tp_list:\n",
    "    if i[\"occurrences_in_detection\"] >= 2:\n",
    "        tp_list_with_more_occurence.append(i)\n",
    "\n",
    "print(f\"Total GT sequences with detection : {len(all_entries)}\")\n",
    "print(f\"TP count : {len(tp_list)}\")\n",
    "print(f\"FN count : {len(fn_list)}\")\n",
    "print(f\"FN with exactly 1 occurrence : {len(fn_list_with_single_occurence)}\")\n",
    "print(f\"TP with more than 1 occurrences : {len(tp_list_with_more_occurence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d84d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
